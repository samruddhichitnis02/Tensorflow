{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('DJI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(251, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>25307.140625</td>\n",
       "      <td>25549.710938</td>\n",
       "      <td>25250.970703</td>\n",
       "      <td>25538.460938</td>\n",
       "      <td>25538.460938</td>\n",
       "      <td>482250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-03</td>\n",
       "      <td>25779.570313</td>\n",
       "      <td>25980.210938</td>\n",
       "      <td>25670.509766</td>\n",
       "      <td>25826.429688</td>\n",
       "      <td>25826.429688</td>\n",
       "      <td>388480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-04</td>\n",
       "      <td>25752.560547</td>\n",
       "      <td>25773.119141</td>\n",
       "      <td>25008.109375</td>\n",
       "      <td>25027.070313</td>\n",
       "      <td>25027.070313</td>\n",
       "      <td>418900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-06</td>\n",
       "      <td>24737.419922</td>\n",
       "      <td>24951.009766</td>\n",
       "      <td>24242.220703</td>\n",
       "      <td>24947.669922</td>\n",
       "      <td>24947.669922</td>\n",
       "      <td>471690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>24918.820313</td>\n",
       "      <td>25095.619141</td>\n",
       "      <td>24284.779297</td>\n",
       "      <td>24388.949219</td>\n",
       "      <td>24388.949219</td>\n",
       "      <td>398230000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date          Open          High           Low         Close  \\\n",
       "0  2018-11-30  25307.140625  25549.710938  25250.970703  25538.460938   \n",
       "1  2018-12-03  25779.570313  25980.210938  25670.509766  25826.429688   \n",
       "2  2018-12-04  25752.560547  25773.119141  25008.109375  25027.070313   \n",
       "3  2018-12-06  24737.419922  24951.009766  24242.220703  24947.669922   \n",
       "4  2018-12-07  24918.820313  25095.619141  24284.779297  24388.949219   \n",
       "\n",
       "      Adj Close     Volume  \n",
       "0  25538.460938  482250000  \n",
       "1  25826.429688  388480000  \n",
       "2  25027.070313  418900000  \n",
       "3  24947.669922  471690000  \n",
       "4  24388.949219  398230000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Date', 'Adj Close', 'Volume'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25307.140625</td>\n",
       "      <td>25549.710938</td>\n",
       "      <td>25250.970703</td>\n",
       "      <td>25538.460938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25779.570313</td>\n",
       "      <td>25980.210938</td>\n",
       "      <td>25670.509766</td>\n",
       "      <td>25826.429688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25752.560547</td>\n",
       "      <td>25773.119141</td>\n",
       "      <td>25008.109375</td>\n",
       "      <td>25027.070313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24737.419922</td>\n",
       "      <td>24951.009766</td>\n",
       "      <td>24242.220703</td>\n",
       "      <td>24947.669922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24918.820313</td>\n",
       "      <td>25095.619141</td>\n",
       "      <td>24284.779297</td>\n",
       "      <td>24388.949219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Open          High           Low         Close\n",
       "0  25307.140625  25549.710938  25250.970703  25538.460938\n",
       "1  25779.570313  25980.210938  25670.509766  25826.429688\n",
       "2  25752.560547  25773.119141  25008.109375  25027.070313\n",
       "3  24737.419922  24951.009766  24242.220703  24947.669922\n",
       "4  24918.820313  25095.619141  24284.779297  24388.949219"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.heatmap(data.isna(),\n",
    "          cbar = False,\n",
    "          yticklabels = False,\n",
    "          cmap = 'viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((251, 3), (251, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 3), (200, 1), (51, 3), (51, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.compat.v1.placeholder(dtype = tf.float32, name = 'x')\n",
    "y = tf.compat.v1.placeholder(dtype = tf.float32, name = 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'x:0' shape=<unknown> dtype=float32>,\n",
       " <tf.Tensor 'y:0' shape=<unknown> dtype=float32>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/admin1/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "w1 = tf.Variable(tf.compat.v1.random_uniform([x_train.shape[1],10]))\n",
    "b1 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "\n",
    "w2 = tf.Variable(tf.compat.v1.random_uniform([10, 5]))\n",
    "b2 = tf.Variable(tf.zeros([5]))\n",
    "\n",
    "\n",
    "\n",
    "wo = tf.Variable(tf.compat.v1.random_uniform([5, 1]))\n",
    "bo = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=(3, 10) dtype=float32>,\n",
       " <tf.Variable 'Variable_1:0' shape=(10,) dtype=float32>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable_2:0' shape=(10, 5) dtype=float32>,\n",
       " <tf.Variable 'Variable_3:0' shape=(5,) dtype=float32>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable_4:0' shape=(5, 1) dtype=float32>,\n",
       " <tf.Variable 'Variable_5:0' shape=(1,) dtype=float32>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wo, bo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = tf.add(tf.matmul(x, w1), b1)\n",
    "hidden1 = tf.nn.relu(hidden1)\n",
    "\n",
    "\n",
    "#Layer2 propagation\n",
    "hidden2 = tf.add(tf.matmul(hidden1, w2), b2)\n",
    "hidden2 = tf.nn.relu(hidden2)\n",
    "\n",
    "\n",
    "\n",
    "#Output Layer propagation\n",
    "out = tf.add(tf.matmul(hidden2, wo), bo, name = 'out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu:0' shape=<unknown> dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_1:0' shape=<unknown> dtype=float32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'out:0' shape=<unknown> dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(out - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.compat.v1.train.GradientDescentOptimizer(0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Training_loss : 2.5215201e+19 Test_loss : 2.739148e+19\n",
      "Epoch : 1 Training_loss : 5528418600000000.0 Test_loss : 5528449000000000.0\n",
      "Epoch : 2 Training_loss : 5309493300000000.0 Test_loss : 5309523000000000.0\n",
      "Epoch : 3 Training_loss : 5099237000000000.0 Test_loss : 5099267000000000.0\n",
      "Epoch : 4 Training_loss : 4897307500000000.0 Test_loss : 4897336500000000.0\n",
      "Epoch : 5 Training_loss : 4703374000000000.0 Test_loss : 4703402600000000.0\n",
      "Epoch : 6 Training_loss : 4517120000000000.0 Test_loss : 4517148000000000.0\n",
      "Epoch : 7 Training_loss : 4338242000000000.0 Test_loss : 4338269400000000.0\n",
      "Epoch : 8 Training_loss : 4166448000000000.0 Test_loss : 4166474800000000.0\n",
      "Epoch : 9 Training_loss : 4001456700000000.0 Test_loss : 4001482500000000.0\n",
      "Epoch : 10 Training_loss : 3842999000000000.0 Test_loss : 3843024300000000.0\n",
      "Epoch : 11 Training_loss : 3690816300000000.0 Test_loss : 3690841000000000.0\n",
      "Epoch : 12 Training_loss : 3544659900000000.0 Test_loss : 3544684000000000.0\n",
      "Epoch : 13 Training_loss : 3404291500000000.0 Test_loss : 3404315400000000.0\n",
      "Epoch : 14 Training_loss : 3269482000000000.0 Test_loss : 3269505200000000.0\n",
      "Epoch : 15 Training_loss : 3140010300000000.0 Test_loss : 3140033400000000.0\n",
      "Epoch : 16 Training_loss : 3015665700000000.0 Test_loss : 3015688200000000.0\n",
      "Epoch : 17 Training_loss : 2896245700000000.0 Test_loss : 2896268000000000.0\n",
      "Epoch : 18 Training_loss : 2781554200000000.0 Test_loss : 2781576200000000.0\n",
      "Epoch : 19 Training_loss : 2671404700000000.0 Test_loss : 2671426000000000.0\n",
      "Epoch : 20 Training_loss : 2565617200000000.0 Test_loss : 2565638200000000.0\n",
      "Epoch : 21 Training_loss : 2464018700000000.0 Test_loss : 2464039000000000.0\n",
      "Epoch : 22 Training_loss : 2366443500000000.0 Test_loss : 2366463600000000.0\n",
      "Epoch : 23 Training_loss : 2272732100000000.0 Test_loss : 2272751700000000.0\n",
      "Epoch : 24 Training_loss : 2182731900000000.0 Test_loss : 2182751200000000.0\n",
      "Epoch : 25 Training_loss : 2096295800000000.0 Test_loss : 2096314700000000.0\n",
      "Epoch : 26 Training_loss : 2013282400000000.0 Test_loss : 2013300800000000.0\n",
      "Epoch : 27 Training_loss : 1933556400000000.0 Test_loss : 1933574500000000.0\n",
      "Epoch : 28 Training_loss : 1856987500000000.0 Test_loss : 1857005300000000.0\n",
      "Epoch : 29 Training_loss : 1783450800000000.0 Test_loss : 1783468100000000.0\n",
      "Epoch : 30 Training_loss : 1712826000000000.0 Test_loss : 1712843000000000.0\n",
      "Epoch : 31 Training_loss : 1644998200000000.0 Test_loss : 1645014900000000.0\n",
      "Epoch : 32 Training_loss : 1579856100000000.0 Test_loss : 1579872700000000.0\n",
      "Epoch : 33 Training_loss : 1517294000000000.0 Test_loss : 1517310100000000.0\n",
      "Epoch : 34 Training_loss : 1457209300000000.0 Test_loss : 1457225000000000.0\n",
      "Epoch : 35 Training_loss : 1399503800000000.0 Test_loss : 1399519300000000.0\n",
      "Epoch : 36 Training_loss : 1344083300000000.0 Test_loss : 1344098500000000.0\n",
      "Epoch : 37 Training_loss : 1290857500000000.0 Test_loss : 1290872400000000.0\n",
      "Epoch : 38 Training_loss : 1239739500000000.0 Test_loss : 1239754000000000.0\n",
      "Epoch : 39 Training_loss : 1190645600000000.0 Test_loss : 1190660000000000.0\n",
      "Epoch : 40 Training_loss : 1143496000000000.0 Test_loss : 1143510000000000.0\n",
      "Epoch : 41 Training_loss : 1098213600000000.0 Test_loss : 1098227160000000.0\n",
      "Epoch : 42 Training_loss : 1054724600000000.0 Test_loss : 1054737700000000.0\n",
      "Epoch : 43 Training_loss : 1012957200000000.0 Test_loss : 1012970400000000.0\n",
      "Epoch : 44 Training_loss : 972844100000000.0 Test_loss : 972857100000000.0\n",
      "Epoch : 45 Training_loss : 934319540000000.0 Test_loss : 934332200000000.0\n",
      "Epoch : 46 Training_loss : 897320540000000.0 Test_loss : 897332950000000.0\n",
      "Epoch : 47 Training_loss : 861786700000000.0 Test_loss : 861798800000000.0\n",
      "Epoch : 48 Training_loss : 827660060000000.0 Test_loss : 827671900000000.0\n",
      "Epoch : 49 Training_loss : 794884600000000.0 Test_loss : 794896240000000.0\n",
      "Epoch : 50 Training_loss : 763407200000000.0 Test_loss : 763418560000000.0\n",
      "Epoch : 51 Training_loss : 733176350000000.0 Test_loss : 733187360000000.0\n",
      "Epoch : 52 Training_loss : 704142500000000.0 Test_loss : 704153400000000.0\n",
      "Epoch : 53 Training_loss : 676258500000000.0 Test_loss : 676269200000000.0\n",
      "Epoch : 54 Training_loss : 649478600000000.0 Test_loss : 649489050000000.0\n",
      "Epoch : 55 Training_loss : 623759200000000.0 Test_loss : 623769440000000.0\n",
      "Epoch : 56 Training_loss : 599058300000000.0 Test_loss : 599068400000000.0\n",
      "Epoch : 57 Training_loss : 575335600000000.0 Test_loss : 575345500000000.0\n",
      "Epoch : 58 Training_loss : 552552270000000.0 Test_loss : 552561940000000.0\n",
      "Epoch : 59 Training_loss : 530671260000000.0 Test_loss : 530680700000000.0\n",
      "Epoch : 60 Training_loss : 509656620000000.0 Test_loss : 509665950000000.0\n",
      "Epoch : 61 Training_loss : 489474240000000.0 Test_loss : 489483360000000.0\n",
      "Epoch : 62 Training_loss : 470090950000000.0 Test_loss : 470099900000000.0\n",
      "Epoch : 63 Training_loss : 451475420000000.0 Test_loss : 451484100000000.0\n",
      "Epoch : 64 Training_loss : 433596950000000.0 Test_loss : 433605640000000.0\n",
      "Epoch : 65 Training_loss : 416426540000000.0 Test_loss : 416434960000000.0\n",
      "Epoch : 66 Training_loss : 399936100000000.0 Test_loss : 399944330000000.0\n",
      "Epoch : 67 Training_loss : 384098620000000.0 Test_loss : 384106700000000.0\n",
      "Epoch : 68 Training_loss : 368888300000000.0 Test_loss : 368896250000000.0\n",
      "Epoch : 69 Training_loss : 354280340000000.0 Test_loss : 354288100000000.0\n",
      "Epoch : 70 Training_loss : 340250870000000.0 Test_loss : 340258450000000.0\n",
      "Epoch : 71 Training_loss : 326776900000000.0 Test_loss : 326784370000000.0\n",
      "Epoch : 72 Training_loss : 313836580000000.0 Test_loss : 313843860000000.0\n",
      "Epoch : 73 Training_loss : 301408600000000.0 Test_loss : 301415800000000.0\n",
      "Epoch : 74 Training_loss : 289472840000000.0 Test_loss : 289479860000000.0\n",
      "Epoch : 75 Training_loss : 278009730000000.0 Test_loss : 278016600000000.0\n",
      "Epoch : 76 Training_loss : 267000550000000.0 Test_loss : 267007300000000.0\n",
      "Epoch : 77 Training_loss : 256427330000000.0 Test_loss : 256433920000000.0\n",
      "Epoch : 78 Training_loss : 246272820000000.0 Test_loss : 246279300000000.0\n",
      "Epoch : 79 Training_loss : 236520410000000.0 Test_loss : 236526750000000.0\n",
      "Epoch : 80 Training_loss : 227154210000000.0 Test_loss : 227160420000000.0\n",
      "Epoch : 81 Training_loss : 218158870000000.0 Test_loss : 218164960000000.0\n",
      "Epoch : 82 Training_loss : 209519800000000.0 Test_loss : 209525750000000.0\n",
      "Epoch : 83 Training_loss : 201222840000000.0 Test_loss : 201228650000000.0\n",
      "Epoch : 84 Training_loss : 193254400000000.0 Test_loss : 193260140000000.0\n",
      "Epoch : 85 Training_loss : 185601500000000.0 Test_loss : 185607150000000.0\n",
      "Epoch : 86 Training_loss : 178251700000000.0 Test_loss : 178257200000000.0\n",
      "Epoch : 87 Training_loss : 171192970000000.0 Test_loss : 171198340000000.0\n",
      "Epoch : 88 Training_loss : 164413710000000.0 Test_loss : 164419000000000.0\n",
      "Epoch : 89 Training_loss : 157902910000000.0 Test_loss : 157908110000000.0\n",
      "Epoch : 90 Training_loss : 151649980000000.0 Test_loss : 151655030000000.0\n",
      "Epoch : 91 Training_loss : 145644640000000.0 Test_loss : 145649600000000.0\n",
      "Epoch : 92 Training_loss : 139877100000000.0 Test_loss : 139881970000000.0\n",
      "Epoch : 93 Training_loss : 134337960000000.0 Test_loss : 134342740000000.0\n",
      "Epoch : 94 Training_loss : 129018220000000.0 Test_loss : 129022870000000.0\n",
      "Epoch : 95 Training_loss : 123909085000000.0 Test_loss : 123913680000000.0\n",
      "Epoch : 96 Training_loss : 119002295000000.0 Test_loss : 119006780000000.0\n",
      "Epoch : 97 Training_loss : 114289810000000.0 Test_loss : 114294205000000.0\n",
      "Epoch : 98 Training_loss : 109763920000000.0 Test_loss : 109768240000000.0\n",
      "Epoch : 99 Training_loss : 105417260000000.0 Test_loss : 105421490000000.0\n",
      "Epoch : 100 Training_loss : 101242730000000.0 Test_loss : 101246880000000.0\n",
      "Epoch : 101 Training_loss : 97233520000000.0 Test_loss : 97237590000000.0\n",
      "Epoch : 102 Training_loss : 93383080000000.0 Test_loss : 93387060000000.0\n",
      "Epoch : 103 Training_loss : 89685100000000.0 Test_loss : 89689010000000.0\n",
      "Epoch : 104 Training_loss : 86133590000000.0 Test_loss : 86137415000000.0\n",
      "Epoch : 105 Training_loss : 82722710000000.0 Test_loss : 82726450000000.0\n",
      "Epoch : 106 Training_loss : 79446880000000.0 Test_loss : 79450550000000.0\n",
      "Epoch : 107 Training_loss : 76300780000000.0 Test_loss : 76304380000000.0\n",
      "Epoch : 108 Training_loss : 73279270000000.0 Test_loss : 73282790000000.0\n",
      "Epoch : 109 Training_loss : 70377410000000.0 Test_loss : 70380866000000.0\n",
      "Epoch : 110 Training_loss : 67590470000000.0 Test_loss : 67593860000000.0\n",
      "Epoch : 111 Training_loss : 64913884000000.0 Test_loss : 64917198000000.0\n",
      "Epoch : 112 Training_loss : 62343287000000.0 Test_loss : 62346540000000.0\n",
      "Epoch : 113 Training_loss : 59874495000000.0 Test_loss : 59877687000000.0\n",
      "Epoch : 114 Training_loss : 57503463000000.0 Test_loss : 57506596000000.0\n",
      "Epoch : 115 Training_loss : 55226334000000.0 Test_loss : 55229396000000.0\n",
      "Epoch : 116 Training_loss : 53039360000000.0 Test_loss : 53042372000000.0\n",
      "Epoch : 117 Training_loss : 50939010000000.0 Test_loss : 50941953000000.0\n",
      "Epoch : 118 Training_loss : 48921825000000.0 Test_loss : 48924710000000.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 119 Training_loss : 46984526000000.0 Test_loss : 46987353000000.0\n",
      "Epoch : 120 Training_loss : 45123937000000.0 Test_loss : 45126710000000.0\n",
      "Epoch : 121 Training_loss : 43337030000000.0 Test_loss : 43339747000000.0\n",
      "Epoch : 122 Training_loss : 41620890000000.0 Test_loss : 41623550000000.0\n",
      "Epoch : 123 Training_loss : 39972707000000.0 Test_loss : 39975310000000.0\n",
      "Epoch : 124 Training_loss : 38389785000000.0 Test_loss : 38392335000000.0\n",
      "Epoch : 125 Training_loss : 36869547000000.0 Test_loss : 36872050000000.0\n",
      "Epoch : 126 Training_loss : 35409514000000.0 Test_loss : 35411970000000.0\n",
      "Epoch : 127 Training_loss : 34007300000000.0 Test_loss : 34009700000000.0\n",
      "Epoch : 128 Training_loss : 32660607000000.0 Test_loss : 32662964000000.0\n",
      "Epoch : 129 Training_loss : 31367247000000.0 Test_loss : 31369554000000.0\n",
      "Epoch : 130 Training_loss : 30125102000000.0 Test_loss : 30127367000000.0\n",
      "Epoch : 131 Training_loss : 28932150000000.0 Test_loss : 28934368000000.0\n",
      "Epoch : 132 Training_loss : 27786442000000.0 Test_loss : 27788610000000.0\n",
      "Epoch : 133 Training_loss : 26686100000000.0 Test_loss : 26688226000000.0\n",
      "Epoch : 134 Training_loss : 25629332000000.0 Test_loss : 25631417000000.0\n",
      "Epoch : 135 Training_loss : 24614410000000.0 Test_loss : 24616454000000.0\n",
      "Epoch : 136 Training_loss : 23639676000000.0 Test_loss : 23641679000000.0\n",
      "Epoch : 137 Training_loss : 22703541000000.0 Test_loss : 22705508000000.0\n",
      "Epoch : 138 Training_loss : 21804482000000.0 Test_loss : 21806409000000.0\n",
      "Epoch : 139 Training_loss : 20941025000000.0 Test_loss : 20942913000000.0\n",
      "Epoch : 140 Training_loss : 20111761000000.0 Test_loss : 20113613000000.0\n",
      "Epoch : 141 Training_loss : 19315336000000.0 Test_loss : 19317146000000.0\n",
      "Epoch : 142 Training_loss : 18550446000000.0 Test_loss : 18552222000000.0\n",
      "Epoch : 143 Training_loss : 17815847000000.0 Test_loss : 17817590000000.0\n",
      "Epoch : 144 Training_loss : 17110343000000.0 Test_loss : 17112046000000.0\n",
      "Epoch : 145 Training_loss : 16432771000000.0 Test_loss : 16434443000000.0\n",
      "Epoch : 146 Training_loss : 15782035000000.0 Test_loss : 15783672000000.0\n",
      "Epoch : 147 Training_loss : 15157065000000.0 Test_loss : 15158671000000.0\n",
      "Epoch : 148 Training_loss : 14556844000000.0 Test_loss : 14558418000000.0\n",
      "Epoch : 149 Training_loss : 13980395000000.0 Test_loss : 13981937000000.0\n",
      "Epoch : 150 Training_loss : 13426771000000.0 Test_loss : 13428282000000.0\n",
      "Epoch : 151 Training_loss : 12895073000000.0 Test_loss : 12896553000000.0\n",
      "Epoch : 152 Training_loss : 12384425000000.0 Test_loss : 12385876000000.0\n",
      "Epoch : 153 Training_loss : 11894003000000.0 Test_loss : 11895425000000.0\n",
      "Epoch : 154 Training_loss : 11423002000000.0 Test_loss : 11424395000000.0\n",
      "Epoch : 155 Training_loss : 10970651000000.0 Test_loss : 10972016000000.0\n",
      "Epoch : 156 Training_loss : 10536212000000.0 Test_loss : 10537551000000.0\n",
      "Epoch : 157 Training_loss : 10118980000000.0 Test_loss : 10120290000000.0\n",
      "Epoch : 158 Training_loss : 9718268000000.0 Test_loss : 9719553000000.0\n",
      "Epoch : 159 Training_loss : 9333424000000.0 Test_loss : 9334684000000.0\n",
      "Epoch : 160 Training_loss : 8963821000000.0 Test_loss : 8965054000000.0\n",
      "Epoch : 161 Training_loss : 8608854600000.0 Test_loss : 8610063000000.0\n",
      "Epoch : 162 Training_loss : 8267943600000.0 Test_loss : 8269128500000.0\n",
      "Epoch : 163 Training_loss : 7940532600000.0 Test_loss : 7941694400000.0\n",
      "Epoch : 164 Training_loss : 7626087700000.0 Test_loss : 7627226000000.0\n",
      "Epoch : 165 Training_loss : 7324094000000.0 Test_loss : 7325210400000.0\n",
      "Epoch : 166 Training_loss : 7034061000000.0 Test_loss : 7035154300000.0\n",
      "Epoch : 167 Training_loss : 6755512000000.0 Test_loss : 6756584400000.0\n",
      "Epoch : 168 Training_loss : 6487995000000.0 Test_loss : 6489044400000.0\n",
      "Epoch : 169 Training_loss : 6231070000000.0 Test_loss : 6232098700000.0\n",
      "Epoch : 170 Training_loss : 5984320000000.0 Test_loss : 5985328000000.0\n",
      "Epoch : 171 Training_loss : 5747340500000.0 Test_loss : 5748329000000.0\n",
      "Epoch : 172 Training_loss : 5519745500000.0 Test_loss : 5520714000000.0\n",
      "Epoch : 173 Training_loss : 5301164600000.0 Test_loss : 5302113500000.0\n",
      "Epoch : 174 Training_loss : 5091238000000.0 Test_loss : 5092168000000.0\n",
      "Epoch : 175 Training_loss : 4889625000000.0 Test_loss : 4890537000000.0\n",
      "Epoch : 176 Training_loss : 4695997000000.0 Test_loss : 4696889000000.0\n",
      "Epoch : 177 Training_loss : 4510035000000.0 Test_loss : 4510910000000.0\n",
      "Epoch : 178 Training_loss : 4331437800000.0 Test_loss : 4332295300000.0\n",
      "Epoch : 179 Training_loss : 4159912800000.0 Test_loss : 4160753200000.0\n",
      "Epoch : 180 Training_loss : 3995180000000.0 Test_loss : 3996003600000.0\n",
      "Epoch : 181 Training_loss : 3836970800000.0 Test_loss : 3837778200000.0\n",
      "Epoch : 182 Training_loss : 3685027000000.0 Test_loss : 3685818000000.0\n",
      "Epoch : 183 Training_loss : 3539100000000.0 Test_loss : 3539875100000.0\n",
      "Epoch : 184 Training_loss : 3398951200000.0 Test_loss : 3399711000000.0\n",
      "Epoch : 185 Training_loss : 3264353000000.0 Test_loss : 3265097600000.0\n",
      "Epoch : 186 Training_loss : 3135085200000.0 Test_loss : 3135814200000.0\n",
      "Epoch : 187 Training_loss : 3010936000000.0 Test_loss : 3011650500000.0\n",
      "Epoch : 188 Training_loss : 2891702000000.0 Test_loss : 2892402800000.0\n",
      "Epoch : 189 Training_loss : 2777191000000.0 Test_loss : 2777877600000.0\n",
      "Epoch : 190 Training_loss : 2667214300000.0 Test_loss : 2667887200000.0\n",
      "Epoch : 191 Training_loss : 2561592500000.0 Test_loss : 2562252300000.0\n",
      "Epoch : 192 Training_loss : 2460153300000.0 Test_loss : 2460799700000.0\n",
      "Epoch : 193 Training_loss : 2362731400000.0 Test_loss : 2363364700000.0\n",
      "Epoch : 194 Training_loss : 2269167200000.0 Test_loss : 2269788000000.0\n",
      "Epoch : 195 Training_loss : 2179308300000.0 Test_loss : 2179916500000.0\n",
      "Epoch : 196 Training_loss : 2093007500000.0 Test_loss : 2093603600000.0\n",
      "Epoch : 197 Training_loss : 2010124500000.0 Test_loss : 2010708600000.0\n",
      "Epoch : 198 Training_loss : 1930523300000.0 Test_loss : 1931095800000.0\n",
      "Epoch : 199 Training_loss : 1854074800000.0 Test_loss : 1854636000000.0\n",
      "Epoch : 200 Training_loss : 1780653500000.0 Test_loss : 1781203300000.0\n",
      "Epoch : 201 Training_loss : 1710139500000.0 Test_loss : 1710678500000.0\n",
      "Epoch : 202 Training_loss : 1642418100000.0 Test_loss : 1642946100000.0\n",
      "Epoch : 203 Training_loss : 1577378300000.0 Test_loss : 1577895700000.0\n",
      "Epoch : 204 Training_loss : 1514914200000.0 Test_loss : 1515421300000.0\n",
      "Epoch : 205 Training_loss : 1454923700000.0 Test_loss : 1455420500000.0\n",
      "Epoch : 206 Training_loss : 1397308800000.0 Test_loss : 1397795700000.0\n",
      "Epoch : 207 Training_loss : 1341975400000.0 Test_loss : 1342452700000.0\n",
      "Epoch : 208 Training_loss : 1288833300000.0 Test_loss : 1289301000000.0\n",
      "Epoch : 209 Training_loss : 1237795500000.0 Test_loss : 1238253800000.0\n",
      "Epoch : 210 Training_loss : 1188778700000.0 Test_loss : 1189227900000.0\n",
      "Epoch : 211 Training_loss : 1141703200000.0 Test_loss : 1142143300000.0\n",
      "Epoch : 212 Training_loss : 1096491900000.0 Test_loss : 1096923350000.0\n",
      "Epoch : 213 Training_loss : 1053071000000.0 Test_loss : 1053493560000.0\n",
      "Epoch : 214 Training_loss : 1011369300000.0 Test_loss : 1011783600000.0\n",
      "Epoch : 215 Training_loss : 971319300000.0 Test_loss : 971725100000.0\n",
      "Epoch : 216 Training_loss : 932855100000.0 Test_loss : 933252900000.0\n",
      "Epoch : 217 Training_loss : 895914100000.0 Test_loss : 896303800000.0\n",
      "Epoch : 218 Training_loss : 860435900000.0 Test_loss : 860818000000.0\n",
      "Epoch : 219 Training_loss : 826362700000.0 Test_loss : 826737030000.0\n",
      "Epoch : 220 Training_loss : 793638800000.0 Test_loss : 794005730000.0\n",
      "Epoch : 221 Training_loss : 762210800000.0 Test_loss : 762570400000.0\n",
      "Epoch : 222 Training_loss : 732027360000.0 Test_loss : 732379600000.0\n",
      "Epoch : 223 Training_loss : 703039100000.0 Test_loss : 703384450000.0\n",
      "Epoch : 224 Training_loss : 675198800000.0 Test_loss : 675537200000.0\n",
      "Epoch : 225 Training_loss : 648461000000.0 Test_loss : 648792700000.0\n",
      "Epoch : 226 Training_loss : 622782100000.0 Test_loss : 623107050000.0\n",
      "Epoch : 227 Training_loss : 598120000000.0 Test_loss : 598438400000.0\n",
      "Epoch : 228 Training_loss : 574434440000.0 Test_loss : 574746600000.0\n",
      "Epoch : 229 Training_loss : 551686960000.0 Test_loss : 551992800000.0\n",
      "Epoch : 230 Training_loss : 529840240000.0 Test_loss : 530139900000.0\n",
      "Epoch : 231 Training_loss : 508858600000.0 Test_loss : 509152260000.0\n",
      "Epoch : 232 Training_loss : 488707850000.0 Test_loss : 488995700000.0\n",
      "Epoch : 233 Training_loss : 469355100000.0 Test_loss : 469637170000.0\n",
      "Epoch : 234 Training_loss : 450768700000.0 Test_loss : 451045130000.0\n",
      "Epoch : 235 Training_loss : 432918360000.0 Test_loss : 433189220000.0\n",
      "Epoch : 236 Training_loss : 415774770000.0 Test_loss : 416040260000.0\n",
      "Epoch : 237 Training_loss : 399310130000.0 Test_loss : 399570270000.0\n",
      "Epoch : 238 Training_loss : 383497470000.0 Test_loss : 383752400000.0\n",
      "Epoch : 239 Training_loss : 368311000000.0 Test_loss : 368560900000.0\n",
      "Epoch : 240 Training_loss : 353726040000.0 Test_loss : 353970850000.0\n",
      "Epoch : 241 Training_loss : 339718470000.0 Test_loss : 339958400000.0\n",
      "Epoch : 242 Training_loss : 326265670000.0 Test_loss : 326500800000.0\n",
      "Epoch : 243 Training_loss : 313345600000.0 Test_loss : 313576030000.0\n",
      "Epoch : 244 Training_loss : 300937120000.0 Test_loss : 301163000000.0\n",
      "Epoch : 245 Training_loss : 289020120000.0 Test_loss : 289241330000.0\n",
      "Epoch : 246 Training_loss : 277574980000.0 Test_loss : 277791770000.0\n",
      "Epoch : 247 Training_loss : 266583050000.0 Test_loss : 266795480000.0\n",
      "Epoch : 248 Training_loss : 256026400000.0 Test_loss : 256234620000.0\n",
      "Epoch : 249 Training_loss : 245887800000.0 Test_loss : 246091860000.0\n",
      "Epoch : 250 Training_loss : 236150700000.0 Test_loss : 236350670000.0\n",
      "Epoch : 251 Training_loss : 226799220000.0 Test_loss : 226995160000.0\n",
      "Epoch : 252 Training_loss : 217818000000.0 Test_loss : 218010080000.0\n",
      "Epoch : 253 Training_loss : 209192480000.0 Test_loss : 209380690000.0\n",
      "Epoch : 254 Training_loss : 200908520000.0 Test_loss : 201092940000.0\n",
      "Epoch : 255 Training_loss : 192952610000.0 Test_loss : 193133350000.0\n",
      "Epoch : 256 Training_loss : 185311740000.0 Test_loss : 185488850000.0\n",
      "Epoch : 257 Training_loss : 177973460000.0 Test_loss : 178147000000.0\n",
      "Epoch : 258 Training_loss : 170925770000.0 Test_loss : 171095850000.0\n",
      "Epoch : 259 Training_loss : 164157180000.0 Test_loss : 164323820000.0\n",
      "Epoch : 260 Training_loss : 157656610000.0 Test_loss : 157819930000.0\n",
      "Epoch : 261 Training_loss : 151413460000.0 Test_loss : 151573510000.0\n",
      "Epoch : 262 Training_loss : 145417550000.0 Test_loss : 145574380000.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 263 Training_loss : 139659080000.0 Test_loss : 139812780000.0\n",
      "Epoch : 264 Training_loss : 134128640000.0 Test_loss : 134279230000.0\n",
      "Epoch : 265 Training_loss : 128817200000.0 Test_loss : 128964770000.0\n",
      "Epoch : 266 Training_loss : 123716090000.0 Test_loss : 123860700000.0\n",
      "Epoch : 267 Training_loss : 118816980000.0 Test_loss : 118958694000.0\n",
      "Epoch : 268 Training_loss : 114111880000.0 Test_loss : 114250740000.0\n",
      "Epoch : 269 Training_loss : 109593080000.0 Test_loss : 109729190000.0\n",
      "Epoch : 270 Training_loss : 105253250000.0 Test_loss : 105386615000.0\n",
      "Epoch : 271 Training_loss : 101085280000.0 Test_loss : 101215986000.0\n",
      "Epoch : 272 Training_loss : 97082360000.0 Test_loss : 97210425000.0\n",
      "Epoch : 273 Training_loss : 93237950000.0 Test_loss : 93363460000.0\n",
      "Epoch : 274 Training_loss : 89545790000.0 Test_loss : 89668760000.0\n",
      "Epoch : 275 Training_loss : 85999830000.0 Test_loss : 86120350000.0\n",
      "Epoch : 276 Training_loss : 82594300000.0 Test_loss : 82712396000.0\n",
      "Epoch : 277 Training_loss : 79323620000.0 Test_loss : 79439350000.0\n",
      "Epoch : 278 Training_loss : 76182460000.0 Test_loss : 76295860000.0\n",
      "Epoch : 279 Training_loss : 73165685000.0 Test_loss : 73276820000.0\n",
      "Epoch : 280 Training_loss : 70268380000.0 Test_loss : 70377280000.0\n",
      "Epoch : 281 Training_loss : 67485810000.0 Test_loss : 67592520000.0\n",
      "Epoch : 282 Training_loss : 64813425000.0 Test_loss : 64918000000.0\n",
      "Epoch : 283 Training_loss : 62246883000.0 Test_loss : 62349340000.0\n",
      "Epoch : 284 Training_loss : 59781960000.0 Test_loss : 59882365000.0\n",
      "Epoch : 285 Training_loss : 57414644000.0 Test_loss : 57513040000.0\n",
      "Epoch : 286 Training_loss : 55141085000.0 Test_loss : 55237505000.0\n",
      "Epoch : 287 Training_loss : 52957557000.0 Test_loss : 53052040000.0\n",
      "Epoch : 288 Training_loss : 50860490000.0 Test_loss : 50953085000.0\n",
      "Epoch : 289 Training_loss : 48846475000.0 Test_loss : 48937200000.0\n",
      "Epoch : 290 Training_loss : 46912210000.0 Test_loss : 47001113000.0\n",
      "Epoch : 291 Training_loss : 45054540000.0 Test_loss : 45141660000.0\n",
      "Epoch : 292 Training_loss : 43270440000.0 Test_loss : 43355810000.0\n",
      "Epoch : 293 Training_loss : 41556990000.0 Test_loss : 41640645000.0\n",
      "Epoch : 294 Training_loss : 39911390000.0 Test_loss : 39993364000.0\n",
      "Epoch : 295 Training_loss : 38330958000.0 Test_loss : 38411280000.0\n",
      "Epoch : 296 Training_loss : 36813110000.0 Test_loss : 36891820000.0\n",
      "Epoch : 297 Training_loss : 35355365000.0 Test_loss : 35432490000.0\n",
      "Epoch : 298 Training_loss : 33955346000.0 Test_loss : 34030928000.0\n",
      "Epoch : 299 Training_loss : 32610777000.0 Test_loss : 32684830000.0\n",
      "Epoch : 300 Training_loss : 31319446000.0 Test_loss : 31392016000.0\n",
      "Epoch : 301 Training_loss : 30079252000.0 Test_loss : 30150360000.0\n",
      "Epoch : 302 Training_loss : 28888168000.0 Test_loss : 28957850000.0\n",
      "Epoch : 303 Training_loss : 27744256000.0 Test_loss : 27812532000.0\n",
      "Epoch : 304 Training_loss : 26645643000.0 Test_loss : 26712543000.0\n",
      "Epoch : 305 Training_loss : 25590532000.0 Test_loss : 25656090000.0\n",
      "Epoch : 306 Training_loss : 24577204000.0 Test_loss : 24641442000.0\n",
      "Epoch : 307 Training_loss : 23603999000.0 Test_loss : 23666946000.0\n",
      "Epoch : 308 Training_loss : 22669337000.0 Test_loss : 22731018000.0\n",
      "Epoch : 309 Training_loss : 21771688000.0 Test_loss : 21832129000.0\n",
      "Epoch : 310 Training_loss : 20909586000.0 Test_loss : 20968806000.0\n",
      "Epoch : 311 Training_loss : 20081620000.0 Test_loss : 20139653000.0\n",
      "Epoch : 312 Training_loss : 19286446000.0 Test_loss : 19343309000.0\n",
      "Epoch : 313 Training_loss : 18522760000.0 Test_loss : 18578475000.0\n",
      "Epoch : 314 Training_loss : 17789313000.0 Test_loss : 17843909000.0\n",
      "Epoch : 315 Training_loss : 17084916000.0 Test_loss : 17138409000.0\n",
      "Epoch : 316 Training_loss : 16408409000.0 Test_loss : 16460827000.0\n",
      "Epoch : 317 Training_loss : 15758689000.0 Test_loss : 15810052000.0\n",
      "Epoch : 318 Training_loss : 15134703000.0 Test_loss : 15185029000.0\n",
      "Epoch : 319 Training_loss : 14535426000.0 Test_loss : 14584739000.0\n",
      "Epoch : 320 Training_loss : 13959880000.0 Test_loss : 14008198000.0\n",
      "Epoch : 321 Training_loss : 13407127000.0 Test_loss : 13454469000.0\n",
      "Epoch : 322 Training_loss : 12876261000.0 Test_loss : 12922648000.0\n",
      "Epoch : 323 Training_loss : 12366418000.0 Test_loss : 12411869000.0\n",
      "Epoch : 324 Training_loss : 11876765000.0 Test_loss : 11921299000.0\n",
      "Epoch : 325 Training_loss : 11406502000.0 Test_loss : 11450139000.0\n",
      "Epoch : 326 Training_loss : 10954861000.0 Test_loss : 10997617000.0\n",
      "Epoch : 327 Training_loss : 10521104000.0 Test_loss : 10562998000.0\n",
      "Epoch : 328 Training_loss : 10104526000.0 Test_loss : 10145573000.0\n",
      "Epoch : 329 Training_loss : 9704444000.0 Test_loss : 9744662000.0\n",
      "Epoch : 330 Training_loss : 9320205000.0 Test_loss : 9359610000.0\n",
      "Epoch : 331 Training_loss : 8951179000.0 Test_loss : 8989789000.0\n",
      "Epoch : 332 Training_loss : 8596770000.0 Test_loss : 8634598000.0\n",
      "Epoch : 333 Training_loss : 8256393700.0 Test_loss : 8293459000.0\n",
      "Epoch : 334 Training_loss : 7929497000.0 Test_loss : 7965813000.0\n",
      "Epoch : 335 Training_loss : 7615546000.0 Test_loss : 7651127300.0\n",
      "Epoch : 336 Training_loss : 7314026000.0 Test_loss : 7348888600.0\n",
      "Epoch : 337 Training_loss : 7024447500.0 Test_loss : 7058603500.0\n",
      "Epoch : 338 Training_loss : 6746336000.0 Test_loss : 6779801600.0\n",
      "Epoch : 339 Training_loss : 6479237600.0 Test_loss : 6512026600.0\n",
      "Epoch : 340 Training_loss : 6222716400.0 Test_loss : 6254842000.0\n",
      "Epoch : 341 Training_loss : 5976354000.0 Test_loss : 6007828000.0\n",
      "Epoch : 342 Training_loss : 5739746000.0 Test_loss : 5770583600.0\n",
      "Epoch : 343 Training_loss : 5512508400.0 Test_loss : 5542721500.0\n",
      "Epoch : 344 Training_loss : 5294269000.0 Test_loss : 5323870000.0\n",
      "Epoch : 345 Training_loss : 5084673000.0 Test_loss : 5113673700.0\n",
      "Epoch : 346 Training_loss : 4883376000.0 Test_loss : 4911789600.0\n",
      "Epoch : 347 Training_loss : 4690051600.0 Test_loss : 4717888000.0\n",
      "Epoch : 348 Training_loss : 4504382000.0 Test_loss : 4531654700.0\n",
      "Epoch : 349 Training_loss : 4326064600.0 Test_loss : 4352784000.0\n",
      "Epoch : 350 Training_loss : 4154809300.0 Test_loss : 4180986000.0\n",
      "Epoch : 351 Training_loss : 3990335700.0 Test_loss : 4015980500.0\n",
      "Epoch : 352 Training_loss : 3832374800.0 Test_loss : 3857499100.0\n",
      "Epoch : 353 Training_loss : 3680669200.0 Test_loss : 3705283300.0\n",
      "Epoch : 354 Training_loss : 3534970600.0 Test_loss : 3559084800.0\n",
      "Epoch : 355 Training_loss : 3395042300.0 Test_loss : 3418665700.0\n",
      "Epoch : 356 Training_loss : 3260655000.0 Test_loss : 3283798500.0\n",
      "Epoch : 357 Training_loss : 3131589600.0 Test_loss : 3154262000.0\n",
      "Epoch : 358 Training_loss : 3007635200.0 Test_loss : 3029846500.0\n",
      "Epoch : 359 Training_loss : 2888589300.0 Test_loss : 2910348300.0\n",
      "Epoch : 360 Training_loss : 2774257400.0 Test_loss : 2795573800.0\n",
      "Epoch : 361 Training_loss : 2664453000.0 Test_loss : 2685335600.0\n",
      "Epoch : 362 Training_loss : 2558998000.0 Test_loss : 2579454000.0\n",
      "Epoch : 363 Training_loss : 2457717800.0 Test_loss : 2477757400.0\n",
      "Epoch : 364 Training_loss : 2360448800.0 Test_loss : 2380079400.0\n",
      "Epoch : 365 Training_loss : 2267031600.0 Test_loss : 2286261800.0\n",
      "Epoch : 366 Training_loss : 2177313800.0 Test_loss : 2196151600.0\n",
      "Epoch : 367 Training_loss : 2091148800.0 Test_loss : 2109601500.0\n",
      "Epoch : 368 Training_loss : 2008395600.0 Test_loss : 2026471800.0\n",
      "Epoch : 369 Training_loss : 1928919800.0 Test_loss : 1946626300.0\n",
      "Epoch : 370 Training_loss : 1852590800.0 Test_loss : 1869935700.0\n",
      "Epoch : 371 Training_loss : 1779284900.0 Test_loss : 1796274800.0\n",
      "Epoch : 372 Training_loss : 1708881700.0 Test_loss : 1725523800.0\n",
      "Epoch : 373 Training_loss : 1641266600.0 Test_loss : 1657567900.0\n",
      "Epoch : 374 Training_loss : 1576329100.0 Test_loss : 1592296600.0\n",
      "Epoch : 375 Training_loss : 1513962900.0 Test_loss : 1529603100.0\n",
      "Epoch : 376 Training_loss : 1454066400.0 Test_loss : 1469386000.0\n",
      "Epoch : 377 Training_loss : 1396542000.0 Test_loss : 1411547100.0\n",
      "Epoch : 378 Training_loss : 1341295400.0 Test_loss : 1355992700.0\n",
      "Epoch : 379 Training_loss : 1288236700.0 Test_loss : 1302632100.0\n",
      "Epoch : 380 Training_loss : 1237278800.0 Test_loss : 1251378600.0\n",
      "Epoch : 381 Training_loss : 1188339200.0 Test_loss : 1202149000.0\n",
      "Epoch : 382 Training_loss : 1141337500.0 Test_loss : 1154863100.0\n",
      "Epoch : 383 Training_loss : 1096197000.0 Test_loss : 1109444400.0\n",
      "Epoch : 384 Training_loss : 1052844200.0 Test_loss : 1065818560.0\n",
      "Epoch : 385 Training_loss : 1011207940.0 Test_loss : 1023915000.0\n",
      "Epoch : 386 Training_loss : 971220700.0 Test_loss : 983665660.0\n",
      "Epoch : 387 Training_loss : 932816830.0 Test_loss : 945005060.0\n",
      "Epoch : 388 Training_loss : 895933800.0 Test_loss : 907870400.0\n",
      "Epoch : 389 Training_loss : 860511360.0 Test_loss : 872201300.0\n",
      "Epoch : 390 Training_loss : 826491500.0 Test_loss : 837939840.0\n",
      "Epoch : 391 Training_loss : 793819100.0 Test_loss : 805030400.0\n",
      "Epoch : 392 Training_loss : 762440260.0 Test_loss : 773419500.0\n",
      "Epoch : 393 Training_loss : 732304260.0 Test_loss : 743056000.0\n",
      "Epoch : 394 Training_loss : 703361400.0 Test_loss : 713890300.0\n",
      "Epoch : 395 Training_loss : 675564740.0 Test_loss : 685875260.0\n",
      "Epoch : 396 Training_loss : 648868900.0 Test_loss : 658965250.0\n",
      "Epoch : 397 Training_loss : 623230200.0 Test_loss : 633116740.0\n",
      "Epoch : 398 Training_loss : 598606800.0 Test_loss : 608287740.0\n",
      "Epoch : 399 Training_loss : 574958500.0 Test_loss : 584437900.0\n",
      "Epoch : 400 Training_loss : 552246660.0 Test_loss : 561528600.0\n",
      "Epoch : 401 Training_loss : 530434140.0 Test_loss : 539522500.0\n",
      "Epoch : 402 Training_loss : 509485600.0 Test_loss : 518384220.0\n",
      "Epoch : 403 Training_loss : 489366400.0 Test_loss : 498079230.0\n",
      "Epoch : 404 Training_loss : 470043940.0 Test_loss : 478574620.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 405 Training_loss : 451486750.0 Test_loss : 459838980.0\n",
      "Epoch : 406 Training_loss : 433664420.0 Test_loss : 441841630.0\n",
      "Epoch : 407 Training_loss : 416547700.0 Test_loss : 424553600.0\n",
      "Epoch : 408 Training_loss : 400109000.0 Test_loss : 407946780.0\n",
      "Epoch : 409 Training_loss : 384321150.0 Test_loss : 391994340.0\n",
      "Epoch : 410 Training_loss : 369158560.0 Test_loss : 376670400.0\n",
      "Epoch : 411 Training_loss : 354596400.0 Test_loss : 361950080.0\n",
      "Epoch : 412 Training_loss : 340610940.0 Test_loss : 347809630.0\n",
      "Epoch : 413 Training_loss : 327179170.0 Test_loss : 334226050.0\n",
      "Epoch : 414 Training_loss : 314279400.0 Test_loss : 321177440.0\n",
      "Epoch : 415 Training_loss : 301890460.0 Test_loss : 308642620.0\n",
      "Epoch : 416 Training_loss : 289992060.0 Test_loss : 296601300.0\n",
      "Epoch : 417 Training_loss : 278564900.0 Test_loss : 285034080.0\n",
      "Epoch : 418 Training_loss : 267590220.0 Test_loss : 273922140.0\n",
      "Epoch : 419 Training_loss : 257050190.0 Test_loss : 263247550.0\n",
      "Epoch : 420 Training_loss : 246927490.0 Test_loss : 252993010.0\n",
      "Epoch : 421 Training_loss : 237205660.0 Test_loss : 243141970.0\n",
      "Epoch : 422 Training_loss : 227868830.0 Test_loss : 233678540.0\n",
      "Epoch : 423 Training_loss : 218901730.0 Test_loss : 224587330.0\n",
      "Epoch : 424 Training_loss : 210289730.0 Test_loss : 215853740.0\n",
      "Epoch : 425 Training_loss : 202018750.0 Test_loss : 207463580.0\n",
      "Epoch : 426 Training_loss : 194075330.0 Test_loss : 199403360.0\n",
      "Epoch : 427 Training_loss : 186446430.0 Test_loss : 191660030.0\n",
      "Epoch : 428 Training_loss : 179119630.0 Test_loss : 184221120.0\n",
      "Epoch : 429 Training_loss : 172083000.0 Test_loss : 177074540.0\n",
      "Epoch : 430 Training_loss : 165325060.0 Test_loss : 170208860.0\n",
      "Epoch : 431 Training_loss : 158834690.0 Test_loss : 163612910.0\n",
      "Epoch : 432 Training_loss : 152601310.0 Test_loss : 157276100.0\n",
      "Epoch : 433 Training_loss : 146614800.0 Test_loss : 151188210.0\n",
      "Epoch : 434 Training_loss : 140865380.0 Test_loss : 145339400.0\n",
      "Epoch : 435 Training_loss : 135343600.0 Test_loss : 139720260.0\n",
      "Epoch : 436 Training_loss : 130040504.0 Test_loss : 134321740.0\n",
      "Epoch : 437 Training_loss : 124947384.0 Test_loss : 129135110.0\n",
      "Epoch : 438 Training_loss : 120055990.0 Test_loss : 124152056.0\n",
      "Epoch : 439 Training_loss : 115358270.0 Test_loss : 119364530.0\n",
      "Epoch : 440 Training_loss : 110846584.0 Test_loss : 114764824.0\n",
      "Epoch : 441 Training_loss : 106513550.0 Test_loss : 110345544.0\n",
      "Epoch : 442 Training_loss : 102352130.0 Test_loss : 106099580.0\n",
      "Epoch : 443 Training_loss : 98355490.0 Test_loss : 102020120.0\n",
      "Epoch : 444 Training_loss : 94517140.0 Test_loss : 98100570.0\n",
      "Epoch : 445 Training_loss : 90830770.0 Test_loss : 94334650.0\n",
      "Epoch : 446 Training_loss : 87290370.0 Test_loss : 90716270.0\n",
      "Epoch : 447 Training_loss : 83890184.0 Test_loss : 87239690.0\n",
      "Epoch : 448 Training_loss : 80624640.0 Test_loss : 83899250.0\n",
      "Epoch : 449 Training_loss : 77488410.0 Test_loss : 80689640.0\n",
      "Epoch : 450 Training_loss : 74476380.0 Test_loss : 77605700.0\n",
      "Epoch : 451 Training_loss : 71583624.0 Test_loss : 74642460.0\n",
      "Epoch : 452 Training_loss : 68805416.0 Test_loss : 71795180.0\n",
      "Epoch : 453 Training_loss : 66137236.0 Test_loss : 69059320.0\n",
      "Epoch : 454 Training_loss : 63574716.0 Test_loss : 66430468.0\n",
      "Epoch : 455 Training_loss : 61113660.0 Test_loss : 63904412.0\n",
      "Epoch : 456 Training_loss : 58750076.0 Test_loss : 61477120.0\n",
      "Epoch : 457 Training_loss : 56480080.0 Test_loss : 59144690.0\n",
      "Epoch : 458 Training_loss : 54299964.0 Test_loss : 56903388.0\n",
      "Epoch : 459 Training_loss : 52206190.0 Test_loss : 54749652.0\n",
      "Epoch : 460 Training_loss : 50195324.0 Test_loss : 52680028.0\n",
      "Epoch : 461 Training_loss : 48264110.0 Test_loss : 50691224.0\n",
      "Epoch : 462 Training_loss : 46409360.0 Test_loss : 48780050.0\n",
      "Epoch : 463 Training_loss : 44628040.0 Test_loss : 46943424.0\n",
      "Epoch : 464 Training_loss : 42917270.0 Test_loss : 45178460.0\n",
      "Epoch : 465 Training_loss : 41274256.0 Test_loss : 43482330.0\n",
      "Epoch : 466 Training_loss : 39696292.0 Test_loss : 41852304.0\n",
      "Epoch : 467 Training_loss : 38180824.0 Test_loss : 40285828.0\n",
      "Epoch : 468 Training_loss : 36725360.0 Test_loss : 38780380.0\n",
      "Epoch : 469 Training_loss : 35327556.0 Test_loss : 37333580.0\n",
      "Epoch : 470 Training_loss : 33985080.0 Test_loss : 35943092.0\n",
      "Epoch : 471 Training_loss : 32695780.0 Test_loss : 34606744.0\n",
      "Epoch : 472 Training_loss : 31457520.0 Test_loss : 33322376.0\n",
      "Epoch : 473 Training_loss : 30268318.0 Test_loss : 32087982.0\n",
      "Epoch : 474 Training_loss : 29126202.0 Test_loss : 30901576.0\n",
      "Epoch : 475 Training_loss : 28029314.0 Test_loss : 29761290.0\n",
      "Epoch : 476 Training_loss : 26975862.0 Test_loss : 28665314.0\n",
      "Epoch : 477 Training_loss : 25964124.0 Test_loss : 27611894.0\n",
      "Epoch : 478 Training_loss : 24992448.0 Test_loss : 26599372.0\n",
      "Epoch : 479 Training_loss : 24059258.0 Test_loss : 25626152.0\n",
      "Epoch : 480 Training_loss : 23163024.0 Test_loss : 24690686.0\n",
      "Epoch : 481 Training_loss : 22302270.0 Test_loss : 23791490.0\n",
      "Epoch : 482 Training_loss : 21475604.0 Test_loss : 22927152.0\n",
      "Epoch : 483 Training_loss : 20681684.0 Test_loss : 22096308.0\n",
      "Epoch : 484 Training_loss : 19919198.0 Test_loss : 21297638.0\n",
      "Epoch : 485 Training_loss : 19186900.0 Test_loss : 20529882.0\n",
      "Epoch : 486 Training_loss : 18483602.0 Test_loss : 19791834.0\n",
      "Epoch : 487 Training_loss : 17808156.0 Test_loss : 19082330.0\n",
      "Epoch : 488 Training_loss : 17159462.0 Test_loss : 18400260.0\n",
      "Epoch : 489 Training_loss : 16536453.0 Test_loss : 17744546.0\n",
      "Epoch : 490 Training_loss : 15938112.0 Test_loss : 17114154.0\n",
      "Epoch : 491 Training_loss : 15363465.0 Test_loss : 16508093.0\n",
      "Epoch : 492 Training_loss : 14811571.0 Test_loss : 15925415.0\n",
      "Epoch : 493 Training_loss : 14281539.0 Test_loss : 15365215.0\n",
      "Epoch : 494 Training_loss : 13772493.0 Test_loss : 14826605.0\n",
      "Epoch : 495 Training_loss : 13283601.0 Test_loss : 14308740.0\n",
      "Epoch : 496 Training_loss : 12814072.0 Test_loss : 13810819.0\n",
      "Epoch : 497 Training_loss : 12363139.0 Test_loss : 13332056.0\n",
      "Epoch : 498 Training_loss : 11930057.0 Test_loss : 12871708.0\n",
      "Epoch : 499 Training_loss : 11514138.0 Test_loss : 12429061.0\n",
      "Epoch : 500 Training_loss : 11114678.0 Test_loss : 12003412.0\n",
      "Epoch : 501 Training_loss : 10731043.0 Test_loss : 11594113.0\n",
      "Epoch : 502 Training_loss : 10362595.0 Test_loss : 11200513.0\n",
      "Epoch : 503 Training_loss : 10008742.0 Test_loss : 10822008.0\n",
      "Epoch : 504 Training_loss : 9668899.0 Test_loss : 10458010.0\n",
      "Epoch : 505 Training_loss : 9342512.0 Test_loss : 10107951.0\n",
      "Epoch : 506 Training_loss : 9029052.0 Test_loss : 9771292.0\n",
      "Epoch : 507 Training_loss : 8728003.0 Test_loss : 9447506.0\n",
      "Epoch : 508 Training_loss : 8438879.0 Test_loss : 9136101.0\n",
      "Epoch : 509 Training_loss : 8161200.5 Test_loss : 8836587.0\n",
      "Epoch : 510 Training_loss : 7894525.5 Test_loss : 8548513.0\n",
      "Epoch : 511 Training_loss : 7638405.0 Test_loss : 8271422.0\n",
      "Epoch : 512 Training_loss : 7392429.0 Test_loss : 8004894.5\n",
      "Epoch : 513 Training_loss : 7156189.5 Test_loss : 7748515.0\n",
      "Epoch : 514 Training_loss : 6929313.5 Test_loss : 7501900.5\n",
      "Epoch : 515 Training_loss : 6711418.0 Test_loss : 7264662.5\n",
      "Epoch : 516 Training_loss : 6502149.0 Test_loss : 7036437.5\n",
      "Epoch : 517 Training_loss : 6301168.0 Test_loss : 6816880.5\n",
      "Epoch : 518 Training_loss : 6108142.5 Test_loss : 6605649.0\n",
      "Epoch : 519 Training_loss : 5922761.0 Test_loss : 6402425.0\n",
      "Epoch : 520 Training_loss : 5744726.0 Test_loss : 6206907.0\n",
      "Epoch : 521 Training_loss : 5573739.5 Test_loss : 6018786.0\n",
      "Epoch : 522 Training_loss : 5409520.5 Test_loss : 5837774.5\n",
      "Epoch : 523 Training_loss : 5251804.0 Test_loss : 5663601.5\n",
      "Epoch : 524 Training_loss : 5100334.5 Test_loss : 5496005.0\n",
      "Epoch : 525 Training_loss : 4954867.0 Test_loss : 5334734.0\n",
      "Epoch : 526 Training_loss : 4815156.5 Test_loss : 5179534.5\n",
      "Epoch : 527 Training_loss : 4680980.5 Test_loss : 5030180.5\n",
      "Epoch : 528 Training_loss : 4552118.0 Test_loss : 4886442.5\n",
      "Epoch : 529 Training_loss : 4428356.0 Test_loss : 4748103.0\n",
      "Epoch : 530 Training_loss : 4309494.5 Test_loss : 4614954.5\n",
      "Epoch : 531 Training_loss : 4195341.0 Test_loss : 4486800.5\n",
      "Epoch : 532 Training_loss : 4085709.0 Test_loss : 4363449.0\n",
      "Epoch : 533 Training_loss : 3980420.8 Test_loss : 4244715.0\n",
      "Epoch : 534 Training_loss : 3879301.8 Test_loss : 4130419.8\n",
      "Epoch : 535 Training_loss : 3782185.5 Test_loss : 4020390.2\n",
      "Epoch : 536 Training_loss : 3688917.8 Test_loss : 3914466.8\n",
      "Epoch : 537 Training_loss : 3599340.8 Test_loss : 3812488.5\n",
      "Epoch : 538 Training_loss : 3513310.8 Test_loss : 3714304.2\n",
      "Epoch : 539 Training_loss : 3430688.8 Test_loss : 3619771.2\n",
      "Epoch : 540 Training_loss : 3351338.2 Test_loss : 3528749.2\n",
      "Epoch : 541 Training_loss : 3275127.0 Test_loss : 3441097.5\n",
      "Epoch : 542 Training_loss : 3201937.5 Test_loss : 3356698.2\n",
      "Epoch : 543 Training_loss : 3131644.5 Test_loss : 3275418.2\n",
      "Epoch : 544 Training_loss : 3064135.0 Test_loss : 3197143.0\n",
      "Epoch : 545 Training_loss : 2999297.2 Test_loss : 3121753.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 546 Training_loss : 2937027.2 Test_loss : 3049143.2\n",
      "Epoch : 547 Training_loss : 2877223.8 Test_loss : 2979206.2\n",
      "Epoch : 548 Training_loss : 2819790.0 Test_loss : 2911842.0\n",
      "Epoch : 549 Training_loss : 2764627.5 Test_loss : 2846947.2\n",
      "Epoch : 550 Training_loss : 2711651.2 Test_loss : 2784433.0\n",
      "Epoch : 551 Training_loss : 2660772.8 Test_loss : 2724208.0\n",
      "Epoch : 552 Training_loss : 2611908.5 Test_loss : 2666183.8\n",
      "Epoch : 553 Training_loss : 2564982.0 Test_loss : 2610280.8\n",
      "Epoch : 554 Training_loss : 2519913.5 Test_loss : 2556415.5\n",
      "Epoch : 555 Training_loss : 2476627.5 Test_loss : 2504508.5\n",
      "Epoch : 556 Training_loss : 2435055.8 Test_loss : 2454487.5\n",
      "Epoch : 557 Training_loss : 2395130.8 Test_loss : 2406283.2\n",
      "Epoch : 558 Training_loss : 2356788.2 Test_loss : 2359826.2\n",
      "Epoch : 559 Training_loss : 2319962.0 Test_loss : 2315048.0\n",
      "Epoch : 560 Training_loss : 2284595.8 Test_loss : 2271889.5\n",
      "Epoch : 561 Training_loss : 2250628.2 Test_loss : 2230284.8\n",
      "Epoch : 562 Training_loss : 2218007.0 Test_loss : 2190179.2\n",
      "Epoch : 563 Training_loss : 2186678.0 Test_loss : 2151515.8\n",
      "Epoch : 564 Training_loss : 2156588.8 Test_loss : 2114239.0\n",
      "Epoch : 565 Training_loss : 2127692.8 Test_loss : 2078299.2\n",
      "Epoch : 566 Training_loss : 2099938.2 Test_loss : 2043641.4\n",
      "Epoch : 567 Training_loss : 2073284.6 Test_loss : 2010222.2\n",
      "Epoch : 568 Training_loss : 2047686.1 Test_loss : 1977994.2\n",
      "Epoch : 569 Training_loss : 2023102.5 Test_loss : 1946913.6\n",
      "Epoch : 570 Training_loss : 1999491.4 Test_loss : 1916934.9\n",
      "Epoch : 571 Training_loss : 1976814.5 Test_loss : 1888018.2\n",
      "Epoch : 572 Training_loss : 1955036.0 Test_loss : 1860124.4\n",
      "Epoch : 573 Training_loss : 1934120.4 Test_loss : 1833215.9\n",
      "Epoch : 574 Training_loss : 1914033.0 Test_loss : 1807255.9\n",
      "Epoch : 575 Training_loss : 1894741.6 Test_loss : 1782209.4\n",
      "Epoch : 576 Training_loss : 1876214.4 Test_loss : 1758042.0\n",
      "Epoch : 577 Training_loss : 1858420.8 Test_loss : 1734721.0\n",
      "Epoch : 578 Training_loss : 1841330.5 Test_loss : 1712214.0\n",
      "Epoch : 579 Training_loss : 1824918.8 Test_loss : 1690493.5\n",
      "Epoch : 580 Training_loss : 1809154.5 Test_loss : 1669526.6\n",
      "Epoch : 581 Training_loss : 1794016.6 Test_loss : 1649290.5\n",
      "Epoch : 582 Training_loss : 1779476.6 Test_loss : 1629753.8\n",
      "Epoch : 583 Training_loss : 1765512.4 Test_loss : 1610892.5\n",
      "Epoch : 584 Training_loss : 1752101.6 Test_loss : 1592683.5\n",
      "Epoch : 585 Training_loss : 1739221.8 Test_loss : 1575100.8\n",
      "Epoch : 586 Training_loss : 1726852.5 Test_loss : 1558122.9\n",
      "Epoch : 587 Training_loss : 1714971.9 Test_loss : 1541725.4\n",
      "Epoch : 588 Training_loss : 1703563.5 Test_loss : 1525891.1\n",
      "Epoch : 589 Training_loss : 1692605.0 Test_loss : 1510594.9\n",
      "Epoch : 590 Training_loss : 1682081.0 Test_loss : 1495819.9\n",
      "Epoch : 591 Training_loss : 1671974.4 Test_loss : 1481547.8\n",
      "Epoch : 592 Training_loss : 1662267.4 Test_loss : 1467758.0\n",
      "Epoch : 593 Training_loss : 1652945.9 Test_loss : 1454435.8\n",
      "Epoch : 594 Training_loss : 1643992.6 Test_loss : 1441561.9\n",
      "Epoch : 595 Training_loss : 1635393.2 Test_loss : 1429119.5\n",
      "Epoch : 596 Training_loss : 1627135.6 Test_loss : 1417096.6\n",
      "Epoch : 597 Training_loss : 1619204.0 Test_loss : 1405474.5\n",
      "Epoch : 598 Training_loss : 1611587.2 Test_loss : 1394241.1\n",
      "Epoch : 599 Training_loss : 1604272.0 Test_loss : 1383382.1\n",
      "Epoch : 600 Training_loss : 1597246.9 Test_loss : 1372883.6\n",
      "Epoch : 601 Training_loss : 1590499.5 Test_loss : 1362732.1\n",
      "Epoch : 602 Training_loss : 1584019.9 Test_loss : 1352916.9\n",
      "Epoch : 603 Training_loss : 1577795.2 Test_loss : 1343423.2\n",
      "Epoch : 604 Training_loss : 1571818.5 Test_loss : 1334243.1\n",
      "Epoch : 605 Training_loss : 1566077.6 Test_loss : 1325362.6\n",
      "Epoch : 606 Training_loss : 1560564.5 Test_loss : 1316772.2\n",
      "Epoch : 607 Training_loss : 1555270.1 Test_loss : 1308463.1\n",
      "Epoch : 608 Training_loss : 1550185.6 Test_loss : 1300424.1\n",
      "Epoch : 609 Training_loss : 1545301.9 Test_loss : 1292644.8\n",
      "Epoch : 610 Training_loss : 1540611.5 Test_loss : 1285116.8\n",
      "Epoch : 611 Training_loss : 1536106.8 Test_loss : 1277830.2\n",
      "Epoch : 612 Training_loss : 1531779.9 Test_loss : 1270777.6\n",
      "Epoch : 613 Training_loss : 1527624.8 Test_loss : 1263951.5\n",
      "Epoch : 614 Training_loss : 1523634.1 Test_loss : 1257342.6\n",
      "Epoch : 615 Training_loss : 1519801.5 Test_loss : 1250944.4\n",
      "Epoch : 616 Training_loss : 1516120.4 Test_loss : 1244749.6\n",
      "Epoch : 617 Training_loss : 1512586.1 Test_loss : 1238751.9\n",
      "Epoch : 618 Training_loss : 1509191.6 Test_loss : 1232943.0\n",
      "Epoch : 619 Training_loss : 1505931.2 Test_loss : 1227317.0\n",
      "Epoch : 620 Training_loss : 1502800.4 Test_loss : 1221867.9\n",
      "Epoch : 621 Training_loss : 1499792.8 Test_loss : 1216587.9\n",
      "Epoch : 622 Training_loss : 1496905.2 Test_loss : 1211473.6\n",
      "Epoch : 623 Training_loss : 1494131.6 Test_loss : 1206517.6\n",
      "Epoch : 624 Training_loss : 1491467.6 Test_loss : 1201714.9\n",
      "Epoch : 625 Training_loss : 1488909.1 Test_loss : 1197059.9\n",
      "Epoch : 626 Training_loss : 1486451.5 Test_loss : 1192548.2\n",
      "Epoch : 627 Training_loss : 1484091.4 Test_loss : 1188175.0\n",
      "Epoch : 628 Training_loss : 1481825.0 Test_loss : 1183935.5\n",
      "Epoch : 629 Training_loss : 1479647.9 Test_loss : 1179825.0\n",
      "Epoch : 630 Training_loss : 1477557.0 Test_loss : 1175839.6\n",
      "Epoch : 631 Training_loss : 1475549.1 Test_loss : 1171974.6\n",
      "Epoch : 632 Training_loss : 1473620.6 Test_loss : 1168226.2\n",
      "Epoch : 633 Training_loss : 1471769.2 Test_loss : 1164591.9\n",
      "Epoch : 634 Training_loss : 1469990.8 Test_loss : 1161066.0\n",
      "Epoch : 635 Training_loss : 1468282.2 Test_loss : 1157644.8\n",
      "Epoch : 636 Training_loss : 1466641.9 Test_loss : 1154326.4\n",
      "Epoch : 637 Training_loss : 1465067.0 Test_loss : 1151106.9\n",
      "Epoch : 638 Training_loss : 1463553.6 Test_loss : 1147981.5\n",
      "Epoch : 639 Training_loss : 1462100.0 Test_loss : 1144948.1\n",
      "Epoch : 640 Training_loss : 1460704.8 Test_loss : 1142005.0\n",
      "Epoch : 641 Training_loss : 1459364.5 Test_loss : 1139147.5\n",
      "Epoch : 642 Training_loss : 1458077.5 Test_loss : 1136373.8\n",
      "Epoch : 643 Training_loss : 1456841.0 Test_loss : 1133681.0\n",
      "Epoch : 644 Training_loss : 1455653.9 Test_loss : 1131066.0\n",
      "Epoch : 645 Training_loss : 1454513.2 Test_loss : 1128525.9\n",
      "Epoch : 646 Training_loss : 1453418.2 Test_loss : 1126059.4\n",
      "Epoch : 647 Training_loss : 1452366.4 Test_loss : 1123663.2\n",
      "Epoch : 648 Training_loss : 1451356.5 Test_loss : 1121337.0\n",
      "Epoch : 649 Training_loss : 1450386.2 Test_loss : 1119075.9\n",
      "Epoch : 650 Training_loss : 1449454.1 Test_loss : 1116878.9\n",
      "Epoch : 651 Training_loss : 1448559.6 Test_loss : 1114745.0\n",
      "Epoch : 652 Training_loss : 1447700.0 Test_loss : 1112670.2\n",
      "Epoch : 653 Training_loss : 1446874.8 Test_loss : 1110654.8\n",
      "Epoch : 654 Training_loss : 1446082.2 Test_loss : 1108695.1\n",
      "Epoch : 655 Training_loss : 1445320.6 Test_loss : 1106790.2\n",
      "Epoch : 656 Training_loss : 1444589.5 Test_loss : 1104939.1\n",
      "Epoch : 657 Training_loss : 1443887.2 Test_loss : 1103138.5\n",
      "Epoch : 658 Training_loss : 1443213.1 Test_loss : 1101388.4\n",
      "Epoch : 659 Training_loss : 1442565.1 Test_loss : 1099685.5\n",
      "Epoch : 660 Training_loss : 1441943.4 Test_loss : 1098030.5\n",
      "Epoch : 661 Training_loss : 1441345.8 Test_loss : 1096420.0\n",
      "Epoch : 662 Training_loss : 1440771.9 Test_loss : 1094853.1\n",
      "Epoch : 663 Training_loss : 1440220.8 Test_loss : 1093329.4\n",
      "Epoch : 664 Training_loss : 1439691.4 Test_loss : 1091846.6\n",
      "Epoch : 665 Training_loss : 1439183.4 Test_loss : 1090404.4\n",
      "Epoch : 666 Training_loss : 1438695.0 Test_loss : 1089000.6\n",
      "Epoch : 667 Training_loss : 1438226.5 Test_loss : 1087634.8\n",
      "Epoch : 668 Training_loss : 1437776.4 Test_loss : 1086305.1\n",
      "Epoch : 669 Training_loss : 1437344.0 Test_loss : 1085011.1\n",
      "Epoch : 670 Training_loss : 1436928.5 Test_loss : 1083751.0\n",
      "Epoch : 671 Training_loss : 1436529.5 Test_loss : 1082524.2\n",
      "Epoch : 672 Training_loss : 1436146.5 Test_loss : 1081330.4\n",
      "Epoch : 673 Training_loss : 1435778.5 Test_loss : 1080167.6\n",
      "Epoch : 674 Training_loss : 1435425.6 Test_loss : 1079035.9\n",
      "Epoch : 675 Training_loss : 1435086.1 Test_loss : 1077933.1\n",
      "Epoch : 676 Training_loss : 1434760.1 Test_loss : 1076859.1\n",
      "Epoch : 677 Training_loss : 1434447.4 Test_loss : 1075813.4\n",
      "Epoch : 678 Training_loss : 1434146.9 Test_loss : 1074794.8\n",
      "Epoch : 679 Training_loss : 1433858.2 Test_loss : 1073802.2\n",
      "Epoch : 680 Training_loss : 1433581.1 Test_loss : 1072834.9\n",
      "Epoch : 681 Training_loss : 1433314.8 Test_loss : 1071892.1\n",
      "Epoch : 682 Training_loss : 1433059.0 Test_loss : 1070973.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 683 Training_loss : 1432813.6 Test_loss : 1070079.1\n",
      "Epoch : 684 Training_loss : 1432577.6 Test_loss : 1069207.1\n",
      "Epoch : 685 Training_loss : 1432351.4 Test_loss : 1068357.6\n",
      "Epoch : 686 Training_loss : 1432133.9 Test_loss : 1067528.8\n",
      "Epoch : 687 Training_loss : 1431924.8 Test_loss : 1066720.8\n",
      "Epoch : 688 Training_loss : 1431724.0 Test_loss : 1065933.0\n",
      "Epoch : 689 Training_loss : 1431531.5 Test_loss : 1065165.4\n",
      "Epoch : 690 Training_loss : 1431346.2 Test_loss : 1064416.6\n",
      "Epoch : 691 Training_loss : 1431168.6 Test_loss : 1063686.5\n",
      "Epoch : 692 Training_loss : 1430998.1 Test_loss : 1062974.8\n",
      "Epoch : 693 Training_loss : 1430834.1 Test_loss : 1062280.1\n",
      "Epoch : 694 Training_loss : 1430676.5 Test_loss : 1061602.2\n",
      "Epoch : 695 Training_loss : 1430525.1 Test_loss : 1060941.4\n",
      "Epoch : 696 Training_loss : 1430380.0 Test_loss : 1060296.9\n",
      "Epoch : 697 Training_loss : 1430240.5 Test_loss : 1059667.8\n",
      "Epoch : 698 Training_loss : 1430106.2 Test_loss : 1059053.8\n",
      "Epoch : 699 Training_loss : 1429977.8 Test_loss : 1058454.8\n",
      "Epoch : 700 Training_loss : 1429854.2 Test_loss : 1057870.6\n",
      "Epoch : 701 Training_loss : 1429735.4 Test_loss : 1057300.0\n",
      "Epoch : 702 Training_loss : 1429621.6 Test_loss : 1056744.1\n",
      "Epoch : 703 Training_loss : 1429512.4 Test_loss : 1056201.4\n",
      "Epoch : 704 Training_loss : 1429407.0 Test_loss : 1055671.0\n",
      "Epoch : 705 Training_loss : 1429305.9 Test_loss : 1055153.8\n",
      "Epoch : 706 Training_loss : 1429209.0 Test_loss : 1054648.5\n",
      "Epoch : 707 Training_loss : 1429115.9 Test_loss : 1054155.1\n",
      "Epoch : 708 Training_loss : 1429026.2 Test_loss : 1053673.5\n",
      "Epoch : 709 Training_loss : 1428940.5 Test_loss : 1053203.4\n",
      "Epoch : 710 Training_loss : 1428858.1 Test_loss : 1052745.0\n",
      "Epoch : 711 Training_loss : 1428778.9 Test_loss : 1052296.9\n",
      "Epoch : 712 Training_loss : 1428702.8 Test_loss : 1051859.2\n",
      "Epoch : 713 Training_loss : 1428629.6 Test_loss : 1051431.9\n",
      "Epoch : 714 Training_loss : 1428559.6 Test_loss : 1051014.6\n",
      "Epoch : 715 Training_loss : 1428492.1 Test_loss : 1050607.5\n",
      "Epoch : 716 Training_loss : 1428427.5 Test_loss : 1050209.4\n",
      "Epoch : 717 Training_loss : 1428365.1 Test_loss : 1049820.2\n",
      "Epoch : 718 Training_loss : 1428305.5 Test_loss : 1049440.1\n",
      "Epoch : 719 Training_loss : 1428248.0 Test_loss : 1049068.5\n",
      "Epoch : 720 Training_loss : 1428193.0 Test_loss : 1048705.8\n",
      "Epoch : 721 Training_loss : 1428140.1 Test_loss : 1048351.8\n",
      "Epoch : 722 Training_loss : 1428089.2 Test_loss : 1048005.5\n",
      "Epoch : 723 Training_loss : 1428040.6 Test_loss : 1047667.6\n",
      "Epoch : 724 Training_loss : 1427993.6 Test_loss : 1047337.44\n",
      "Epoch : 725 Training_loss : 1427949.0 Test_loss : 1047014.56\n",
      "Epoch : 726 Training_loss : 1427905.6 Test_loss : 1046699.2\n",
      "Epoch : 727 Training_loss : 1427864.1 Test_loss : 1046391.2\n",
      "Epoch : 728 Training_loss : 1427824.5 Test_loss : 1046089.6\n",
      "Epoch : 729 Training_loss : 1427786.1 Test_loss : 1045795.3\n",
      "Epoch : 730 Training_loss : 1427749.1 Test_loss : 1045507.3\n",
      "Epoch : 731 Training_loss : 1427713.9 Test_loss : 1045225.56\n",
      "Epoch : 732 Training_loss : 1427680.1 Test_loss : 1044950.1\n",
      "Epoch : 733 Training_loss : 1427647.5 Test_loss : 1044680.8\n",
      "Epoch : 734 Training_loss : 1427616.0 Test_loss : 1044417.6\n",
      "Epoch : 735 Training_loss : 1427586.2 Test_loss : 1044160.4\n",
      "Epoch : 736 Training_loss : 1427557.5 Test_loss : 1043909.3\n",
      "Epoch : 737 Training_loss : 1427529.6 Test_loss : 1043663.3\n",
      "Epoch : 738 Training_loss : 1427503.2 Test_loss : 1043423.44\n",
      "Epoch : 739 Training_loss : 1427477.5 Test_loss : 1043188.56\n",
      "Epoch : 740 Training_loss : 1427453.1 Test_loss : 1042958.7\n",
      "Epoch : 741 Training_loss : 1427429.2 Test_loss : 1042733.8\n",
      "Epoch : 742 Training_loss : 1427406.8 Test_loss : 1042513.8\n",
      "Epoch : 743 Training_loss : 1427385.1 Test_loss : 1042298.75\n",
      "Epoch : 744 Training_loss : 1427364.0 Test_loss : 1042088.6\n",
      "Epoch : 745 Training_loss : 1427344.4 Test_loss : 1041883.4\n",
      "Epoch : 746 Training_loss : 1427325.0 Test_loss : 1041682.2\n",
      "Epoch : 747 Training_loss : 1427306.4 Test_loss : 1041485.8\n",
      "Epoch : 748 Training_loss : 1427288.6 Test_loss : 1041293.25\n",
      "Epoch : 749 Training_loss : 1427271.4 Test_loss : 1041104.7\n",
      "Epoch : 750 Training_loss : 1427255.0 Test_loss : 1040920.9\n",
      "Epoch : 751 Training_loss : 1427239.4 Test_loss : 1040740.9\n",
      "Epoch : 752 Training_loss : 1427224.4 Test_loss : 1040564.7\n",
      "Epoch : 753 Training_loss : 1427209.6 Test_loss : 1040392.25\n",
      "Epoch : 754 Training_loss : 1427195.9 Test_loss : 1040223.75\n",
      "Epoch : 755 Training_loss : 1427182.5 Test_loss : 1040058.9\n",
      "Epoch : 756 Training_loss : 1427169.6 Test_loss : 1039897.1\n",
      "Epoch : 757 Training_loss : 1427157.1 Test_loss : 1039739.06\n",
      "Epoch : 758 Training_loss : 1427145.5 Test_loss : 1039584.6\n",
      "Epoch : 759 Training_loss : 1427133.9 Test_loss : 1039433.1\n",
      "Epoch : 760 Training_loss : 1427123.0 Test_loss : 1039285.25\n",
      "Epoch : 761 Training_loss : 1427112.5 Test_loss : 1039140.3\n",
      "Epoch : 762 Training_loss : 1427102.5 Test_loss : 1038998.9\n",
      "Epoch : 763 Training_loss : 1427092.8 Test_loss : 1038860.4\n",
      "Epoch : 764 Training_loss : 1427083.5 Test_loss : 1038724.8\n",
      "Epoch : 765 Training_loss : 1427074.5 Test_loss : 1038591.75\n",
      "Epoch : 766 Training_loss : 1427065.9 Test_loss : 1038461.5\n",
      "Epoch : 767 Training_loss : 1427057.8 Test_loss : 1038334.1\n",
      "Epoch : 768 Training_loss : 1427049.8 Test_loss : 1038209.44\n",
      "Epoch : 769 Training_loss : 1427042.2 Test_loss : 1038087.44\n",
      "Epoch : 770 Training_loss : 1427034.5 Test_loss : 1037968.25\n",
      "Epoch : 771 Training_loss : 1427027.6 Test_loss : 1037851.75\n",
      "Epoch : 772 Training_loss : 1427021.2 Test_loss : 1037737.0\n",
      "Epoch : 773 Training_loss : 1427014.5 Test_loss : 1037625.0\n",
      "Epoch : 774 Training_loss : 1427008.5 Test_loss : 1037515.75\n",
      "Epoch : 775 Training_loss : 1427002.5 Test_loss : 1037408.3\n",
      "Epoch : 776 Training_loss : 1426996.5 Test_loss : 1037303.6\n",
      "Epoch : 777 Training_loss : 1426991.4 Test_loss : 1037200.56\n",
      "Epoch : 778 Training_loss : 1426985.8 Test_loss : 1037100.3\n",
      "Epoch : 779 Training_loss : 1426980.8 Test_loss : 1037001.9\n",
      "Epoch : 780 Training_loss : 1426975.9 Test_loss : 1036905.2\n",
      "Epoch : 781 Training_loss : 1426971.4 Test_loss : 1036811.2\n",
      "Epoch : 782 Training_loss : 1426966.8 Test_loss : 1036718.9\n",
      "Epoch : 783 Training_loss : 1426962.5 Test_loss : 1036628.4\n",
      "Epoch : 784 Training_loss : 1426958.5 Test_loss : 1036539.7\n",
      "Epoch : 785 Training_loss : 1426954.2 Test_loss : 1036452.7\n",
      "Epoch : 786 Training_loss : 1426950.5 Test_loss : 1036367.6\n",
      "Epoch : 787 Training_loss : 1426946.9 Test_loss : 1036284.2\n",
      "Epoch : 788 Training_loss : 1426943.2 Test_loss : 1036202.44\n",
      "Epoch : 789 Training_loss : 1426940.0 Test_loss : 1036122.7\n",
      "Epoch : 790 Training_loss : 1426936.5 Test_loss : 1036044.4\n",
      "Epoch : 791 Training_loss : 1426933.5 Test_loss : 1035968.06\n",
      "Epoch : 792 Training_loss : 1426930.5 Test_loss : 1035893.3\n",
      "Epoch : 793 Training_loss : 1426927.6 Test_loss : 1035820.5\n",
      "Epoch : 794 Training_loss : 1426924.8 Test_loss : 1035748.5\n",
      "Epoch : 795 Training_loss : 1426922.1 Test_loss : 1035678.1\n",
      "Epoch : 796 Training_loss : 1426919.6 Test_loss : 1035609.56\n",
      "Epoch : 797 Training_loss : 1426917.2 Test_loss : 1035541.9\n",
      "Epoch : 798 Training_loss : 1426914.9 Test_loss : 1035476.0\n",
      "Epoch : 799 Training_loss : 1426912.6 Test_loss : 1035411.6\n",
      "Epoch : 800 Training_loss : 1426910.4 Test_loss : 1035348.3\n",
      "Epoch : 801 Training_loss : 1426908.4 Test_loss : 1035286.56\n",
      "Epoch : 802 Training_loss : 1426906.4 Test_loss : 1035225.8\n",
      "Epoch : 803 Training_loss : 1426904.4 Test_loss : 1035166.7\n",
      "Epoch : 804 Training_loss : 1426902.5 Test_loss : 1035108.5\n",
      "Epoch : 805 Training_loss : 1426900.6 Test_loss : 1035051.1\n",
      "Epoch : 806 Training_loss : 1426899.2 Test_loss : 1034995.5\n",
      "Epoch : 807 Training_loss : 1426897.6 Test_loss : 1034940.7\n",
      "Epoch : 808 Training_loss : 1426896.0 Test_loss : 1034887.6\n",
      "Epoch : 809 Training_loss : 1426894.2 Test_loss : 1034835.4\n",
      "Epoch : 810 Training_loss : 1426893.0 Test_loss : 1034784.06\n",
      "Epoch : 811 Training_loss : 1426891.5 Test_loss : 1034733.5\n",
      "Epoch : 812 Training_loss : 1426890.2 Test_loss : 1034684.7\n",
      "Epoch : 813 Training_loss : 1426889.0 Test_loss : 1034636.7\n",
      "Epoch : 814 Training_loss : 1426887.9 Test_loss : 1034589.5\n",
      "Epoch : 815 Training_loss : 1426886.4 Test_loss : 1034543.2\n",
      "Epoch : 816 Training_loss : 1426885.5 Test_loss : 1034497.8\n",
      "Epoch : 817 Training_loss : 1426884.5 Test_loss : 1034453.25\n",
      "Epoch : 818 Training_loss : 1426883.0 Test_loss : 1034409.56\n",
      "Epoch : 819 Training_loss : 1426882.4 Test_loss : 1034366.56\n",
      "Epoch : 820 Training_loss : 1426881.2 Test_loss : 1034324.56\n",
      "Epoch : 821 Training_loss : 1426880.4 Test_loss : 1034283.3\n",
      "Epoch : 822 Training_loss : 1426879.5 Test_loss : 1034243.06\n",
      "Epoch : 823 Training_loss : 1426878.4 Test_loss : 1034203.5\n",
      "Epoch : 824 Training_loss : 1426877.8 Test_loss : 1034164.9\n",
      "Epoch : 825 Training_loss : 1426877.1 Test_loss : 1034127.0\n",
      "Epoch : 826 Training_loss : 1426876.1 Test_loss : 1034089.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 827 Training_loss : 1426875.5 Test_loss : 1034053.8\n",
      "Epoch : 828 Training_loss : 1426874.5 Test_loss : 1034018.5\n",
      "Epoch : 829 Training_loss : 1426874.2 Test_loss : 1033984.0\n",
      "Epoch : 830 Training_loss : 1426873.5 Test_loss : 1033950.4\n",
      "Epoch : 831 Training_loss : 1426872.8 Test_loss : 1033917.6\n",
      "Epoch : 832 Training_loss : 1426872.4 Test_loss : 1033884.7\n",
      "Epoch : 833 Training_loss : 1426871.5 Test_loss : 1033852.7\n",
      "Epoch : 834 Training_loss : 1426871.0 Test_loss : 1033821.6\n",
      "Epoch : 835 Training_loss : 1426870.5 Test_loss : 1033791.3\n",
      "Epoch : 836 Training_loss : 1426870.1 Test_loss : 1033761.0\n",
      "Epoch : 837 Training_loss : 1426869.8 Test_loss : 1033731.6\n",
      "Epoch : 838 Training_loss : 1426869.1 Test_loss : 1033702.9\n",
      "Epoch : 839 Training_loss : 1426868.8 Test_loss : 1033675.06\n",
      "Epoch : 840 Training_loss : 1426868.1 Test_loss : 1033647.3\n",
      "Epoch : 841 Training_loss : 1426867.9 Test_loss : 1033620.25\n",
      "Epoch : 842 Training_loss : 1426867.4 Test_loss : 1033594.06\n",
      "Epoch : 843 Training_loss : 1426867.2 Test_loss : 1033568.0\n",
      "Epoch : 844 Training_loss : 1426866.8 Test_loss : 1033542.7\n",
      "Epoch : 845 Training_loss : 1426866.2 Test_loss : 1033517.25\n",
      "Epoch : 846 Training_loss : 1426865.9 Test_loss : 1033492.8\n",
      "Epoch : 847 Training_loss : 1426865.8 Test_loss : 1033469.2\n",
      "Epoch : 848 Training_loss : 1426865.5 Test_loss : 1033445.5\n",
      "Epoch : 849 Training_loss : 1426865.1 Test_loss : 1033422.56\n",
      "Epoch : 850 Training_loss : 1426864.8 Test_loss : 1033399.75\n",
      "Epoch : 851 Training_loss : 1426864.6 Test_loss : 1033377.6\n",
      "Epoch : 852 Training_loss : 1426864.1 Test_loss : 1033356.6\n",
      "Epoch : 853 Training_loss : 1426863.9 Test_loss : 1033335.3\n",
      "Epoch : 854 Training_loss : 1426863.6 Test_loss : 1033315.0\n",
      "Epoch : 855 Training_loss : 1426863.4 Test_loss : 1033294.56\n",
      "Epoch : 856 Training_loss : 1426863.2 Test_loss : 1033275.06\n",
      "Epoch : 857 Training_loss : 1426863.0 Test_loss : 1033255.5\n",
      "Epoch : 858 Training_loss : 1426862.9 Test_loss : 1033236.8\n",
      "Epoch : 859 Training_loss : 1426862.5 Test_loss : 1033218.1\n",
      "Epoch : 860 Training_loss : 1426862.4 Test_loss : 1033199.4\n",
      "Epoch : 861 Training_loss : 1426862.2 Test_loss : 1033181.44\n",
      "Epoch : 862 Training_loss : 1426862.1 Test_loss : 1033163.6\n",
      "Epoch : 863 Training_loss : 1426862.1 Test_loss : 1033146.5\n",
      "Epoch : 864 Training_loss : 1426861.6 Test_loss : 1033129.44\n",
      "Epoch : 865 Training_loss : 1426861.6 Test_loss : 1033113.2\n",
      "Epoch : 866 Training_loss : 1426861.5 Test_loss : 1033096.94\n",
      "Epoch : 867 Training_loss : 1426861.5 Test_loss : 1033080.6\n",
      "Epoch : 868 Training_loss : 1426861.1 Test_loss : 1033065.25\n",
      "Epoch : 869 Training_loss : 1426861.1 Test_loss : 1033049.75\n",
      "Epoch : 870 Training_loss : 1426860.8 Test_loss : 1033035.1\n",
      "Epoch : 871 Training_loss : 1426860.8 Test_loss : 1033020.5\n",
      "Epoch : 872 Training_loss : 1426860.5 Test_loss : 1033005.94\n",
      "Epoch : 873 Training_loss : 1426860.6 Test_loss : 1032992.2\n",
      "Epoch : 874 Training_loss : 1426860.5 Test_loss : 1032978.4\n",
      "Epoch : 875 Training_loss : 1426860.4 Test_loss : 1032964.56\n",
      "Epoch : 876 Training_loss : 1426860.4 Test_loss : 1032951.44\n",
      "Epoch : 877 Training_loss : 1426860.1 Test_loss : 1032938.5\n",
      "Epoch : 878 Training_loss : 1426859.9 Test_loss : 1032925.56\n",
      "Epoch : 879 Training_loss : 1426859.9 Test_loss : 1032913.5\n",
      "Epoch : 880 Training_loss : 1426860.0 Test_loss : 1032901.3\n",
      "Epoch : 881 Training_loss : 1426859.9 Test_loss : 1032889.0\n",
      "Epoch : 882 Training_loss : 1426859.5 Test_loss : 1032877.75\n",
      "Epoch : 883 Training_loss : 1426859.5 Test_loss : 1032866.4\n",
      "Epoch : 884 Training_loss : 1426859.4 Test_loss : 1032855.1\n",
      "Epoch : 885 Training_loss : 1426859.5 Test_loss : 1032843.6\n",
      "Epoch : 886 Training_loss : 1426859.5 Test_loss : 1032833.2\n",
      "Epoch : 887 Training_loss : 1426859.2 Test_loss : 1032822.7\n",
      "Epoch : 888 Training_loss : 1426859.2 Test_loss : 1032812.06\n",
      "Epoch : 889 Training_loss : 1426859.2 Test_loss : 1032801.6\n",
      "Epoch : 890 Training_loss : 1426859.4 Test_loss : 1032791.8\n",
      "Epoch : 891 Training_loss : 1426859.0 Test_loss : 1032782.1\n",
      "Epoch : 892 Training_loss : 1426859.2 Test_loss : 1032772.4\n",
      "Epoch : 893 Training_loss : 1426859.0 Test_loss : 1032762.7\n",
      "Epoch : 894 Training_loss : 1426858.9 Test_loss : 1032753.8\n",
      "Epoch : 895 Training_loss : 1426859.0 Test_loss : 1032744.9\n",
      "Epoch : 896 Training_loss : 1426859.0 Test_loss : 1032735.94\n",
      "Epoch : 897 Training_loss : 1426858.9 Test_loss : 1032727.06\n",
      "Epoch : 898 Training_loss : 1426858.9 Test_loss : 1032719.06\n",
      "Epoch : 899 Training_loss : 1426858.9 Test_loss : 1032710.9\n",
      "Epoch : 900 Training_loss : 1426858.8 Test_loss : 1032702.75\n",
      "Epoch : 901 Training_loss : 1426858.9 Test_loss : 1032694.75\n",
      "Epoch : 902 Training_loss : 1426858.4 Test_loss : 1032686.7\n",
      "Epoch : 903 Training_loss : 1426858.5 Test_loss : 1032679.3\n",
      "Epoch : 904 Training_loss : 1426858.4 Test_loss : 1032672.06\n",
      "Epoch : 905 Training_loss : 1426858.5 Test_loss : 1032664.8\n",
      "Epoch : 906 Training_loss : 1426858.5 Test_loss : 1032657.5\n",
      "Epoch : 907 Training_loss : 1426858.5 Test_loss : 1032650.2\n",
      "Epoch : 908 Training_loss : 1426858.4 Test_loss : 1032642.9\n",
      "Epoch : 909 Training_loss : 1426858.2 Test_loss : 1032636.5\n",
      "Epoch : 910 Training_loss : 1426858.2 Test_loss : 1032630.06\n",
      "Epoch : 911 Training_loss : 1426858.5 Test_loss : 1032623.5\n",
      "Epoch : 912 Training_loss : 1426858.2 Test_loss : 1032617.0\n",
      "Epoch : 913 Training_loss : 1426858.2 Test_loss : 1032610.56\n",
      "Epoch : 914 Training_loss : 1426858.2 Test_loss : 1032604.2\n",
      "Epoch : 915 Training_loss : 1426858.2 Test_loss : 1032598.56\n",
      "Epoch : 916 Training_loss : 1426858.2 Test_loss : 1032592.9\n",
      "Epoch : 917 Training_loss : 1426858.4 Test_loss : 1032587.2\n",
      "Epoch : 918 Training_loss : 1426858.2 Test_loss : 1032581.5\n",
      "Epoch : 919 Training_loss : 1426858.2 Test_loss : 1032575.8\n",
      "Epoch : 920 Training_loss : 1426858.2 Test_loss : 1032570.2\n",
      "Epoch : 921 Training_loss : 1426858.2 Test_loss : 1032564.6\n",
      "Epoch : 922 Training_loss : 1426858.2 Test_loss : 1032559.7\n",
      "Epoch : 923 Training_loss : 1426858.2 Test_loss : 1032554.9\n",
      "Epoch : 924 Training_loss : 1426858.2 Test_loss : 1032549.94\n",
      "Epoch : 925 Training_loss : 1426858.2 Test_loss : 1032545.2\n",
      "Epoch : 926 Training_loss : 1426858.2 Test_loss : 1032540.4\n",
      "Epoch : 927 Training_loss : 1426858.2 Test_loss : 1032535.5\n",
      "Epoch : 928 Training_loss : 1426858.2 Test_loss : 1032530.75\n",
      "Epoch : 929 Training_loss : 1426858.2 Test_loss : 1032525.9\n",
      "Epoch : 930 Training_loss : 1426858.2 Test_loss : 1032521.8\n",
      "Epoch : 931 Training_loss : 1426858.2 Test_loss : 1032517.75\n",
      "Epoch : 932 Training_loss : 1426858.2 Test_loss : 1032513.6\n",
      "Epoch : 933 Training_loss : 1426857.9 Test_loss : 1032509.6\n",
      "Epoch : 934 Training_loss : 1426858.1 Test_loss : 1032505.6\n",
      "Epoch : 935 Training_loss : 1426858.1 Test_loss : 1032501.6\n",
      "Epoch : 936 Training_loss : 1426858.1 Test_loss : 1032497.56\n",
      "Epoch : 937 Training_loss : 1426858.1 Test_loss : 1032493.56\n",
      "Epoch : 938 Training_loss : 1426858.2 Test_loss : 1032489.5\n",
      "Epoch : 939 Training_loss : 1426857.9 Test_loss : 1032485.44\n",
      "Epoch : 940 Training_loss : 1426858.1 Test_loss : 1032482.1\n",
      "Epoch : 941 Training_loss : 1426858.1 Test_loss : 1032479.06\n",
      "Epoch : 942 Training_loss : 1426858.1 Test_loss : 1032475.75\n",
      "Epoch : 943 Training_loss : 1426857.9 Test_loss : 1032472.5\n",
      "Epoch : 944 Training_loss : 1426857.9 Test_loss : 1032469.3\n",
      "Epoch : 945 Training_loss : 1426857.9 Test_loss : 1032466.06\n",
      "Epoch : 946 Training_loss : 1426858.1 Test_loss : 1032462.9\n",
      "Epoch : 947 Training_loss : 1426858.1 Test_loss : 1032459.6\n",
      "Epoch : 948 Training_loss : 1426857.9 Test_loss : 1032456.4\n",
      "Epoch : 949 Training_loss : 1426857.9 Test_loss : 1032453.2\n",
      "Epoch : 950 Training_loss : 1426857.9 Test_loss : 1032450.06\n",
      "Epoch : 951 Training_loss : 1426857.9 Test_loss : 1032446.75\n",
      "Epoch : 952 Training_loss : 1426858.1 Test_loss : 1032443.5\n",
      "Epoch : 953 Training_loss : 1426857.9 Test_loss : 1032441.1\n",
      "Epoch : 954 Training_loss : 1426857.9 Test_loss : 1032438.7\n",
      "Epoch : 955 Training_loss : 1426857.9 Test_loss : 1032436.3\n",
      "Epoch : 956 Training_loss : 1426857.8 Test_loss : 1032433.9\n",
      "Epoch : 957 Training_loss : 1426857.9 Test_loss : 1032431.4\n",
      "Epoch : 958 Training_loss : 1426857.9 Test_loss : 1032428.94\n",
      "Epoch : 959 Training_loss : 1426857.9 Test_loss : 1032426.5\n",
      "Epoch : 960 Training_loss : 1426857.9 Test_loss : 1032424.06\n",
      "Epoch : 961 Training_loss : 1426857.9 Test_loss : 1032421.6\n",
      "Epoch : 962 Training_loss : 1426857.9 Test_loss : 1032419.3\n",
      "Epoch : 963 Training_loss : 1426857.9 Test_loss : 1032416.94\n",
      "Epoch : 964 Training_loss : 1426857.9 Test_loss : 1032414.5\n",
      "Epoch : 965 Training_loss : 1426857.9 Test_loss : 1032412.0\n",
      "Epoch : 966 Training_loss : 1426857.9 Test_loss : 1032409.56\n",
      "Epoch : 967 Training_loss : 1426857.9 Test_loss : 1032407.1\n",
      "Epoch : 968 Training_loss : 1426857.8 Test_loss : 1032404.9\n",
      "Epoch : 969 Training_loss : 1426857.9 Test_loss : 1032403.2\n",
      "Epoch : 970 Training_loss : 1426857.9 Test_loss : 1032401.56\n",
      "Epoch : 971 Training_loss : 1426857.8 Test_loss : 1032399.8\n",
      "Epoch : 972 Training_loss : 1426857.8 Test_loss : 1032398.4\n",
      "Epoch : 973 Training_loss : 1426857.9 Test_loss : 1032396.8\n",
      "Epoch : 974 Training_loss : 1426857.9 Test_loss : 1032395.2\n",
      "Epoch : 975 Training_loss : 1426857.9 Test_loss : 1032393.5\n",
      "Epoch : 976 Training_loss : 1426857.9 Test_loss : 1032391.8\n",
      "Epoch : 977 Training_loss : 1426857.8 Test_loss : 1032390.25\n",
      "Epoch : 978 Training_loss : 1426857.8 Test_loss : 1032388.7\n",
      "Epoch : 979 Training_loss : 1426857.8 Test_loss : 1032387.06\n",
      "Epoch : 980 Training_loss : 1426857.9 Test_loss : 1032385.44\n",
      "Epoch : 981 Training_loss : 1426857.9 Test_loss : 1032383.8\n",
      "Epoch : 982 Training_loss : 1426857.8 Test_loss : 1032382.2\n",
      "Epoch : 983 Training_loss : 1426857.8 Test_loss : 1032380.56\n",
      "Epoch : 984 Training_loss : 1426857.9 Test_loss : 1032379.0\n",
      "Epoch : 985 Training_loss : 1426857.9 Test_loss : 1032377.5\n",
      "Epoch : 986 Training_loss : 1426857.9 Test_loss : 1032375.8\n",
      "Epoch : 987 Training_loss : 1426857.9 Test_loss : 1032374.1\n",
      "Epoch : 988 Training_loss : 1426857.8 Test_loss : 1032372.56\n",
      "Epoch : 989 Training_loss : 1426858.1 Test_loss : 1032371.0\n",
      "Epoch : 990 Training_loss : 1426857.9 Test_loss : 1032369.44\n",
      "Epoch : 991 Training_loss : 1426857.9 Test_loss : 1032367.7\n",
      "Epoch : 992 Training_loss : 1426857.9 Test_loss : 1032366.1\n",
      "Epoch : 993 Training_loss : 1426857.8 Test_loss : 1032364.56\n",
      "Epoch : 994 Training_loss : 1426857.8 Test_loss : 1032363.7\n",
      "Epoch : 995 Training_loss : 1426857.6 Test_loss : 1032362.8\n",
      "Epoch : 996 Training_loss : 1426858.1 Test_loss : 1032362.06\n",
      "Epoch : 997 Training_loss : 1426857.9 Test_loss : 1032361.25\n",
      "Epoch : 998 Training_loss : 1426857.9 Test_loss : 1032360.5\n",
      "Epoch : 999 Training_loss : 1426857.8 Test_loss : 1032359.6\n",
      "Epoch : 1000 Training_loss : 1426857.9 Test_loss : 1032358.8\n",
      "Epoch : 1001 Training_loss : 1426857.8 Test_loss : 1032358.1\n",
      "Epoch : 1002 Training_loss : 1426857.9 Test_loss : 1032357.25\n",
      "Epoch : 1003 Training_loss : 1426857.9 Test_loss : 1032356.4\n",
      "Epoch : 1004 Training_loss : 1426857.9 Test_loss : 1032355.7\n",
      "Epoch : 1005 Training_loss : 1426857.9 Test_loss : 1032354.9\n",
      "Epoch : 1006 Training_loss : 1426857.9 Test_loss : 1032354.06\n",
      "Epoch : 1007 Training_loss : 1426857.8 Test_loss : 1032353.25\n",
      "Epoch : 1008 Training_loss : 1426857.8 Test_loss : 1032352.3\n",
      "Epoch : 1009 Training_loss : 1426857.9 Test_loss : 1032351.7\n",
      "Epoch : 1010 Training_loss : 1426857.9 Test_loss : 1032350.8\n",
      "Epoch : 1011 Training_loss : 1426857.8 Test_loss : 1032349.94\n",
      "Epoch : 1012 Training_loss : 1426857.9 Test_loss : 1032349.25\n",
      "Epoch : 1013 Training_loss : 1426857.9 Test_loss : 1032348.4\n",
      "Epoch : 1014 Training_loss : 1426857.9 Test_loss : 1032347.7\n",
      "Epoch : 1015 Training_loss : 1426857.9 Test_loss : 1032346.75\n",
      "Epoch : 1016 Training_loss : 1426857.9 Test_loss : 1032345.94\n",
      "Epoch : 1017 Training_loss : 1426857.9 Test_loss : 1032345.2\n",
      "Epoch : 1018 Training_loss : 1426857.9 Test_loss : 1032344.4\n",
      "Epoch : 1019 Training_loss : 1426857.8 Test_loss : 1032343.5\n",
      "Epoch : 1020 Training_loss : 1426857.8 Test_loss : 1032342.75\n",
      "Epoch : 1021 Training_loss : 1426857.9 Test_loss : 1032341.9\n",
      "Epoch : 1022 Training_loss : 1426857.9 Test_loss : 1032341.1\n",
      "Epoch : 1023 Training_loss : 1426857.9 Test_loss : 1032340.4\n",
      "Epoch : 1024 Training_loss : 1426857.9 Test_loss : 1032339.5\n",
      "Epoch : 1025 Training_loss : 1426857.9 Test_loss : 1032338.75\n",
      "Epoch : 1026 Training_loss : 1426857.9 Test_loss : 1032337.9\n",
      "Epoch : 1027 Training_loss : 1426857.8 Test_loss : 1032337.0\n",
      "Epoch : 1028 Training_loss : 1426857.9 Test_loss : 1032336.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1029 Training_loss : 1426857.8 Test_loss : 1032335.5\n",
      "Epoch : 1030 Training_loss : 1426857.9 Test_loss : 1032334.56\n",
      "Epoch : 1031 Training_loss : 1426857.6 Test_loss : 1032333.9\n",
      "Epoch : 1032 Training_loss : 1426857.8 Test_loss : 1032333.0\n",
      "Epoch : 1033 Training_loss : 1426857.6 Test_loss : 1032332.25\n",
      "Epoch : 1034 Training_loss : 1426857.8 Test_loss : 1032331.5\n",
      "Epoch : 1035 Training_loss : 1426857.6 Test_loss : 1032330.7\n",
      "Epoch : 1036 Training_loss : 1426857.9 Test_loss : 1032329.8\n",
      "Epoch : 1037 Training_loss : 1426857.9 Test_loss : 1032329.0\n",
      "Epoch : 1038 Training_loss : 1426857.9 Test_loss : 1032328.25\n",
      "Epoch : 1039 Training_loss : 1426857.8 Test_loss : 1032327.5\n",
      "Epoch : 1040 Training_loss : 1426857.9 Test_loss : 1032326.75\n",
      "Epoch : 1041 Training_loss : 1426857.9 Test_loss : 1032325.8\n",
      "Epoch : 1042 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1043 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1044 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1045 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1046 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1047 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1048 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1049 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1050 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1051 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1052 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1053 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1054 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1055 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1056 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1057 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1058 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1059 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1060 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1061 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1062 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1063 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1064 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1065 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1066 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1067 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1068 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1069 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1070 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1071 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1072 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1073 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1074 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1075 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1076 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1077 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1078 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1079 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1080 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1081 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1082 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1083 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1084 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1085 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1086 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1087 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1088 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1089 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1090 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1091 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1092 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1093 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1094 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1095 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1096 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1097 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1098 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1099 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1100 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1101 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1102 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1103 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1104 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1105 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1106 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1107 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1108 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1109 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1110 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1111 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1112 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1113 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1114 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1115 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1116 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1117 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1118 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1119 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1120 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1121 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1122 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1123 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1124 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1125 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1126 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1127 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1128 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1129 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1130 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1131 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1132 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1133 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1134 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1135 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1136 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1137 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1138 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1139 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1140 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1141 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1142 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1143 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1144 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1145 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1146 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1147 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1148 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1149 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1150 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1151 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1152 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1153 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1154 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1155 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1156 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1157 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1158 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1159 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1160 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1161 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1162 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1163 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1164 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1165 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1166 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1167 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1168 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1169 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1170 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1171 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1172 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1173 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1174 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1175 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1176 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1177 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1178 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1179 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1180 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1181 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1182 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1183 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1184 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1185 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1186 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1187 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1188 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1189 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1190 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1191 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1192 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1193 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1194 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1195 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1196 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1197 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1198 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1199 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1200 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1201 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1202 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1203 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1204 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1205 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1206 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1207 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1208 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1209 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1210 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1211 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1212 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1213 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1214 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1215 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1216 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1217 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1218 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1219 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1220 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1221 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1222 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1223 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1224 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1225 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1226 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1227 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1228 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1229 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1230 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1231 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1232 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1233 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1234 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1235 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1236 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1237 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1238 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1239 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1240 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1241 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1242 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1243 Training_loss : 1426857.9 Test_loss : 1032325.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1244 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1245 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1246 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1247 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1248 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1249 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1250 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1251 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1252 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1253 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1254 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1255 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1256 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1257 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1258 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1259 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1260 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1261 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1262 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1263 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1264 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1265 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1266 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1267 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1268 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1269 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1270 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1271 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1272 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1273 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1274 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1275 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1276 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1277 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1278 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1279 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1280 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1281 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1282 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1283 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1284 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1285 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1286 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1287 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1288 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1289 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1290 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1291 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1292 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1293 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1294 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1295 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1296 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1297 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1298 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1299 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1300 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1301 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1302 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1303 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1304 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1305 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1306 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1307 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1308 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1309 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1310 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1311 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1312 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1313 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1314 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1315 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1316 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1317 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1318 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1319 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1320 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1321 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1322 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1323 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1324 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1325 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1326 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1327 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1328 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1329 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1330 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1331 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1332 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1333 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1334 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1335 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1336 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1337 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1338 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1339 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1340 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1341 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1342 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1343 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1344 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1345 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1346 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1347 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1348 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1349 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1350 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1351 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1352 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1353 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1354 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1355 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1356 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1357 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1358 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1359 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1360 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1361 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1362 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1363 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1364 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1365 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1366 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1367 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1368 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1369 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1370 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1371 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1372 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1373 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1374 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1375 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1376 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1377 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1378 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1379 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1380 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1381 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1382 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1383 Training_loss : 1426857.9 Test_loss : 1032325.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1384 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1385 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1386 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1387 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1388 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1389 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1390 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1391 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1392 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1393 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1394 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1395 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1396 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1397 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1398 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1399 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1400 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1401 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1402 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1403 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1404 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1405 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1406 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1407 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1408 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1409 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1410 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1411 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1412 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1413 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1414 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1415 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1416 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1417 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1418 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1419 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1420 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1421 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1422 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1423 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1424 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1425 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1426 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1427 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1428 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1429 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1430 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1431 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1432 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1433 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1434 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1435 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1436 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1437 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1438 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1439 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1440 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1441 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1442 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1443 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1444 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1445 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1446 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1447 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1448 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1449 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1450 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1451 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1452 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1453 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1454 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1455 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1456 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1457 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1458 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1459 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1460 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1461 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1462 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1463 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1464 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1465 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1466 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1467 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1468 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1469 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1470 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1471 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1472 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1473 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1474 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1475 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1476 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1477 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1478 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1479 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1480 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1481 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1482 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1483 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1484 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1485 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1486 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1487 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1488 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1489 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1490 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1491 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1492 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1493 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1494 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1495 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1496 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1497 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1498 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1499 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1500 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1501 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1502 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1503 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1504 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1505 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1506 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1507 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1508 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1509 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1510 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1511 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1512 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1513 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1514 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1515 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1516 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1517 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1518 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1519 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1520 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1521 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1522 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1523 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1524 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1525 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1526 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1527 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1528 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1529 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1530 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1531 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1532 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1533 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1534 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1535 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1536 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1537 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1538 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1539 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1540 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1541 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1542 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1543 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1544 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1545 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1546 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1547 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1548 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1549 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1550 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1551 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1552 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1553 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1554 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1555 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1556 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1557 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1558 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1559 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1560 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1561 Training_loss : 1426857.9 Test_loss : 1032325.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1562 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1563 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1564 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1565 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1566 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1567 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1568 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1569 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1570 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1571 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1572 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1573 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1574 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1575 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1576 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1577 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1578 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1579 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1580 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1581 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1582 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1583 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1584 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1585 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1586 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1587 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1588 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1589 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1590 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1591 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1592 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1593 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1594 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1595 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1596 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1597 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1598 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1599 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1600 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1601 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1602 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1603 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1604 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1605 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1606 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1607 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1608 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1609 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1610 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1611 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1612 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1613 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1614 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1615 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1616 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1617 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1618 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1619 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1620 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1621 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1622 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1623 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1624 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1625 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1626 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1627 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1628 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1629 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1630 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1631 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1632 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1633 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1634 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1635 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1636 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1637 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1638 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1639 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1640 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1641 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1642 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1643 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1644 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1645 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1646 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1647 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1648 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1649 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1650 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1651 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1652 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1653 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1654 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1655 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1656 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1657 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1658 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1659 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1660 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1661 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1662 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1663 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1664 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1665 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1666 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1667 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1668 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1669 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1670 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1671 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1672 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1673 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1674 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1675 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1676 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1677 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1678 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1679 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1680 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1681 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1682 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1683 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1684 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1685 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1686 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1687 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1688 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1689 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1690 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1691 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1692 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1693 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1694 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1695 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1696 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1697 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1698 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1699 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1700 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1701 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1702 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1703 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1704 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1705 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1706 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1707 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1708 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1709 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1710 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1711 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1712 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1713 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1714 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1715 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1716 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1717 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1718 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1719 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1720 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1721 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1722 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1723 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1724 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1725 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1726 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1727 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1728 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1729 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1730 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1731 Training_loss : 1426857.9 Test_loss : 1032325.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1732 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1733 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1734 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1735 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1736 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1737 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1738 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1739 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1740 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1741 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1742 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1743 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1744 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1745 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1746 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1747 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1748 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1749 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1750 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1751 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1752 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1753 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1754 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1755 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1756 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1757 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1758 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1759 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1760 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1761 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1762 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1763 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1764 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1765 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1766 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1767 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1768 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1769 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1770 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1771 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1772 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1773 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1774 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1775 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1776 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1777 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1778 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1779 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1780 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1781 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1782 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1783 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1784 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1785 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1786 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1787 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1788 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1789 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1790 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1791 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1792 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1793 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1794 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1795 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1796 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1797 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1798 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1799 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1800 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1801 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1802 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1803 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1804 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1805 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1806 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1807 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1808 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1809 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1810 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1811 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1812 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1813 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1814 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1815 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1816 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1817 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1818 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1819 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1820 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1821 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1822 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1823 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1824 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1825 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1826 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1827 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1828 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1829 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1830 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1831 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1832 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1833 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1834 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1835 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1836 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1837 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1838 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1839 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1840 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1841 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1842 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1843 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1844 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1845 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1846 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1847 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1848 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1849 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1850 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1851 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1852 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1853 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1854 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1855 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1856 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1857 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1858 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1859 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1860 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1861 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1862 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1863 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1864 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1865 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1866 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1867 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1868 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1869 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1870 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1871 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1872 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1873 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1874 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1875 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1876 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1877 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1878 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1879 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1880 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1881 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1882 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1883 Training_loss : 1426857.9 Test_loss : 1032325.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1884 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1885 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1886 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1887 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1888 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1889 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1890 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1891 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1892 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1893 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1894 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1895 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1896 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1897 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1898 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1899 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1900 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1901 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1902 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1903 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1904 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1905 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1906 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1907 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1908 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1909 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1910 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1911 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1912 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1913 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1914 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1915 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1916 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1917 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1918 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1919 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1920 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1921 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1922 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1923 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1924 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1925 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1926 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1927 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1928 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1929 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1930 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1931 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1932 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1933 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1934 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1935 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1936 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1937 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1938 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1939 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1940 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1941 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1942 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1943 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1944 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1945 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1946 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1947 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1948 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1949 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1950 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1951 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1952 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1953 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1954 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1955 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1956 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1957 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1958 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1959 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1960 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1961 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1962 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1963 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1964 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1965 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1966 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1967 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1968 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1969 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1970 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1971 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1972 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1973 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1974 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1975 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1976 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1977 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1978 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1979 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1980 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1981 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1982 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1983 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1984 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1985 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1986 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1987 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1988 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1989 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1990 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1991 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1992 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1993 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1994 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1995 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1996 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1997 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1998 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1999 Training_loss : 1426857.9 Test_loss : 1032325.1\n"
     ]
    }
   ],
   "source": [
    "Training_loss = [ ]\n",
    "Test_loss = [ ]\n",
    "epoch = 2000\n",
    "for i in range(epoch):\n",
    "    sess.run(train, feed_dict = {x:x_train, y:y_train})\n",
    "    \n",
    "    Training_loss.append(sess.run(cost, feed_dict = {x: x_train, y: y_train}))\n",
    "    Test_loss.append(sess.run(cost, feed_dict = {x: x_test, y: y_test}))\n",
    "    \n",
    "\n",
    "    print('Epoch :',i,'Training_loss :',Training_loss[i], 'Test_loss :', Test_loss[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sess.run(out, feed_dict = {x:x_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25989.865],\n",
       "       [25989.865],\n",
       "       [25989.865],\n",
       "       [25989.865],\n",
       "       [25989.865]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27359.160156],\n",
       "       [25169.880859],\n",
       "       [26452.660156],\n",
       "       [27171.900391],\n",
       "       [26783.490234]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Test_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/yahoo_dataset.ckpt'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.compat.v1.train.Saver()\n",
    "saver.save(sess,'models/yahoo_dataset.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/admin1/Tensorflow/models/yahoo_dataset.ckpt\n",
      "[[25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]\n",
      " [25989.865]]\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as ses:\n",
    "    saver = tf.compat.v1.train.import_meta_graph('/home/admin1/Tensorflow/models/yahoo_dataset.ckpt.meta')\n",
    "    model = saver.restore(ses, tf.train.latest_checkpoint('/home/admin1/Tensorflow/models/'))\n",
    "    \n",
    "    graph = tf.compat.v1.get_default_graph()\n",
    "    input_x = graph.get_tensor_by_name(\"x:0\")\n",
    "    input_y = graph.get_tensor_by_name(\"y:0\")\n",
    "    \n",
    "    output = graph.get_tensor_by_name(\"out:0\")\n",
    "    \n",
    "    \n",
    "    feed_dict = {input_x: x_test}\n",
    "    predictions = output.eval(feed_dict = feed_dict)\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
