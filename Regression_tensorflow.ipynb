{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('DJI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(251, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>25307.140625</td>\n",
       "      <td>25549.710938</td>\n",
       "      <td>25250.970703</td>\n",
       "      <td>25538.460938</td>\n",
       "      <td>25538.460938</td>\n",
       "      <td>482250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-03</td>\n",
       "      <td>25779.570313</td>\n",
       "      <td>25980.210938</td>\n",
       "      <td>25670.509766</td>\n",
       "      <td>25826.429688</td>\n",
       "      <td>25826.429688</td>\n",
       "      <td>388480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-04</td>\n",
       "      <td>25752.560547</td>\n",
       "      <td>25773.119141</td>\n",
       "      <td>25008.109375</td>\n",
       "      <td>25027.070313</td>\n",
       "      <td>25027.070313</td>\n",
       "      <td>418900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-06</td>\n",
       "      <td>24737.419922</td>\n",
       "      <td>24951.009766</td>\n",
       "      <td>24242.220703</td>\n",
       "      <td>24947.669922</td>\n",
       "      <td>24947.669922</td>\n",
       "      <td>471690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>24918.820313</td>\n",
       "      <td>25095.619141</td>\n",
       "      <td>24284.779297</td>\n",
       "      <td>24388.949219</td>\n",
       "      <td>24388.949219</td>\n",
       "      <td>398230000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date          Open          High           Low         Close  \\\n",
       "0  2018-11-30  25307.140625  25549.710938  25250.970703  25538.460938   \n",
       "1  2018-12-03  25779.570313  25980.210938  25670.509766  25826.429688   \n",
       "2  2018-12-04  25752.560547  25773.119141  25008.109375  25027.070313   \n",
       "3  2018-12-06  24737.419922  24951.009766  24242.220703  24947.669922   \n",
       "4  2018-12-07  24918.820313  25095.619141  24284.779297  24388.949219   \n",
       "\n",
       "      Adj Close     Volume  \n",
       "0  25538.460938  482250000  \n",
       "1  25826.429688  388480000  \n",
       "2  25027.070313  418900000  \n",
       "3  24947.669922  471690000  \n",
       "4  24388.949219  398230000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Date', 'Adj Close', 'Volume'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25307.140625</td>\n",
       "      <td>25549.710938</td>\n",
       "      <td>25250.970703</td>\n",
       "      <td>25538.460938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25779.570313</td>\n",
       "      <td>25980.210938</td>\n",
       "      <td>25670.509766</td>\n",
       "      <td>25826.429688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25752.560547</td>\n",
       "      <td>25773.119141</td>\n",
       "      <td>25008.109375</td>\n",
       "      <td>25027.070313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24737.419922</td>\n",
       "      <td>24951.009766</td>\n",
       "      <td>24242.220703</td>\n",
       "      <td>24947.669922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24918.820313</td>\n",
       "      <td>25095.619141</td>\n",
       "      <td>24284.779297</td>\n",
       "      <td>24388.949219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Open          High           Low         Close\n",
       "0  25307.140625  25549.710938  25250.970703  25538.460938\n",
       "1  25779.570313  25980.210938  25670.509766  25826.429688\n",
       "2  25752.560547  25773.119141  25008.109375  25027.070313\n",
       "3  24737.419922  24951.009766  24242.220703  24947.669922\n",
       "4  24918.820313  25095.619141  24284.779297  24388.949219"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.heatmap(data.isna(),\n",
    "          cbar = False,\n",
    "          yticklabels = False,\n",
    "          cmap = 'viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((251, 3), (251, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 3), (200, 1), (51, 3), (51, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.compat.v1.placeholder(dtype = tf.float32)\n",
    "y = tf.compat.v1.placeholder(dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Placeholder:0' shape=<unknown> dtype=float32>,\n",
       " <tf.Tensor 'Placeholder_1:0' shape=<unknown> dtype=float32>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/admin1/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "w1 = tf.Variable(tf.compat.v1.random_uniform([x_train.shape[1],10]))\n",
    "b1 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "\n",
    "w2 = tf.Variable(tf.compat.v1.random_uniform([10, 5]))\n",
    "b2 = tf.Variable(tf.zeros([5]))\n",
    "\n",
    "\n",
    "\n",
    "wo = tf.Variable(tf.compat.v1.random_uniform([5, 1]))\n",
    "bo = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=(3, 10) dtype=float32>,\n",
       " <tf.Variable 'Variable_1:0' shape=(10,) dtype=float32>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable_2:0' shape=(10, 5) dtype=float32>,\n",
       " <tf.Variable 'Variable_3:0' shape=(5,) dtype=float32>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable_4:0' shape=(5, 1) dtype=float32>,\n",
       " <tf.Variable 'Variable_5:0' shape=(1,) dtype=float32>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wo, bo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = tf.add(tf.matmul(x, w1), b1)\n",
    "hidden1 = tf.nn.relu(hidden1)\n",
    "\n",
    "\n",
    "#Layer2 propagation\n",
    "hidden2 = tf.add(tf.matmul(hidden1, w2), b2)\n",
    "hidden2 = tf.nn.relu(hidden2)\n",
    "\n",
    "\n",
    "\n",
    "#Output Layer propagation\n",
    "out = tf.add(tf.matmul(hidden2, wo), bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu:0' shape=<unknown> dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_1:0' shape=<unknown> dtype=float32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Add_2:0' shape=<unknown> dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(out - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.compat.v1.train.GradientDescentOptimizer(0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Training_loss : 2.5118136e+20 Test_loss : 2.728569e+20\n",
      "Epoch : 1 Training_loss : 5.5016074e+16 Test_loss : 5.5016173e+16\n",
      "Epoch : 2 Training_loss : 5.283744e+16 Test_loss : 5.283754e+16\n",
      "Epoch : 3 Training_loss : 5.0745073e+16 Test_loss : 5.0745167e+16\n",
      "Epoch : 4 Training_loss : 4.8735574e+16 Test_loss : 4.873566e+16\n",
      "Epoch : 5 Training_loss : 4.6805643e+16 Test_loss : 4.680573e+16\n",
      "Epoch : 6 Training_loss : 4.4952137e+16 Test_loss : 4.4952223e+16\n",
      "Epoch : 7 Training_loss : 4.317203e+16 Test_loss : 4.3172114e+16\n",
      "Epoch : 8 Training_loss : 4.146242e+16 Test_loss : 4.1462506e+16\n",
      "Epoch : 9 Training_loss : 3.982051e+16 Test_loss : 3.982059e+16\n",
      "Epoch : 10 Training_loss : 3.824362e+16 Test_loss : 3.8243703e+16\n",
      "Epoch : 11 Training_loss : 3.672917e+16 Test_loss : 3.672925e+16\n",
      "Epoch : 12 Training_loss : 3.5274697e+16 Test_loss : 3.5274775e+16\n",
      "Epoch : 13 Training_loss : 3.3877817e+16 Test_loss : 3.3877899e+16\n",
      "Epoch : 14 Training_loss : 3.2536258e+16 Test_loss : 3.2536334e+16\n",
      "Epoch : 15 Training_loss : 3.1247824e+16 Test_loss : 3.1247895e+16\n",
      "Epoch : 16 Training_loss : 3.0010412e+16 Test_loss : 3.001048e+16\n",
      "Epoch : 17 Training_loss : 2.8821997e+16 Test_loss : 2.8822065e+16\n",
      "Epoch : 18 Training_loss : 2.7680645e+16 Test_loss : 2.7680714e+16\n",
      "Epoch : 19 Training_loss : 2.658449e+16 Test_loss : 2.658456e+16\n",
      "Epoch : 20 Training_loss : 2.5531742e+16 Test_loss : 2.553181e+16\n",
      "Epoch : 21 Training_loss : 2.4520692e+16 Test_loss : 2.4520756e+16\n",
      "Epoch : 22 Training_loss : 2.3549664e+16 Test_loss : 2.3549733e+16\n",
      "Epoch : 23 Training_loss : 2.2617102e+16 Test_loss : 2.2617162e+16\n",
      "Epoch : 24 Training_loss : 2.1721464e+16 Test_loss : 2.1721524e+16\n",
      "Epoch : 25 Training_loss : 2.0861292e+16 Test_loss : 2.0861352e+16\n",
      "Epoch : 26 Training_loss : 2.0035187e+16 Test_loss : 2.0035243e+16\n",
      "Epoch : 27 Training_loss : 1.924179e+16 Test_loss : 1.9241849e+16\n",
      "Epoch : 28 Training_loss : 1.847982e+16 Test_loss : 1.8479876e+16\n",
      "Epoch : 29 Training_loss : 1.774802e+16 Test_loss : 1.7748074e+16\n",
      "Epoch : 30 Training_loss : 1.7045195e+16 Test_loss : 1.7045252e+16\n",
      "Epoch : 31 Training_loss : 1.637021e+16 Test_loss : 1.6370262e+16\n",
      "Epoch : 32 Training_loss : 1.572195e+16 Test_loss : 1.5722e+16\n",
      "Epoch : 33 Training_loss : 1.5099362e+16 Test_loss : 1.509941e+16\n",
      "Epoch : 34 Training_loss : 1.4501423e+16 Test_loss : 1.4501474e+16\n",
      "Epoch : 35 Training_loss : 1.3927169e+16 Test_loss : 1.3927217e+16\n",
      "Epoch : 36 Training_loss : 1.3375649e+16 Test_loss : 1.33757e+16\n",
      "Epoch : 37 Training_loss : 1.2845973e+16 Test_loss : 1.2846022e+16\n",
      "Epoch : 38 Training_loss : 1.2337275e+16 Test_loss : 1.233732e+16\n",
      "Epoch : 39 Training_loss : 1.184872e+16 Test_loss : 1.1848765e+16\n",
      "Epoch : 40 Training_loss : 1.1379509e+16 Test_loss : 1.1379553e+16\n",
      "Epoch : 41 Training_loss : 1.0928881e+16 Test_loss : 1.0928923e+16\n",
      "Epoch : 42 Training_loss : 1.0496097e+16 Test_loss : 1.049614e+16\n",
      "Epoch : 43 Training_loss : 1.0080451e+16 Test_loss : 1.0080493e+16\n",
      "Epoch : 44 Training_loss : 9681265000000000.0 Test_loss : 9681306000000000.0\n",
      "Epoch : 45 Training_loss : 9297887000000000.0 Test_loss : 9297927000000000.0\n",
      "Epoch : 46 Training_loss : 8929690000000000.0 Test_loss : 8929728000000000.0\n",
      "Epoch : 47 Training_loss : 8576073700000000.0 Test_loss : 8576112300000000.0\n",
      "Epoch : 48 Training_loss : 8236461000000000.0 Test_loss : 8236499600000000.0\n",
      "Epoch : 49 Training_loss : 7910297400000000.0 Test_loss : 7910334000000000.0\n",
      "Epoch : 50 Training_loss : 7597049300000000.0 Test_loss : 7597085000000000.0\n",
      "Epoch : 51 Training_loss : 7296205000000000.0 Test_loss : 7296241000000000.0\n",
      "Epoch : 52 Training_loss : 7007275700000000.0 Test_loss : 7007310000000000.0\n",
      "Epoch : 53 Training_loss : 6729787500000000.0 Test_loss : 6729821000000000.0\n",
      "Epoch : 54 Training_loss : 6463288000000000.0 Test_loss : 6463321000000000.0\n",
      "Epoch : 55 Training_loss : 6207342000000000.0 Test_loss : 6207375000000000.0\n",
      "Epoch : 56 Training_loss : 5961530000000000.0 Test_loss : 5961563000000000.0\n",
      "Epoch : 57 Training_loss : 5725455000000000.0 Test_loss : 5725485600000000.0\n",
      "Epoch : 58 Training_loss : 5498726400000000.0 Test_loss : 5498757000000000.0\n",
      "Epoch : 59 Training_loss : 5280977400000000.0 Test_loss : 5281007500000000.0\n",
      "Epoch : 60 Training_loss : 5071850000000000.0 Test_loss : 5071880000000000.0\n",
      "Epoch : 61 Training_loss : 4871006000000000.0 Test_loss : 4871034000000000.0\n",
      "Epoch : 62 Training_loss : 4678114300000000.0 Test_loss : 4678142000000000.0\n",
      "Epoch : 63 Training_loss : 4492860600000000.0 Test_loss : 4492888800000000.0\n",
      "Epoch : 64 Training_loss : 4314943500000000.0 Test_loss : 4314970300000000.0\n",
      "Epoch : 65 Training_loss : 4144071700000000.0 Test_loss : 4144098000000000.0\n",
      "Epoch : 66 Training_loss : 3979967100000000.0 Test_loss : 3979993200000000.0\n",
      "Epoch : 67 Training_loss : 3822361000000000.0 Test_loss : 3822385600000000.0\n",
      "Epoch : 68 Training_loss : 3670994400000000.0 Test_loss : 3671019400000000.0\n",
      "Epoch : 69 Training_loss : 3525623200000000.0 Test_loss : 3525647400000000.0\n",
      "Epoch : 70 Training_loss : 3386008600000000.0 Test_loss : 3386032200000000.0\n",
      "Epoch : 71 Training_loss : 3251922400000000.0 Test_loss : 3251946000000000.0\n",
      "Epoch : 72 Training_loss : 3123146700000000.0 Test_loss : 3123169800000000.0\n",
      "Epoch : 73 Training_loss : 2999470000000000.0 Test_loss : 2999492400000000.0\n",
      "Epoch : 74 Training_loss : 2880690400000000.0 Test_loss : 2880712700000000.0\n",
      "Epoch : 75 Training_loss : 2766615300000000.0 Test_loss : 2766637300000000.0\n",
      "Epoch : 76 Training_loss : 2657057400000000.0 Test_loss : 2657078600000000.0\n",
      "Epoch : 77 Training_loss : 2551838000000000.0 Test_loss : 2551859000000000.0\n",
      "Epoch : 78 Training_loss : 2450785000000000.0 Test_loss : 2450805800000000.0\n",
      "Epoch : 79 Training_loss : 2353734200000000.0 Test_loss : 2353754300000000.0\n",
      "Epoch : 80 Training_loss : 2260526400000000.0 Test_loss : 2260546000000000.0\n",
      "Epoch : 81 Training_loss : 2171009700000000.0 Test_loss : 2171028800000000.0\n",
      "Epoch : 82 Training_loss : 2085037500000000.0 Test_loss : 2085056600000000.0\n",
      "Epoch : 83 Training_loss : 2002470400000000.0 Test_loss : 2002488800000000.0\n",
      "Epoch : 84 Training_loss : 1923172700000000.0 Test_loss : 1923190400000000.0\n",
      "Epoch : 85 Training_loss : 1847015000000000.0 Test_loss : 1847032400000000.0\n",
      "Epoch : 86 Training_loss : 1773873000000000.0 Test_loss : 1773890200000000.0\n",
      "Epoch : 87 Training_loss : 1703627500000000.0 Test_loss : 1703644300000000.0\n",
      "Epoch : 88 Training_loss : 1636163900000000.0 Test_loss : 1636180400000000.0\n",
      "Epoch : 89 Training_loss : 1571371900000000.0 Test_loss : 1571388000000000.0\n",
      "Epoch : 90 Training_loss : 1509145200000000.0 Test_loss : 1509161000000000.0\n",
      "Epoch : 91 Training_loss : 1449383200000000.0 Test_loss : 1449398700000000.0\n",
      "Epoch : 92 Training_loss : 1391987200000000.0 Test_loss : 1392002700000000.0\n",
      "Epoch : 93 Training_loss : 1336864400000000.0 Test_loss : 1336879700000000.0\n",
      "Epoch : 94 Training_loss : 1283924500000000.0 Test_loss : 1283939400000000.0\n",
      "Epoch : 95 Training_loss : 1233081400000000.0 Test_loss : 1233095800000000.0\n",
      "Epoch : 96 Training_loss : 1184251300000000.0 Test_loss : 1184265700000000.0\n",
      "Epoch : 97 Training_loss : 1137355000000000.0 Test_loss : 1137369000000000.0\n",
      "Epoch : 98 Training_loss : 1092315700000000.0 Test_loss : 1092329400000000.0\n",
      "Epoch : 99 Training_loss : 1049060000000000.0 Test_loss : 1049073400000000.0\n",
      "Epoch : 100 Training_loss : 1007517300000000.0 Test_loss : 1007530340000000.0\n",
      "Epoch : 101 Training_loss : 967619560000000.0 Test_loss : 967632440000000.0\n",
      "Epoch : 102 Training_loss : 929301900000000.0 Test_loss : 929314300000000.0\n",
      "Epoch : 103 Training_loss : 892501500000000.0 Test_loss : 892513930000000.0\n",
      "Epoch : 104 Training_loss : 857158400000000.0 Test_loss : 857170450000000.0\n",
      "Epoch : 105 Training_loss : 823215040000000.0 Test_loss : 823226850000000.0\n",
      "Epoch : 106 Training_loss : 790615630000000.0 Test_loss : 790627200000000.0\n",
      "Epoch : 107 Training_loss : 759307300000000.0 Test_loss : 759318600000000.0\n",
      "Epoch : 108 Training_loss : 729238600000000.0 Test_loss : 729249900000000.0\n",
      "Epoch : 109 Training_loss : 700360900000000.0 Test_loss : 700371800000000.0\n",
      "Epoch : 110 Training_loss : 672626600000000.0 Test_loss : 672637240000000.0\n",
      "Epoch : 111 Training_loss : 645990660000000.0 Test_loss : 646001100000000.0\n",
      "Epoch : 112 Training_loss : 620409500000000.0 Test_loss : 620419700000000.0\n",
      "Epoch : 113 Training_loss : 595841200000000.0 Test_loss : 595851300000000.0\n",
      "Epoch : 114 Training_loss : 572245900000000.0 Test_loss : 572255800000000.0\n",
      "Epoch : 115 Training_loss : 549584920000000.0 Test_loss : 549594600000000.0\n",
      "Epoch : 116 Training_loss : 527821400000000.0 Test_loss : 527830880000000.0\n",
      "Epoch : 117 Training_loss : 506919620000000.0 Test_loss : 506928900000000.0\n",
      "Epoch : 118 Training_loss : 486845600000000.0 Test_loss : 486854700000000.0\n",
      "Epoch : 119 Training_loss : 467566450000000.0 Test_loss : 467575400000000.0\n",
      "Epoch : 120 Training_loss : 449050900000000.0 Test_loss : 449059600000000.0\n",
      "Epoch : 121 Training_loss : 431268500000000.0 Test_loss : 431277030000000.0\n",
      "Epoch : 122 Training_loss : 414190200000000.0 Test_loss : 414198600000000.0\n",
      "Epoch : 123 Training_loss : 397788260000000.0 Test_loss : 397796500000000.0\n",
      "Epoch : 124 Training_loss : 382035860000000.0 Test_loss : 382043950000000.0\n",
      "Epoch : 125 Training_loss : 366907280000000.0 Test_loss : 366915200000000.0\n",
      "Epoch : 126 Training_loss : 352377800000000.0 Test_loss : 352385520000000.0\n",
      "Epoch : 127 Training_loss : 338423630000000.0 Test_loss : 338431240000000.0\n",
      "Epoch : 128 Training_loss : 325022050000000.0 Test_loss : 325029470000000.0\n",
      "Epoch : 129 Training_loss : 312151140000000.0 Test_loss : 312158420000000.0\n",
      "Epoch : 130 Training_loss : 299790000000000.0 Test_loss : 299797100000000.0\n",
      "Epoch : 131 Training_loss : 287918330000000.0 Test_loss : 287925300000000.0\n",
      "Epoch : 132 Training_loss : 276516760000000.0 Test_loss : 276523600000000.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 133 Training_loss : 265566670000000.0 Test_loss : 265573410000000.0\n",
      "Epoch : 134 Training_loss : 255050270000000.0 Test_loss : 255056850000000.0\n",
      "Epoch : 135 Training_loss : 244950270000000.0 Test_loss : 244956720000000.0\n",
      "Epoch : 136 Training_loss : 235250190000000.0 Test_loss : 235256570000000.0\n",
      "Epoch : 137 Training_loss : 225934300000000.0 Test_loss : 225940530000000.0\n",
      "Epoch : 138 Training_loss : 216987330000000.0 Test_loss : 216993390000000.0\n",
      "Epoch : 139 Training_loss : 208394630000000.0 Test_loss : 208400590000000.0\n",
      "Epoch : 140 Training_loss : 200142170000000.0 Test_loss : 200148030000000.0\n",
      "Epoch : 141 Training_loss : 192216530000000.0 Test_loss : 192222270000000.0\n",
      "Epoch : 142 Training_loss : 184604800000000.0 Test_loss : 184610400000000.0\n",
      "Epoch : 143 Training_loss : 177294420000000.0 Test_loss : 177299940000000.0\n",
      "Epoch : 144 Training_loss : 170273580000000.0 Test_loss : 170278960000000.0\n",
      "Epoch : 145 Training_loss : 163530750000000.0 Test_loss : 163536010000000.0\n",
      "Epoch : 146 Training_loss : 157054940000000.0 Test_loss : 157060110000000.0\n",
      "Epoch : 147 Training_loss : 150835560000000.0 Test_loss : 150840630000000.0\n",
      "Epoch : 148 Training_loss : 144862490000000.0 Test_loss : 144867430000000.0\n",
      "Epoch : 149 Training_loss : 139125930000000.0 Test_loss : 139130790000000.0\n",
      "Epoch : 150 Training_loss : 133616520000000.0 Test_loss : 133621300000000.0\n",
      "Epoch : 151 Training_loss : 128325310000000.0 Test_loss : 128330000000000.0\n",
      "Epoch : 152 Training_loss : 123243630000000.0 Test_loss : 123248230000000.0\n",
      "Epoch : 153 Training_loss : 118363225000000.0 Test_loss : 118367710000000.0\n",
      "Epoch : 154 Training_loss : 113676020000000.0 Test_loss : 113680420000000.0\n",
      "Epoch : 155 Training_loss : 109174450000000.0 Test_loss : 109178765000000.0\n",
      "Epoch : 156 Training_loss : 104851150000000.0 Test_loss : 104855380000000.0\n",
      "Epoch : 157 Training_loss : 100699030000000.0 Test_loss : 100703175000000.0\n",
      "Epoch : 158 Training_loss : 96711350000000.0 Test_loss : 96715410000000.0\n",
      "Epoch : 159 Training_loss : 92881590000000.0 Test_loss : 92885555000000.0\n",
      "Epoch : 160 Training_loss : 89203480000000.0 Test_loss : 89207370000000.0\n",
      "Epoch : 161 Training_loss : 85671025000000.0 Test_loss : 85674825000000.0\n",
      "Epoch : 162 Training_loss : 82278450000000.0 Test_loss : 82282190000000.0\n",
      "Epoch : 163 Training_loss : 79020220000000.0 Test_loss : 79023880000000.0\n",
      "Epoch : 164 Training_loss : 75891020000000.0 Test_loss : 75894605000000.0\n",
      "Epoch : 165 Training_loss : 72885740000000.0 Test_loss : 72889240000000.0\n",
      "Epoch : 166 Training_loss : 69999460000000.0 Test_loss : 70002910000000.0\n",
      "Epoch : 167 Training_loss : 67227480000000.0 Test_loss : 67230860000000.0\n",
      "Epoch : 168 Training_loss : 64565280000000.0 Test_loss : 64568580000000.0\n",
      "Epoch : 169 Training_loss : 62008494000000.0 Test_loss : 62011736000000.0\n",
      "Epoch : 170 Training_loss : 59552955000000.0 Test_loss : 59556130000000.0\n",
      "Epoch : 171 Training_loss : 57194660000000.0 Test_loss : 57197780000000.0\n",
      "Epoch : 172 Training_loss : 54929750000000.0 Test_loss : 54932804000000.0\n",
      "Epoch : 173 Training_loss : 52754534000000.0 Test_loss : 52757524000000.0\n",
      "Epoch : 174 Training_loss : 50665447000000.0 Test_loss : 50668388000000.0\n",
      "Epoch : 175 Training_loss : 48659102000000.0 Test_loss : 48661975000000.0\n",
      "Epoch : 176 Training_loss : 46732197000000.0 Test_loss : 46735024000000.0\n",
      "Epoch : 177 Training_loss : 44881607000000.0 Test_loss : 44884367000000.0\n",
      "Epoch : 178 Training_loss : 43104290000000.0 Test_loss : 43107000000000.0\n",
      "Epoch : 179 Training_loss : 41397365000000.0 Test_loss : 41400020000000.0\n",
      "Epoch : 180 Training_loss : 39758034000000.0 Test_loss : 39760635000000.0\n",
      "Epoch : 181 Training_loss : 38183614000000.0 Test_loss : 38186156000000.0\n",
      "Epoch : 182 Training_loss : 36671540000000.0 Test_loss : 36674034000000.0\n",
      "Epoch : 183 Training_loss : 35219350000000.0 Test_loss : 35221794000000.0\n",
      "Epoch : 184 Training_loss : 33824658000000.0 Test_loss : 33827055000000.0\n",
      "Epoch : 185 Training_loss : 32485200000000.0 Test_loss : 32487554000000.0\n",
      "Epoch : 186 Training_loss : 31198787000000.0 Test_loss : 31201092000000.0\n",
      "Epoch : 187 Training_loss : 29963320000000.0 Test_loss : 29965574000000.0\n",
      "Epoch : 188 Training_loss : 28776772000000.0 Test_loss : 28778984000000.0\n",
      "Epoch : 189 Training_loss : 27637213000000.0 Test_loss : 27639377000000.0\n",
      "Epoch : 190 Training_loss : 26542783000000.0 Test_loss : 26544903000000.0\n",
      "Epoch : 191 Training_loss : 25491684000000.0 Test_loss : 25493766000000.0\n",
      "Epoch : 192 Training_loss : 24482215000000.0 Test_loss : 24484254000000.0\n",
      "Epoch : 193 Training_loss : 23512719000000.0 Test_loss : 23514717000000.0\n",
      "Epoch : 194 Training_loss : 22581617000000.0 Test_loss : 22583571000000.0\n",
      "Epoch : 195 Training_loss : 21687383000000.0 Test_loss : 21689304000000.0\n",
      "Epoch : 196 Training_loss : 20828561000000.0 Test_loss : 20830445000000.0\n",
      "Epoch : 197 Training_loss : 20003751000000.0 Test_loss : 20005597000000.0\n",
      "Epoch : 198 Training_loss : 19211607000000.0 Test_loss : 19213412000000.0\n",
      "Epoch : 199 Training_loss : 18450825000000.0 Test_loss : 18452597000000.0\n",
      "Epoch : 200 Training_loss : 17720173000000.0 Test_loss : 17721910000000.0\n",
      "Epoch : 201 Training_loss : 17018455000000.0 Test_loss : 17020155000000.0\n",
      "Epoch : 202 Training_loss : 16344527000000.0 Test_loss : 16346193000000.0\n",
      "Epoch : 203 Training_loss : 15697282000000.0 Test_loss : 15698915000000.0\n",
      "Epoch : 204 Training_loss : 15075671000000.0 Test_loss : 15077271000000.0\n",
      "Epoch : 205 Training_loss : 14478674000000.0 Test_loss : 14480243000000.0\n",
      "Epoch : 206 Training_loss : 13905317000000.0 Test_loss : 13906857000000.0\n",
      "Epoch : 207 Training_loss : 13354668000000.0 Test_loss : 13356175000000.0\n",
      "Epoch : 208 Training_loss : 12825822000000.0 Test_loss : 12827298000000.0\n",
      "Epoch : 209 Training_loss : 12317920000000.0 Test_loss : 12319366000000.0\n",
      "Epoch : 210 Training_loss : 11830131000000.0 Test_loss : 11831550000000.0\n",
      "Epoch : 211 Training_loss : 11361659000000.0 Test_loss : 11363047000000.0\n",
      "Epoch : 212 Training_loss : 10911738000000.0 Test_loss : 10913099000000.0\n",
      "Epoch : 213 Training_loss : 10479633000000.0 Test_loss : 10480968000000.0\n",
      "Epoch : 214 Training_loss : 10064638000000.0 Test_loss : 10065947000000.0\n",
      "Epoch : 215 Training_loss : 9666081000000.0 Test_loss : 9667362000000.0\n",
      "Epoch : 216 Training_loss : 9283303000000.0 Test_loss : 9284559000000.0\n",
      "Epoch : 217 Training_loss : 8915685000000.0 Test_loss : 8916915000000.0\n",
      "Epoch : 218 Training_loss : 8562623400000.0 Test_loss : 8563830000000.0\n",
      "Epoch : 219 Training_loss : 8223543300000.0 Test_loss : 8224726000000.0\n",
      "Epoch : 220 Training_loss : 7897890700000.0 Test_loss : 7899049400000.0\n",
      "Epoch : 221 Training_loss : 7585135000000.0 Test_loss : 7586271700000.0\n",
      "Epoch : 222 Training_loss : 7284764700000.0 Test_loss : 7285875700000.0\n",
      "Epoch : 223 Training_loss : 6996288300000.0 Test_loss : 6997378300000.0\n",
      "Epoch : 224 Training_loss : 6719235000000.0 Test_loss : 6720304000000.0\n",
      "Epoch : 225 Training_loss : 6453152700000.0 Test_loss : 6454199700000.0\n",
      "Epoch : 226 Training_loss : 6197608400000.0 Test_loss : 6198634500000.0\n",
      "Epoch : 227 Training_loss : 5952183000000.0 Test_loss : 5953188500000.0\n",
      "Epoch : 228 Training_loss : 5716476700000.0 Test_loss : 5717462000000.0\n",
      "Epoch : 229 Training_loss : 5490105000000.0 Test_loss : 5491070000000.0\n",
      "Epoch : 230 Training_loss : 5272696000000.0 Test_loss : 5273642600000.0\n",
      "Epoch : 231 Training_loss : 5063897000000.0 Test_loss : 5064825000000.0\n",
      "Epoch : 232 Training_loss : 4863367000000.0 Test_loss : 4864276000000.0\n",
      "Epoch : 233 Training_loss : 4670778000000.0 Test_loss : 4671669000000.0\n",
      "Epoch : 234 Training_loss : 4485815000000.0 Test_loss : 4486688000000.0\n",
      "Epoch : 235 Training_loss : 4308176200000.0 Test_loss : 4309032000000.0\n",
      "Epoch : 236 Training_loss : 4137573000000.0 Test_loss : 4138411000000.0\n",
      "Epoch : 237 Training_loss : 3973725300000.0 Test_loss : 3974546800000.0\n",
      "Epoch : 238 Training_loss : 3816365800000.0 Test_loss : 3817171300000.0\n",
      "Epoch : 239 Training_loss : 3665238000000.0 Test_loss : 3666027000000.0\n",
      "Epoch : 240 Training_loss : 3520094800000.0 Test_loss : 3520867900000.0\n",
      "Epoch : 241 Training_loss : 3380699200000.0 Test_loss : 3381456500000.0\n",
      "Epoch : 242 Training_loss : 3246823000000.0 Test_loss : 3247565400000.0\n",
      "Epoch : 243 Training_loss : 3118249000000.0 Test_loss : 3118976500000.0\n",
      "Epoch : 244 Training_loss : 2994765800000.0 Test_loss : 2995479400000.0\n",
      "Epoch : 245 Training_loss : 2876173500000.0 Test_loss : 2876872300000.0\n",
      "Epoch : 246 Training_loss : 2762277400000.0 Test_loss : 2762962400000.0\n",
      "Epoch : 247 Training_loss : 2652891000000.0 Test_loss : 2653562300000.0\n",
      "Epoch : 248 Training_loss : 2547837000000.0 Test_loss : 2548494500000.0\n",
      "Epoch : 249 Training_loss : 2446942800000.0 Test_loss : 2447587100000.0\n",
      "Epoch : 250 Training_loss : 2350044000000.0 Test_loss : 2350675400000.0\n",
      "Epoch : 251 Training_loss : 2256981700000.0 Test_loss : 2257601000000.0\n",
      "Epoch : 252 Training_loss : 2167605600000.0 Test_loss : 2168212000000.0\n",
      "Epoch : 253 Training_loss : 2081768100000.0 Test_loss : 2082362800000.0\n",
      "Epoch : 254 Training_loss : 1999330500000.0 Test_loss : 1999913100000.0\n",
      "Epoch : 255 Training_loss : 1920157100000.0 Test_loss : 1920727900000.0\n",
      "Epoch : 256 Training_loss : 1844119200000.0 Test_loss : 1844678400000.0\n",
      "Epoch : 257 Training_loss : 1771092200000.0 Test_loss : 1771640300000.0\n",
      "Epoch : 258 Training_loss : 1700957000000.0 Test_loss : 1701494100000.0\n",
      "Epoch : 259 Training_loss : 1633599100000.0 Test_loss : 1634125500000.0\n",
      "Epoch : 260 Training_loss : 1568908200000.0 Test_loss : 1569424300000.0\n",
      "Epoch : 261 Training_loss : 1506779600000.0 Test_loss : 1507285300000.0\n",
      "Epoch : 262 Training_loss : 1447111200000.0 Test_loss : 1447606700000.0\n",
      "Epoch : 263 Training_loss : 1389805700000.0 Test_loss : 1390291300000.0\n",
      "Epoch : 264 Training_loss : 1334769600000.0 Test_loss : 1335245500000.0\n",
      "Epoch : 265 Training_loss : 1281912700000.0 Test_loss : 1282379400000.0\n",
      "Epoch : 266 Training_loss : 1231149300000.0 Test_loss : 1231606300000.0\n",
      "Epoch : 267 Training_loss : 1182395900000.0 Test_loss : 1182843900000.0\n",
      "Epoch : 268 Training_loss : 1135573200000.0 Test_loss : 1136012200000.0\n",
      "Epoch : 269 Training_loss : 1090604600000.0 Test_loss : 1091034740000.0\n",
      "Epoch : 270 Training_loss : 1047416700000.0 Test_loss : 1047838400000.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 271 Training_loss : 1005939000000.0 Test_loss : 1006352140000.0\n",
      "Epoch : 272 Training_loss : 966103900000.0 Test_loss : 966508740000.0\n",
      "Epoch : 273 Training_loss : 927846040000.0 Test_loss : 928242900000.0\n",
      "Epoch : 274 Training_loss : 891103540000.0 Test_loss : 891492300000.0\n",
      "Epoch : 275 Training_loss : 855815900000.0 Test_loss : 856196900000.0\n",
      "Epoch : 276 Training_loss : 821925700000.0 Test_loss : 822299000000.0\n",
      "Epoch : 277 Training_loss : 789377400000.0 Test_loss : 789743340000.0\n",
      "Epoch : 278 Training_loss : 758118150000.0 Test_loss : 758476700000.0\n",
      "Epoch : 279 Training_loss : 728096640000.0 Test_loss : 728448100000.0\n",
      "Epoch : 280 Training_loss : 699264140000.0 Test_loss : 699608500000.0\n",
      "Epoch : 281 Training_loss : 671573340000.0 Test_loss : 671910850000.0\n",
      "Epoch : 282 Training_loss : 644979160000.0 Test_loss : 645309860000.0\n",
      "Epoch : 283 Training_loss : 619437950000.0 Test_loss : 619762000000.0\n",
      "Epoch : 284 Training_loss : 594908350000.0 Test_loss : 595225900000.0\n",
      "Epoch : 285 Training_loss : 571349900000.0 Test_loss : 571661160000.0\n",
      "Epoch : 286 Training_loss : 548724500000.0 Test_loss : 549029580000.0\n",
      "Epoch : 287 Training_loss : 526995100000.0 Test_loss : 527294000000.0\n",
      "Epoch : 288 Training_loss : 506126100000.0 Test_loss : 506419020000.0\n",
      "Epoch : 289 Training_loss : 486083600000.0 Test_loss : 486370640000.0\n",
      "Epoch : 290 Training_loss : 466834720000.0 Test_loss : 467115970000.0\n",
      "Epoch : 291 Training_loss : 448348030000.0 Test_loss : 448623770000.0\n",
      "Epoch : 292 Training_loss : 430593500000.0 Test_loss : 430863650000.0\n",
      "Epoch : 293 Training_loss : 413542100000.0 Test_loss : 413806850000.0\n",
      "Epoch : 294 Training_loss : 397165850000.0 Test_loss : 397425300000.0\n",
      "Epoch : 295 Training_loss : 381438230000.0 Test_loss : 381692440000.0\n",
      "Epoch : 296 Training_loss : 366333260000.0 Test_loss : 366582430000.0\n",
      "Epoch : 297 Training_loss : 351826540000.0 Test_loss : 352070700000.0\n",
      "Epoch : 298 Training_loss : 337894300000.0 Test_loss : 338133580000.0\n",
      "Epoch : 299 Training_loss : 324513800000.0 Test_loss : 324748280000.0\n",
      "Epoch : 300 Training_loss : 311663100000.0 Test_loss : 311892870000.0\n",
      "Epoch : 301 Training_loss : 299321300000.0 Test_loss : 299546440000.0\n",
      "Epoch : 302 Training_loss : 287468220000.0 Test_loss : 287688850000.0\n",
      "Epoch : 303 Training_loss : 276084520000.0 Test_loss : 276300760000.0\n",
      "Epoch : 304 Training_loss : 265151640000.0 Test_loss : 265363540000.0\n",
      "Epoch : 305 Training_loss : 254651710000.0 Test_loss : 254859380000.0\n",
      "Epoch : 306 Training_loss : 244567560000.0 Test_loss : 244771060000.0\n",
      "Epoch : 307 Training_loss : 234882740000.0 Test_loss : 235082190000.0\n",
      "Epoch : 308 Training_loss : 225581450000.0 Test_loss : 225776880000.0\n",
      "Epoch : 309 Training_loss : 216648480000.0 Test_loss : 216839980000.0\n",
      "Epoch : 310 Training_loss : 208069260000.0 Test_loss : 208256940000.0\n",
      "Epoch : 311 Training_loss : 199829770000.0 Test_loss : 200013700000.0\n",
      "Epoch : 312 Training_loss : 191916590000.0 Test_loss : 192096800000.0\n",
      "Epoch : 313 Training_loss : 184316760000.0 Test_loss : 184493380000.0\n",
      "Epoch : 314 Training_loss : 177017880000.0 Test_loss : 177190930000.0\n",
      "Epoch : 315 Training_loss : 170008000000.0 Test_loss : 170177610000.0\n",
      "Epoch : 316 Training_loss : 163275750000.0 Test_loss : 163441930000.0\n",
      "Epoch : 317 Training_loss : 156810080000.0 Test_loss : 156972940000.0\n",
      "Epoch : 318 Training_loss : 150600470000.0 Test_loss : 150760080000.0\n",
      "Epoch : 319 Training_loss : 144636760000.0 Test_loss : 144793160000.0\n",
      "Epoch : 320 Training_loss : 138909190000.0 Test_loss : 139062440000.0\n",
      "Epoch : 321 Training_loss : 133408450000.0 Test_loss : 133558630000.0\n",
      "Epoch : 322 Training_loss : 128125530000.0 Test_loss : 128272695000.0\n",
      "Epoch : 323 Training_loss : 123051820000.0 Test_loss : 123196040000.0\n",
      "Epoch : 324 Training_loss : 118179004000.0 Test_loss : 118320350000.0\n",
      "Epoch : 325 Training_loss : 113499180000.0 Test_loss : 113637680000.0\n",
      "Epoch : 326 Training_loss : 109004670000.0 Test_loss : 109140390000.0\n",
      "Epoch : 327 Training_loss : 104688150000.0 Test_loss : 104821146000.0\n",
      "Epoch : 328 Training_loss : 100542550000.0 Test_loss : 100672880000.0\n",
      "Epoch : 329 Training_loss : 96561110000.0 Test_loss : 96688830000.0\n",
      "Epoch : 330 Training_loss : 92737340000.0 Test_loss : 92862520000.0\n",
      "Epoch : 331 Training_loss : 89065020000.0 Test_loss : 89187656000.0\n",
      "Epoch : 332 Training_loss : 85538080000.0 Test_loss : 85658260000.0\n",
      "Epoch : 333 Training_loss : 82150834000.0 Test_loss : 82268610000.0\n",
      "Epoch : 334 Training_loss : 78897710000.0 Test_loss : 79013134000.0\n",
      "Epoch : 335 Training_loss : 75773420000.0 Test_loss : 75886520000.0\n",
      "Epoch : 336 Training_loss : 72772850000.0 Test_loss : 72883680000.0\n",
      "Epoch : 337 Training_loss : 69891100000.0 Test_loss : 69999700000.0\n",
      "Epoch : 338 Training_loss : 67123470000.0 Test_loss : 67229897000.0\n",
      "Epoch : 339 Training_loss : 64465445000.0 Test_loss : 64569733000.0\n",
      "Epoch : 340 Training_loss : 61912680000.0 Test_loss : 62014860000.0\n",
      "Epoch : 341 Training_loss : 59460985000.0 Test_loss : 59561120000.0\n",
      "Epoch : 342 Training_loss : 57106390000.0 Test_loss : 57204515000.0\n",
      "Epoch : 343 Training_loss : 54845034000.0 Test_loss : 54941192000.0\n",
      "Epoch : 344 Training_loss : 52673230000.0 Test_loss : 52767453000.0\n",
      "Epoch : 345 Training_loss : 50587427000.0 Test_loss : 50679770000.0\n",
      "Epoch : 346 Training_loss : 48584220000.0 Test_loss : 48674700000.0\n",
      "Epoch : 347 Training_loss : 46660340000.0 Test_loss : 46749004000.0\n",
      "Epoch : 348 Training_loss : 44812650000.0 Test_loss : 44899533000.0\n",
      "Epoch : 349 Training_loss : 43038126000.0 Test_loss : 43123266000.0\n",
      "Epoch : 350 Training_loss : 41333870000.0 Test_loss : 41417298000.0\n",
      "Epoch : 351 Training_loss : 39697105000.0 Test_loss : 39778853000.0\n",
      "Epoch : 352 Training_loss : 38125160000.0 Test_loss : 38205270000.0\n",
      "Epoch : 353 Training_loss : 36615455000.0 Test_loss : 36693955000.0\n",
      "Epoch : 354 Training_loss : 35165544000.0 Test_loss : 35242463000.0\n",
      "Epoch : 355 Training_loss : 33773044000.0 Test_loss : 33848416000.0\n",
      "Epoch : 356 Training_loss : 32435692000.0 Test_loss : 32509544000.0\n",
      "Epoch : 357 Training_loss : 31151290000.0 Test_loss : 31223667000.0\n",
      "Epoch : 358 Training_loss : 29917755000.0 Test_loss : 29988678000.0\n",
      "Epoch : 359 Training_loss : 28733073000.0 Test_loss : 28802564000.0\n",
      "Epoch : 360 Training_loss : 27595300000.0 Test_loss : 27663393000.0\n",
      "Epoch : 361 Training_loss : 26502582000.0 Test_loss : 26569310000.0\n",
      "Epoch : 362 Training_loss : 25453140000.0 Test_loss : 25518520000.0\n",
      "Epoch : 363 Training_loss : 24445250000.0 Test_loss : 24509315000.0\n",
      "Epoch : 364 Training_loss : 23477273000.0 Test_loss : 23540050000.0\n",
      "Epoch : 365 Training_loss : 22547630000.0 Test_loss : 22609146000.0\n",
      "Epoch : 366 Training_loss : 21654800000.0 Test_loss : 21715075000.0\n",
      "Epoch : 367 Training_loss : 20797331000.0 Test_loss : 20856392000.0\n",
      "Epoch : 368 Training_loss : 19973810000.0 Test_loss : 20031685000.0\n",
      "Epoch : 369 Training_loss : 19182903000.0 Test_loss : 19239612000.0\n",
      "Epoch : 370 Training_loss : 18423319000.0 Test_loss : 18478883000.0\n",
      "Epoch : 371 Training_loss : 17693809000.0 Test_loss : 17748257000.0\n",
      "Epoch : 372 Training_loss : 16993191000.0 Test_loss : 17046540000.0\n",
      "Epoch : 373 Training_loss : 16320317000.0 Test_loss : 16372592000.0\n",
      "Epoch : 374 Training_loss : 15674090000.0 Test_loss : 15725311000.0\n",
      "Epoch : 375 Training_loss : 15053451000.0 Test_loss : 15103640000.0\n",
      "Epoch : 376 Training_loss : 14457391000.0 Test_loss : 14506569000.0\n",
      "Epoch : 377 Training_loss : 13884936000.0 Test_loss : 13933121000.0\n",
      "Epoch : 378 Training_loss : 13335150000.0 Test_loss : 13382364000.0\n",
      "Epoch : 379 Training_loss : 12807134000.0 Test_loss : 12853395000.0\n",
      "Epoch : 380 Training_loss : 12300029000.0 Test_loss : 12345357000.0\n",
      "Epoch : 381 Training_loss : 11813004000.0 Test_loss : 11857419000.0\n",
      "Epoch : 382 Training_loss : 11345266000.0 Test_loss : 11388782000.0\n",
      "Epoch : 383 Training_loss : 10896049000.0 Test_loss : 10938689000.0\n",
      "Epoch : 384 Training_loss : 10464621000.0 Test_loss : 10506400000.0\n",
      "Epoch : 385 Training_loss : 10050278000.0 Test_loss : 10091215000.0\n",
      "Epoch : 386 Training_loss : 9652343000.0 Test_loss : 9692453000.0\n",
      "Epoch : 387 Training_loss : 9270167000.0 Test_loss : 9309466000.0\n",
      "Epoch : 388 Training_loss : 8903126000.0 Test_loss : 8941630000.0\n",
      "Epoch : 389 Training_loss : 8550617600.0 Test_loss : 8588345000.0\n",
      "Epoch : 390 Training_loss : 8212070000.0 Test_loss : 8249035000.0\n",
      "Epoch : 391 Training_loss : 7886928400.0 Test_loss : 7923145700.0\n",
      "Epoch : 392 Training_loss : 7574662700.0 Test_loss : 7610146300.0\n",
      "Epoch : 393 Training_loss : 7274761000.0 Test_loss : 7309529000.0\n",
      "Epoch : 394 Training_loss : 6986737000.0 Test_loss : 7020801500.0\n",
      "Epoch : 395 Training_loss : 6710119400.0 Test_loss : 6743495000.0\n",
      "Epoch : 396 Training_loss : 6444455000.0 Test_loss : 6477154300.0\n",
      "Epoch : 397 Training_loss : 6189310500.0 Test_loss : 6221349400.0\n",
      "Epoch : 398 Training_loss : 5944271400.0 Test_loss : 5975660500.0\n",
      "Epoch : 399 Training_loss : 5708934000.0 Test_loss : 5739688000.0\n",
      "Epoch : 400 Training_loss : 5482917400.0 Test_loss : 5513047600.0\n",
      "Epoch : 401 Training_loss : 5265850000.0 Test_loss : 5295369700.0\n",
      "Epoch : 402 Training_loss : 5057379000.0 Test_loss : 5086301000.0\n",
      "Epoch : 403 Training_loss : 4857163000.0 Test_loss : 4885498400.0\n",
      "Epoch : 404 Training_loss : 4664876000.0 Test_loss : 4692637700.0\n",
      "Epoch : 405 Training_loss : 4480203300.0 Test_loss : 4507401000.0\n",
      "Epoch : 406 Training_loss : 4302844400.0 Test_loss : 4329490400.0\n",
      "Epoch : 407 Training_loss : 4132508000.0 Test_loss : 4158613200.0\n",
      "Epoch : 408 Training_loss : 3968917200.0 Test_loss : 3994492400.0\n",
      "Epoch : 409 Training_loss : 3811804400.0 Test_loss : 3836860400.0\n",
      "Epoch : 410 Training_loss : 3660913200.0 Test_loss : 3685460000.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 411 Training_loss : 3515997200.0 Test_loss : 3540045800.0\n",
      "Epoch : 412 Training_loss : 3376820700.0 Test_loss : 3400380000.0\n",
      "Epoch : 413 Training_loss : 3243155500.0 Test_loss : 3266235100.0\n",
      "Epoch : 414 Training_loss : 3114782500.0 Test_loss : 3137393400.0\n",
      "Epoch : 415 Training_loss : 2991493600.0 Test_loss : 3013644800.0\n",
      "Epoch : 416 Training_loss : 2873087000.0 Test_loss : 2894786800.0\n",
      "Epoch : 417 Training_loss : 2759369500.0 Test_loss : 2780627200.0\n",
      "Epoch : 418 Training_loss : 2650155000.0 Test_loss : 2670979300.0\n",
      "Epoch : 419 Training_loss : 2545265000.0 Test_loss : 2565665500.0\n",
      "Epoch : 420 Training_loss : 2444529400.0 Test_loss : 2464513300.0\n",
      "Epoch : 421 Training_loss : 2347782700.0 Test_loss : 2367359200.0\n",
      "Epoch : 422 Training_loss : 2254867000.0 Test_loss : 2274044200.0\n",
      "Epoch : 423 Training_loss : 2165630500.0 Test_loss : 2184416500.0\n",
      "Epoch : 424 Training_loss : 2079928200.0 Test_loss : 2098330500.0\n",
      "Epoch : 425 Training_loss : 1997619600.0 Test_loss : 2015645800.0\n",
      "Epoch : 426 Training_loss : 1918570400.0 Test_loss : 1936228000.0\n",
      "Epoch : 427 Training_loss : 1842651100.0 Test_loss : 1859948300.0\n",
      "Epoch : 428 Training_loss : 1769738800.0 Test_loss : 1786682000.0\n",
      "Epoch : 429 Training_loss : 1699713500.0 Test_loss : 1716310100.0\n",
      "Epoch : 430 Training_loss : 1632461400.0 Test_loss : 1648718100.0\n",
      "Epoch : 431 Training_loss : 1567872600.0 Test_loss : 1583796000.0\n",
      "Epoch : 432 Training_loss : 1505841300.0 Test_loss : 1521438300.0\n",
      "Epoch : 433 Training_loss : 1446266500.0 Test_loss : 1461544100.0\n",
      "Epoch : 434 Training_loss : 1389050600.0 Test_loss : 1404014700.0\n",
      "Epoch : 435 Training_loss : 1334100700.0 Test_loss : 1348757800.0\n",
      "Epoch : 436 Training_loss : 1281327100.0 Test_loss : 1295682700.0\n",
      "Epoch : 437 Training_loss : 1230643000.0 Test_loss : 1244703700.0\n",
      "Epoch : 438 Training_loss : 1181966000.0 Test_loss : 1195737600.0\n",
      "Epoch : 439 Training_loss : 1135216600.0 Test_loss : 1148705000.0\n",
      "Epoch : 440 Training_loss : 1090318500.0 Test_loss : 1103529200.0\n",
      "Epoch : 441 Training_loss : 1047198460.0 Test_loss : 1060137000.0\n",
      "Epoch : 442 Training_loss : 1005785900.0 Test_loss : 1018457800.0\n",
      "Epoch : 443 Training_loss : 966013250.0 Test_loss : 978423800.0\n",
      "Epoch : 444 Training_loss : 927815700.0 Test_loss : 939970050.0\n",
      "Epoch : 445 Training_loss : 891130600.0 Test_loss : 903034100.0\n",
      "Epoch : 446 Training_loss : 855898430.0 Test_loss : 867555900.0\n",
      "Epoch : 447 Training_loss : 822061440.0 Test_loss : 833477800.0\n",
      "Epoch : 448 Training_loss : 789564160.0 Test_loss : 800744400.0\n",
      "Epoch : 449 Training_loss : 758354000.0 Test_loss : 769302700.0\n",
      "Epoch : 450 Training_loss : 728379700.0 Test_loss : 739101600.0\n",
      "Epoch : 451 Training_loss : 699592400.0 Test_loss : 710091900.0\n",
      "Epoch : 452 Training_loss : 671945000.0 Test_loss : 682226700.0\n",
      "Epoch : 453 Training_loss : 645392450.0 Test_loss : 655460540.0\n",
      "Epoch : 454 Training_loss : 619891460.0 Test_loss : 629750300.0\n",
      "Epoch : 455 Training_loss : 595400200.0 Test_loss : 605054000.0\n",
      "Epoch : 456 Training_loss : 571878900.0 Test_loss : 581331700.0\n",
      "Epoch : 457 Training_loss : 549289000.0 Test_loss : 558544960.0\n",
      "Epoch : 458 Training_loss : 527593660.0 Test_loss : 536656540.0\n",
      "Epoch : 459 Training_loss : 506757400.0 Test_loss : 515631230.0\n",
      "Epoch : 460 Training_loss : 486746370.0 Test_loss : 495434700.0\n",
      "Epoch : 461 Training_loss : 467527680.0 Test_loss : 476034340.0\n",
      "Epoch : 462 Training_loss : 449070100.0 Test_loss : 457398800.0\n",
      "Epoch : 463 Training_loss : 431343420.0 Test_loss : 439497630.0\n",
      "Epoch : 464 Training_loss : 414318750.0 Test_loss : 422301950.0\n",
      "Epoch : 465 Training_loss : 397968200.0 Test_loss : 405783940.0\n",
      "Epoch : 466 Training_loss : 382265180.0 Test_loss : 389916640.0\n",
      "Epoch : 467 Training_loss : 367183970.0 Test_loss : 374674500.0\n",
      "Epoch : 468 Training_loss : 352700000.0 Test_loss : 360032830.0\n",
      "Epoch : 469 Training_loss : 338789570.0 Test_loss : 345967900.0\n",
      "Epoch : 470 Training_loss : 325429980.0 Test_loss : 332456830.0\n",
      "Epoch : 471 Training_loss : 312599520.0 Test_loss : 319477920.0\n",
      "Epoch : 472 Training_loss : 300277020.0 Test_loss : 307010000.0\n",
      "Epoch : 473 Training_loss : 288442560.0 Test_loss : 295032960.0\n",
      "Epoch : 474 Training_loss : 277076740.0 Test_loss : 283527460.0\n",
      "Epoch : 475 Training_loss : 266161000.0 Test_loss : 272474800.0\n",
      "Epoch : 476 Training_loss : 255677520.0 Test_loss : 261857180.0\n",
      "Epoch : 477 Training_loss : 245609200.0 Test_loss : 251657360.0\n",
      "Epoch : 478 Training_loss : 235939600.0 Test_loss : 241858880.0\n",
      "Epoch : 479 Training_loss : 226652880.0 Test_loss : 232445890.0\n",
      "Epoch : 480 Training_loss : 217733940.0 Test_loss : 223403180.0\n",
      "Epoch : 481 Training_loss : 209168160.0 Test_loss : 214716180.0\n",
      "Epoch : 482 Training_loss : 200941620.0 Test_loss : 206370770.0\n",
      "Epoch : 483 Training_loss : 193040880.0 Test_loss : 198353520.0\n",
      "Epoch : 484 Training_loss : 185452930.0 Test_loss : 190651460.0\n",
      "Epoch : 485 Training_loss : 178165490.0 Test_loss : 183252180.0\n",
      "Epoch : 486 Training_loss : 171166660.0 Test_loss : 176143700.0\n",
      "Epoch : 487 Training_loss : 164444960.0 Test_loss : 169314600.0\n",
      "Epoch : 488 Training_loss : 157989440.0 Test_loss : 162753760.0\n",
      "Epoch : 489 Training_loss : 151789570.0 Test_loss : 156450700.0\n",
      "Epoch : 490 Training_loss : 145835200.0 Test_loss : 150395220.0\n",
      "Epoch : 491 Training_loss : 140116640.0 Test_loss : 144577570.0\n",
      "Epoch : 492 Training_loss : 134624530.0 Test_loss : 138988340.0\n",
      "Epoch : 493 Training_loss : 129349896.0 Test_loss : 133618540.0\n",
      "Epoch : 494 Training_loss : 124284160.0 Test_loss : 128459540.0\n",
      "Epoch : 495 Training_loss : 119419000.0 Test_loss : 123503000.0\n",
      "Epoch : 496 Training_loss : 114746530.0 Test_loss : 118740940.0\n",
      "Epoch : 497 Training_loss : 110259060.0 Test_loss : 114165720.0\n",
      "Epoch : 498 Training_loss : 105949300.0 Test_loss : 109769930.0\n",
      "Epoch : 499 Training_loss : 101810216.0 Test_loss : 105546530.0\n",
      "Epoch : 500 Training_loss : 97835030.0 Test_loss : 101488730.0\n",
      "Epoch : 501 Training_loss : 94017280.0 Test_loss : 97590010.0\n",
      "Epoch : 502 Training_loss : 90350710.0 Test_loss : 93844100.0\n",
      "Epoch : 503 Training_loss : 86829304.0 Test_loss : 90244936.0\n",
      "Epoch : 504 Training_loss : 83447384.0 Test_loss : 86786810.0\n",
      "Epoch : 505 Training_loss : 80199360.0 Test_loss : 83464120.0\n",
      "Epoch : 506 Training_loss : 77079970.0 Test_loss : 80271540.0\n",
      "Epoch : 507 Training_loss : 74084120.0 Test_loss : 77203960.0\n",
      "Epoch : 508 Training_loss : 71206880.0 Test_loss : 74256450.0\n",
      "Epoch : 509 Training_loss : 68443600.0 Test_loss : 71424280.0\n",
      "Epoch : 510 Training_loss : 65789748.0 Test_loss : 68702930.0\n",
      "Epoch : 511 Training_loss : 63240984.0 Test_loss : 66088000.0\n",
      "Epoch : 512 Training_loss : 60793140.0 Test_loss : 63575320.0\n",
      "Epoch : 513 Training_loss : 58442228.0 Test_loss : 61160880.0\n",
      "Epoch : 514 Training_loss : 56184412.0 Test_loss : 58840796.0\n",
      "Epoch : 515 Training_loss : 54016024.0 Test_loss : 56611384.0\n",
      "Epoch : 516 Training_loss : 51933490.0 Test_loss : 54469050.0\n",
      "Epoch : 517 Training_loss : 49933416.0 Test_loss : 52410380.0\n",
      "Epoch : 518 Training_loss : 48012560.0 Test_loss : 50432096.0\n",
      "Epoch : 519 Training_loss : 46167756.0 Test_loss : 48531016.0\n",
      "Epoch : 520 Training_loss : 44396016.0 Test_loss : 46704110.0\n",
      "Epoch : 521 Training_loss : 42694450.0 Test_loss : 44948492.0\n",
      "Epoch : 522 Training_loss : 41060236.0 Test_loss : 43261308.0\n",
      "Epoch : 523 Training_loss : 39490756.0 Test_loss : 41639916.0\n",
      "Epoch : 524 Training_loss : 37983420.0 Test_loss : 40081700.0\n",
      "Epoch : 525 Training_loss : 36535784.0 Test_loss : 38584204.0\n",
      "Epoch : 526 Training_loss : 35145460.0 Test_loss : 37145030.0\n",
      "Epoch : 527 Training_loss : 33810210.0 Test_loss : 35761900.0\n",
      "Epoch : 528 Training_loss : 32527836.0 Test_loss : 34432596.0\n",
      "Epoch : 529 Training_loss : 31296226.0 Test_loss : 33155004.0\n",
      "Epoch : 530 Training_loss : 30113390.0 Test_loss : 31927102.0\n",
      "Epoch : 531 Training_loss : 28977408.0 Test_loss : 30746952.0\n",
      "Epoch : 532 Training_loss : 27886418.0 Test_loss : 29612674.0\n",
      "Epoch : 533 Training_loss : 26838626.0 Test_loss : 28522470.0\n",
      "Epoch : 534 Training_loss : 25832320.0 Test_loss : 27474596.0\n",
      "Epoch : 535 Training_loss : 24865854.0 Test_loss : 26467394.0\n",
      "Epoch : 536 Training_loss : 23937664.0 Test_loss : 25499280.0\n",
      "Epoch : 537 Training_loss : 23046242.0 Test_loss : 24568736.0\n",
      "Epoch : 538 Training_loss : 22190108.0 Test_loss : 23674260.0\n",
      "Epoch : 539 Training_loss : 21367888.0 Test_loss : 22814466.0\n",
      "Epoch : 540 Training_loss : 20578224.0 Test_loss : 21987980.0\n",
      "Epoch : 541 Training_loss : 19819822.0 Test_loss : 21193492.0\n",
      "Epoch : 542 Training_loss : 19091466.0 Test_loss : 20429772.0\n",
      "Epoch : 543 Training_loss : 18391948.0 Test_loss : 19695596.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 544 Training_loss : 17720134.0 Test_loss : 18989818.0\n",
      "Epoch : 545 Training_loss : 17074916.0 Test_loss : 18311314.0\n",
      "Epoch : 546 Training_loss : 16455254.0 Test_loss : 17659034.0\n",
      "Epoch : 547 Training_loss : 15860127.0 Test_loss : 17031942.0\n",
      "Epoch : 548 Training_loss : 15288568.0 Test_loss : 16429055.0\n",
      "Epoch : 549 Training_loss : 14739643.0 Test_loss : 15849427.0\n",
      "Epoch : 550 Training_loss : 14212462.0 Test_loss : 15292160.0\n",
      "Epoch : 551 Training_loss : 13706150.0 Test_loss : 14756367.0\n",
      "Epoch : 552 Training_loss : 13219892.0 Test_loss : 14241210.0\n",
      "Epoch : 553 Training_loss : 12752888.0 Test_loss : 13745890.0\n",
      "Epoch : 554 Training_loss : 12304376.0 Test_loss : 13269626.0\n",
      "Epoch : 555 Training_loss : 11873620.0 Test_loss : 12811676.0\n",
      "Epoch : 556 Training_loss : 11459932.0 Test_loss : 12371334.0\n",
      "Epoch : 557 Training_loss : 11062625.0 Test_loss : 11947909.0\n",
      "Epoch : 558 Training_loss : 10681043.0 Test_loss : 11540728.0\n",
      "Epoch : 559 Training_loss : 10314575.0 Test_loss : 11149175.0\n",
      "Epoch : 560 Training_loss : 9962618.0 Test_loss : 10772636.0\n",
      "Epoch : 561 Training_loss : 9624605.0 Test_loss : 10410531.0\n",
      "Epoch : 562 Training_loss : 9299973.0 Test_loss : 10062290.0\n",
      "Epoch : 563 Training_loss : 8988203.0 Test_loss : 9727382.0\n",
      "Epoch : 564 Training_loss : 8688771.0 Test_loss : 9405277.0\n",
      "Epoch : 565 Training_loss : 8401196.0 Test_loss : 9095480.0\n",
      "Epoch : 566 Training_loss : 8125011.0 Test_loss : 8797517.0\n",
      "Epoch : 567 Training_loss : 7859762.5 Test_loss : 8510929.0\n",
      "Epoch : 568 Training_loss : 7605019.0 Test_loss : 8235269.5\n",
      "Epoch : 569 Training_loss : 7360367.5 Test_loss : 7970122.5\n",
      "Epoch : 570 Training_loss : 7125405.0 Test_loss : 7715074.0\n",
      "Epoch : 571 Training_loss : 6899737.5 Test_loss : 7469722.5\n",
      "Epoch : 572 Training_loss : 6683008.5 Test_loss : 7233703.0\n",
      "Epoch : 573 Training_loss : 6474861.0 Test_loss : 7006649.5\n",
      "Epoch : 574 Training_loss : 6274960.0 Test_loss : 6788222.0\n",
      "Epoch : 575 Training_loss : 6082974.0 Test_loss : 6578079.5\n",
      "Epoch : 576 Training_loss : 5898590.5 Test_loss : 6375903.5\n",
      "Epoch : 577 Training_loss : 5721510.5 Test_loss : 6181385.0\n",
      "Epoch : 578 Training_loss : 5551441.5 Test_loss : 5994227.5\n",
      "Epoch : 579 Training_loss : 5388111.5 Test_loss : 5814151.5\n",
      "Epoch : 580 Training_loss : 5231243.0 Test_loss : 5640870.5\n",
      "Epoch : 581 Training_loss : 5080589.0 Test_loss : 5474133.5\n",
      "Epoch : 582 Training_loss : 4935904.5 Test_loss : 5313687.0\n",
      "Epoch : 583 Training_loss : 4796946.5 Test_loss : 5159282.0\n",
      "Epoch : 584 Training_loss : 4663493.5 Test_loss : 5010692.5\n",
      "Epoch : 585 Training_loss : 4535325.5 Test_loss : 4867689.5\n",
      "Epoch : 586 Training_loss : 4412230.5 Test_loss : 4730055.0\n",
      "Epoch : 587 Training_loss : 4294008.5 Test_loss : 4597587.0\n",
      "Epoch : 588 Training_loss : 4180469.8 Test_loss : 4470085.5\n",
      "Epoch : 589 Training_loss : 4071428.5 Test_loss : 4347360.5\n",
      "Epoch : 590 Training_loss : 3966706.5 Test_loss : 4229229.0\n",
      "Epoch : 591 Training_loss : 3866131.2 Test_loss : 4115512.2\n",
      "Epoch : 592 Training_loss : 3769536.0 Test_loss : 4006038.2\n",
      "Epoch : 593 Training_loss : 3676766.8 Test_loss : 3900648.5\n",
      "Epoch : 594 Training_loss : 3587668.8 Test_loss : 3799181.5\n",
      "Epoch : 595 Training_loss : 3502103.0 Test_loss : 3701494.5\n",
      "Epoch : 596 Training_loss : 3419926.0 Test_loss : 3607439.0\n",
      "Epoch : 597 Training_loss : 3341003.0 Test_loss : 3516875.0\n",
      "Epoch : 598 Training_loss : 3265203.8 Test_loss : 3429668.0\n",
      "Epoch : 599 Training_loss : 3192405.8 Test_loss : 3345689.0\n",
      "Epoch : 600 Training_loss : 3122491.0 Test_loss : 3264818.8\n",
      "Epoch : 601 Training_loss : 3055344.8 Test_loss : 3186935.2\n",
      "Epoch : 602 Training_loss : 2990859.5 Test_loss : 3111926.5\n",
      "Epoch : 603 Training_loss : 2928925.5 Test_loss : 3039680.8\n",
      "Epoch : 604 Training_loss : 2869443.5 Test_loss : 2970092.8\n",
      "Epoch : 605 Training_loss : 2812317.5 Test_loss : 2903062.5\n",
      "Epoch : 606 Training_loss : 2757453.5 Test_loss : 2838492.8\n",
      "Epoch : 607 Training_loss : 2704762.0 Test_loss : 2776289.0\n",
      "Epoch : 608 Training_loss : 2654155.8 Test_loss : 2716361.5\n",
      "Epoch : 609 Training_loss : 2605555.8 Test_loss : 2658626.2\n",
      "Epoch : 610 Training_loss : 2558880.8 Test_loss : 2602999.0\n",
      "Epoch : 611 Training_loss : 2514052.8 Test_loss : 2549397.8\n",
      "Epoch : 612 Training_loss : 2471000.2 Test_loss : 2497747.2\n",
      "Epoch : 613 Training_loss : 2429650.5 Test_loss : 2447971.0\n",
      "Epoch : 614 Training_loss : 2389941.0 Test_loss : 2400004.2\n",
      "Epoch : 615 Training_loss : 2351803.8 Test_loss : 2353775.0\n",
      "Epoch : 616 Training_loss : 2315176.8 Test_loss : 2309217.5\n",
      "Epoch : 617 Training_loss : 2280000.2 Test_loss : 2266269.5\n",
      "Epoch : 618 Training_loss : 2246216.8 Test_loss : 2224869.8\n",
      "Epoch : 619 Training_loss : 2213770.2 Test_loss : 2184959.2\n",
      "Epoch : 620 Training_loss : 2182607.8 Test_loss : 2146481.8\n",
      "Epoch : 621 Training_loss : 2152680.0 Test_loss : 2109385.5\n",
      "Epoch : 622 Training_loss : 2123937.5 Test_loss : 2073617.8\n",
      "Epoch : 623 Training_loss : 2096331.5 Test_loss : 2039126.8\n",
      "Epoch : 624 Training_loss : 2069818.5 Test_loss : 2005866.6\n",
      "Epoch : 625 Training_loss : 2044357.5 Test_loss : 1973793.2\n",
      "Epoch : 626 Training_loss : 2019903.4 Test_loss : 1942859.5\n",
      "Epoch : 627 Training_loss : 1996417.9 Test_loss : 1913023.6\n",
      "Epoch : 628 Training_loss : 1973863.5 Test_loss : 1884246.1\n",
      "Epoch : 629 Training_loss : 1952203.9 Test_loss : 1856487.9\n",
      "Epoch : 630 Training_loss : 1931400.0 Test_loss : 1829707.5\n",
      "Epoch : 631 Training_loss : 1911419.2 Test_loss : 1803869.0\n",
      "Epoch : 632 Training_loss : 1892230.8 Test_loss : 1778940.5\n",
      "Epoch : 633 Training_loss : 1873801.0 Test_loss : 1754885.0\n",
      "Epoch : 634 Training_loss : 1856101.5 Test_loss : 1731672.8\n",
      "Epoch : 635 Training_loss : 1839103.0 Test_loss : 1709272.0\n",
      "Epoch : 636 Training_loss : 1822777.5 Test_loss : 1687651.6\n",
      "Epoch : 637 Training_loss : 1807099.6 Test_loss : 1666785.6\n",
      "Epoch : 638 Training_loss : 1792043.2 Test_loss : 1646644.4\n",
      "Epoch : 639 Training_loss : 1777581.8 Test_loss : 1627200.1\n",
      "Epoch : 640 Training_loss : 1763693.9 Test_loss : 1608429.1\n",
      "Epoch : 641 Training_loss : 1750355.4 Test_loss : 1590304.6\n",
      "Epoch : 642 Training_loss : 1737545.2 Test_loss : 1572804.8\n",
      "Epoch : 643 Training_loss : 1725241.2 Test_loss : 1555904.4\n",
      "Epoch : 644 Training_loss : 1713424.6 Test_loss : 1539583.0\n",
      "Epoch : 645 Training_loss : 1702075.6 Test_loss : 1523819.8\n",
      "Epoch : 646 Training_loss : 1691177.2 Test_loss : 1508595.5\n",
      "Epoch : 647 Training_loss : 1680709.2 Test_loss : 1493887.6\n",
      "Epoch : 648 Training_loss : 1670657.2 Test_loss : 1479681.2\n",
      "Epoch : 649 Training_loss : 1661003.2 Test_loss : 1465955.5\n",
      "Epoch : 650 Training_loss : 1651730.9 Test_loss : 1452693.4\n",
      "Epoch : 651 Training_loss : 1642825.6 Test_loss : 1439877.8\n",
      "Epoch : 652 Training_loss : 1634272.8 Test_loss : 1427492.9\n",
      "Epoch : 653 Training_loss : 1626058.8 Test_loss : 1415522.5\n",
      "Epoch : 654 Training_loss : 1618170.8 Test_loss : 1403954.6\n",
      "Epoch : 655 Training_loss : 1610594.5 Test_loss : 1392771.8\n",
      "Epoch : 656 Training_loss : 1603319.2 Test_loss : 1381962.0\n",
      "Epoch : 657 Training_loss : 1596330.5 Test_loss : 1371509.1\n",
      "Epoch : 658 Training_loss : 1589618.5 Test_loss : 1361402.2\n",
      "Epoch : 659 Training_loss : 1583173.8 Test_loss : 1351630.4\n",
      "Epoch : 660 Training_loss : 1576982.9 Test_loss : 1342179.2\n",
      "Epoch : 661 Training_loss : 1571037.5 Test_loss : 1333038.2\n",
      "Epoch : 662 Training_loss : 1565328.1 Test_loss : 1324198.2\n",
      "Epoch : 663 Training_loss : 1559845.5 Test_loss : 1315647.5\n",
      "Epoch : 664 Training_loss : 1554579.5 Test_loss : 1307374.6\n",
      "Epoch : 665 Training_loss : 1549521.1 Test_loss : 1299369.1\n",
      "Epoch : 666 Training_loss : 1544664.1 Test_loss : 1291624.5\n",
      "Epoch : 667 Training_loss : 1539998.8 Test_loss : 1284128.4\n",
      "Epoch : 668 Training_loss : 1535518.8 Test_loss : 1276875.1\n",
      "Epoch : 669 Training_loss : 1531216.4 Test_loss : 1269854.9\n",
      "Epoch : 670 Training_loss : 1527083.6 Test_loss : 1263058.5\n",
      "Epoch : 671 Training_loss : 1523114.5 Test_loss : 1256478.8\n",
      "Epoch : 672 Training_loss : 1519302.8 Test_loss : 1250108.6\n",
      "Epoch : 673 Training_loss : 1515642.5 Test_loss : 1243941.4\n",
      "Epoch : 674 Training_loss : 1512127.0 Test_loss : 1237969.0\n",
      "Epoch : 675 Training_loss : 1508750.4 Test_loss : 1232184.8\n",
      "Epoch : 676 Training_loss : 1505507.9 Test_loss : 1226582.8\n",
      "Epoch : 677 Training_loss : 1502393.1 Test_loss : 1221155.2\n",
      "Epoch : 678 Training_loss : 1499402.2 Test_loss : 1215898.4\n",
      "Epoch : 679 Training_loss : 1496529.6 Test_loss : 1210804.6\n",
      "Epoch : 680 Training_loss : 1493770.4 Test_loss : 1205869.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 681 Training_loss : 1491120.6 Test_loss : 1201085.9\n",
      "Epoch : 682 Training_loss : 1488575.4 Test_loss : 1196450.4\n",
      "Epoch : 683 Training_loss : 1486131.5 Test_loss : 1191957.8\n",
      "Epoch : 684 Training_loss : 1483784.1 Test_loss : 1187603.0\n",
      "Epoch : 685 Training_loss : 1481530.2 Test_loss : 1183381.6\n",
      "Epoch : 686 Training_loss : 1479365.5 Test_loss : 1179289.1\n",
      "Epoch : 687 Training_loss : 1477286.4 Test_loss : 1175321.0\n",
      "Epoch : 688 Training_loss : 1475289.0 Test_loss : 1171471.5\n",
      "Epoch : 689 Training_loss : 1473370.5 Test_loss : 1167738.1\n",
      "Epoch : 690 Training_loss : 1471529.2 Test_loss : 1164118.5\n",
      "Epoch : 691 Training_loss : 1469760.4 Test_loss : 1160607.2\n",
      "Epoch : 692 Training_loss : 1468061.6 Test_loss : 1157200.4\n",
      "Epoch : 693 Training_loss : 1466429.6 Test_loss : 1153894.2\n",
      "Epoch : 694 Training_loss : 1464862.5 Test_loss : 1150686.9\n",
      "Epoch : 695 Training_loss : 1463357.9 Test_loss : 1147574.8\n",
      "Epoch : 696 Training_loss : 1461912.6 Test_loss : 1144554.4\n",
      "Epoch : 697 Training_loss : 1460524.8 Test_loss : 1141622.6\n",
      "Epoch : 698 Training_loss : 1459191.4 Test_loss : 1138776.4\n",
      "Epoch : 699 Training_loss : 1457911.0 Test_loss : 1136013.6\n",
      "Epoch : 700 Training_loss : 1456681.6 Test_loss : 1133331.5\n",
      "Epoch : 701 Training_loss : 1455501.0 Test_loss : 1130727.0\n",
      "Epoch : 702 Training_loss : 1454366.8 Test_loss : 1128197.2\n",
      "Epoch : 703 Training_loss : 1453277.8 Test_loss : 1125740.9\n",
      "Epoch : 704 Training_loss : 1452231.6 Test_loss : 1123355.0\n",
      "Epoch : 705 Training_loss : 1451226.9 Test_loss : 1121036.9\n",
      "Epoch : 706 Training_loss : 1450261.8 Test_loss : 1118783.9\n",
      "Epoch : 707 Training_loss : 1449334.4 Test_loss : 1116595.0\n",
      "Epoch : 708 Training_loss : 1448444.6 Test_loss : 1114468.9\n",
      "Epoch : 709 Training_loss : 1447589.5 Test_loss : 1112402.0\n",
      "Epoch : 710 Training_loss : 1446769.0 Test_loss : 1110394.2\n",
      "Epoch : 711 Training_loss : 1445980.4 Test_loss : 1108442.0\n",
      "Epoch : 712 Training_loss : 1445223.0 Test_loss : 1106544.5\n",
      "Epoch : 713 Training_loss : 1444495.6 Test_loss : 1104699.5\n",
      "Epoch : 714 Training_loss : 1443797.1 Test_loss : 1102905.8\n",
      "Epoch : 715 Training_loss : 1443126.1 Test_loss : 1101161.4\n",
      "Epoch : 716 Training_loss : 1442482.2 Test_loss : 1099465.6\n",
      "Epoch : 717 Training_loss : 1441863.4 Test_loss : 1097816.0\n",
      "Epoch : 718 Training_loss : 1441269.2 Test_loss : 1096212.1\n",
      "Epoch : 719 Training_loss : 1440698.5 Test_loss : 1094652.0\n",
      "Epoch : 720 Training_loss : 1440150.5 Test_loss : 1093133.5\n",
      "Epoch : 721 Training_loss : 1439624.0 Test_loss : 1091655.9\n",
      "Epoch : 722 Training_loss : 1439118.4 Test_loss : 1090218.6\n",
      "Epoch : 723 Training_loss : 1438632.6 Test_loss : 1088819.8\n",
      "Epoch : 724 Training_loss : 1438166.5 Test_loss : 1087459.0\n",
      "Epoch : 725 Training_loss : 1437718.8 Test_loss : 1086134.0\n",
      "Epoch : 726 Training_loss : 1437288.6 Test_loss : 1084845.0\n",
      "Epoch : 727 Training_loss : 1436875.5 Test_loss : 1083589.4\n",
      "Epoch : 728 Training_loss : 1436478.9 Test_loss : 1082367.2\n",
      "Epoch : 729 Training_loss : 1436097.9 Test_loss : 1081178.0\n",
      "Epoch : 730 Training_loss : 1435732.1 Test_loss : 1080019.8\n",
      "Epoch : 731 Training_loss : 1435380.8 Test_loss : 1078891.2\n",
      "Epoch : 732 Training_loss : 1435043.2 Test_loss : 1077793.0\n",
      "Epoch : 733 Training_loss : 1434719.2 Test_loss : 1076723.2\n",
      "Epoch : 734 Training_loss : 1434407.6 Test_loss : 1075680.6\n",
      "Epoch : 735 Training_loss : 1434108.8 Test_loss : 1074665.0\n",
      "Epoch : 736 Training_loss : 1433821.8 Test_loss : 1073675.5\n",
      "Epoch : 737 Training_loss : 1433546.1 Test_loss : 1072711.2\n",
      "Epoch : 738 Training_loss : 1433281.1 Test_loss : 1071772.4\n",
      "Epoch : 739 Training_loss : 1433026.9 Test_loss : 1070856.8\n",
      "Epoch : 740 Training_loss : 1432782.4 Test_loss : 1069965.1\n",
      "Epoch : 741 Training_loss : 1432547.9 Test_loss : 1069096.1\n",
      "Epoch : 742 Training_loss : 1432322.5 Test_loss : 1068249.1\n",
      "Epoch : 743 Training_loss : 1432106.2 Test_loss : 1067423.1\n",
      "Epoch : 744 Training_loss : 1431898.4 Test_loss : 1066617.8\n",
      "Epoch : 745 Training_loss : 1431698.9 Test_loss : 1065832.6\n",
      "Epoch : 746 Training_loss : 1431507.0 Test_loss : 1065067.9\n",
      "Epoch : 747 Training_loss : 1431323.2 Test_loss : 1064321.8\n",
      "Epoch : 748 Training_loss : 1431146.4 Test_loss : 1063594.0\n",
      "Epoch : 749 Training_loss : 1430976.4 Test_loss : 1062883.9\n",
      "Epoch : 750 Training_loss : 1430813.2 Test_loss : 1062191.6\n",
      "Epoch : 751 Training_loss : 1430657.0 Test_loss : 1061516.5\n",
      "Epoch : 752 Training_loss : 1430506.2 Test_loss : 1060857.9\n",
      "Epoch : 753 Training_loss : 1430361.8 Test_loss : 1060215.0\n",
      "Epoch : 754 Training_loss : 1430223.0 Test_loss : 1059588.4\n",
      "Epoch : 755 Training_loss : 1430089.8 Test_loss : 1058976.5\n",
      "Epoch : 756 Training_loss : 1429961.9 Test_loss : 1058380.0\n",
      "Epoch : 757 Training_loss : 1429839.0 Test_loss : 1057797.2\n",
      "Epoch : 758 Training_loss : 1429720.8 Test_loss : 1057229.0\n",
      "Epoch : 759 Training_loss : 1429607.5 Test_loss : 1056674.4\n",
      "Epoch : 760 Training_loss : 1429498.5 Test_loss : 1056133.1\n",
      "Epoch : 761 Training_loss : 1429393.9 Test_loss : 1055604.9\n",
      "Epoch : 762 Training_loss : 1429293.5 Test_loss : 1055089.0\n",
      "Epoch : 763 Training_loss : 1429197.2 Test_loss : 1054585.9\n",
      "Epoch : 764 Training_loss : 1429104.6 Test_loss : 1054094.6\n",
      "Epoch : 765 Training_loss : 1429015.6 Test_loss : 1053615.2\n",
      "Epoch : 766 Training_loss : 1428930.1 Test_loss : 1053146.4\n",
      "Epoch : 767 Training_loss : 1428848.0 Test_loss : 1052689.0\n",
      "Epoch : 768 Training_loss : 1428769.2 Test_loss : 1052242.2\n",
      "Epoch : 769 Training_loss : 1428693.5 Test_loss : 1051805.8\n",
      "Epoch : 770 Training_loss : 1428620.6 Test_loss : 1051379.5\n",
      "Epoch : 771 Training_loss : 1428550.9 Test_loss : 1050963.5\n",
      "Epoch : 772 Training_loss : 1428483.9 Test_loss : 1050557.4\n",
      "Epoch : 773 Training_loss : 1428419.5 Test_loss : 1050160.4\n",
      "Epoch : 774 Training_loss : 1428357.8 Test_loss : 1049772.4\n",
      "Epoch : 775 Training_loss : 1428298.2 Test_loss : 1049393.1\n",
      "Epoch : 776 Training_loss : 1428241.1 Test_loss : 1049023.0\n",
      "Epoch : 777 Training_loss : 1428186.2 Test_loss : 1048661.2\n",
      "Epoch : 778 Training_loss : 1428133.9 Test_loss : 1048308.4\n",
      "Epoch : 779 Training_loss : 1428083.2 Test_loss : 1047963.1\n",
      "Epoch : 780 Training_loss : 1428034.8 Test_loss : 1047626.4\n",
      "Epoch : 781 Training_loss : 1427988.1 Test_loss : 1047297.0\n",
      "Epoch : 782 Training_loss : 1427943.4 Test_loss : 1046975.4\n",
      "Epoch : 783 Training_loss : 1427900.6 Test_loss : 1046661.0\n",
      "Epoch : 784 Training_loss : 1427859.2 Test_loss : 1046353.2\n",
      "Epoch : 785 Training_loss : 1427819.5 Test_loss : 1046052.7\n",
      "Epoch : 786 Training_loss : 1427781.2 Test_loss : 1045758.4\n",
      "Epoch : 787 Training_loss : 1427745.0 Test_loss : 1045471.44\n",
      "Epoch : 788 Training_loss : 1427709.8 Test_loss : 1045190.75\n",
      "Epoch : 789 Training_loss : 1427675.9 Test_loss : 1044916.25\n",
      "Epoch : 790 Training_loss : 1427643.5 Test_loss : 1044647.8\n",
      "Epoch : 791 Training_loss : 1427612.5 Test_loss : 1044385.56\n",
      "Epoch : 792 Training_loss : 1427582.4 Test_loss : 1044129.44\n",
      "Epoch : 793 Training_loss : 1427553.6 Test_loss : 1043878.5\n",
      "Epoch : 794 Training_loss : 1427526.2 Test_loss : 1043633.5\n",
      "Epoch : 795 Training_loss : 1427499.9 Test_loss : 1043393.5\n",
      "Epoch : 796 Training_loss : 1427474.4 Test_loss : 1043159.7\n",
      "Epoch : 797 Training_loss : 1427450.1 Test_loss : 1042930.7\n",
      "Epoch : 798 Training_loss : 1427426.5 Test_loss : 1042706.9\n",
      "Epoch : 799 Training_loss : 1427404.1 Test_loss : 1042487.8\n",
      "Epoch : 800 Training_loss : 1427382.5 Test_loss : 1042273.8\n",
      "Epoch : 801 Training_loss : 1427361.6 Test_loss : 1042063.6\n",
      "Epoch : 802 Training_loss : 1427341.8 Test_loss : 1041858.5\n",
      "Epoch : 803 Training_loss : 1427322.8 Test_loss : 1041658.2\n",
      "Epoch : 804 Training_loss : 1427304.0 Test_loss : 1041461.8\n",
      "Epoch : 805 Training_loss : 1427286.4 Test_loss : 1041270.2\n",
      "Epoch : 806 Training_loss : 1427269.5 Test_loss : 1041082.56\n",
      "Epoch : 807 Training_loss : 1427253.2 Test_loss : 1040898.75\n",
      "Epoch : 808 Training_loss : 1427237.5 Test_loss : 1040718.75\n",
      "Epoch : 809 Training_loss : 1427222.4 Test_loss : 1040542.56\n",
      "Epoch : 810 Training_loss : 1427208.0 Test_loss : 1040370.25\n",
      "Epoch : 811 Training_loss : 1427194.1 Test_loss : 1040201.8\n",
      "Epoch : 812 Training_loss : 1427180.8 Test_loss : 1040037.1\n",
      "Epoch : 813 Training_loss : 1427168.0 Test_loss : 1039876.06\n",
      "Epoch : 814 Training_loss : 1427155.6 Test_loss : 1039718.9\n",
      "Epoch : 815 Training_loss : 1427143.9 Test_loss : 1039564.56\n",
      "Epoch : 816 Training_loss : 1427132.5 Test_loss : 1039413.9\n",
      "Epoch : 817 Training_loss : 1427121.6 Test_loss : 1039266.1\n",
      "Epoch : 818 Training_loss : 1427111.0 Test_loss : 1039122.06\n",
      "Epoch : 819 Training_loss : 1427101.1 Test_loss : 1038980.6\n",
      "Epoch : 820 Training_loss : 1427091.5 Test_loss : 1038842.25\n",
      "Epoch : 821 Training_loss : 1427082.2 Test_loss : 1038706.5\n",
      "Epoch : 822 Training_loss : 1427073.5 Test_loss : 1038574.44\n",
      "Epoch : 823 Training_loss : 1427065.0 Test_loss : 1038445.2\n",
      "Epoch : 824 Training_loss : 1427056.6 Test_loss : 1038318.56\n",
      "Epoch : 825 Training_loss : 1427049.0 Test_loss : 1038194.75\n",
      "Epoch : 826 Training_loss : 1427041.2 Test_loss : 1038072.9\n",
      "Epoch : 827 Training_loss : 1427033.9 Test_loss : 1037953.56\n",
      "Epoch : 828 Training_loss : 1427027.0 Test_loss : 1037837.2\n",
      "Epoch : 829 Training_loss : 1427020.1 Test_loss : 1037723.4\n",
      "Epoch : 830 Training_loss : 1427013.8 Test_loss : 1037611.44\n",
      "Epoch : 831 Training_loss : 1427007.6 Test_loss : 1037502.1\n",
      "Epoch : 832 Training_loss : 1427001.6 Test_loss : 1037395.6\n",
      "Epoch : 833 Training_loss : 1426996.1 Test_loss : 1037290.8\n",
      "Epoch : 834 Training_loss : 1426990.5 Test_loss : 1037188.7\n",
      "Epoch : 835 Training_loss : 1426985.6 Test_loss : 1037088.5\n",
      "Epoch : 836 Training_loss : 1426980.1 Test_loss : 1036990.06\n",
      "Epoch : 837 Training_loss : 1426975.5 Test_loss : 1036894.1\n",
      "Epoch : 838 Training_loss : 1426970.8 Test_loss : 1036800.06\n",
      "Epoch : 839 Training_loss : 1426966.5 Test_loss : 1036707.8\n",
      "Epoch : 840 Training_loss : 1426962.1 Test_loss : 1036617.5\n",
      "Epoch : 841 Training_loss : 1426957.8 Test_loss : 1036528.7\n",
      "Epoch : 842 Training_loss : 1426953.8 Test_loss : 1036442.56\n",
      "Epoch : 843 Training_loss : 1426950.1 Test_loss : 1036358.25\n",
      "Epoch : 844 Training_loss : 1426946.4 Test_loss : 1036275.75\n",
      "Epoch : 845 Training_loss : 1426943.0 Test_loss : 1036194.8\n",
      "Epoch : 846 Training_loss : 1426939.6 Test_loss : 1036115.94\n",
      "Epoch : 847 Training_loss : 1426936.4 Test_loss : 1036037.6\n",
      "Epoch : 848 Training_loss : 1426933.2 Test_loss : 1035961.44\n",
      "Epoch : 849 Training_loss : 1426930.5 Test_loss : 1035886.7\n",
      "Epoch : 850 Training_loss : 1426927.4 Test_loss : 1035813.75\n",
      "Epoch : 851 Training_loss : 1426924.6 Test_loss : 1035742.56\n",
      "Epoch : 852 Training_loss : 1426922.1 Test_loss : 1035672.25\n",
      "Epoch : 853 Training_loss : 1426919.5 Test_loss : 1035603.7\n",
      "Epoch : 854 Training_loss : 1426917.0 Test_loss : 1035536.9\n",
      "Epoch : 855 Training_loss : 1426914.5 Test_loss : 1035470.8\n",
      "Epoch : 856 Training_loss : 1426912.5 Test_loss : 1035406.56\n",
      "Epoch : 857 Training_loss : 1426910.2 Test_loss : 1035343.3\n",
      "Epoch : 858 Training_loss : 1426908.0 Test_loss : 1035281.6\n",
      "Epoch : 859 Training_loss : 1426906.2 Test_loss : 1035220.9\n",
      "Epoch : 860 Training_loss : 1426904.1 Test_loss : 1035161.6\n",
      "Epoch : 861 Training_loss : 1426902.5 Test_loss : 1035103.44\n",
      "Epoch : 862 Training_loss : 1426900.8 Test_loss : 1035047.0\n",
      "Epoch : 863 Training_loss : 1426899.0 Test_loss : 1034991.4\n",
      "Epoch : 864 Training_loss : 1426897.2 Test_loss : 1034936.56\n",
      "Epoch : 865 Training_loss : 1426895.6 Test_loss : 1034883.44\n",
      "Epoch : 866 Training_loss : 1426894.4 Test_loss : 1034831.1\n",
      "Epoch : 867 Training_loss : 1426892.8 Test_loss : 1034779.8\n",
      "Epoch : 868 Training_loss : 1426891.2 Test_loss : 1034729.3\n",
      "Epoch : 869 Training_loss : 1426890.1 Test_loss : 1034680.5\n",
      "Epoch : 870 Training_loss : 1426888.8 Test_loss : 1034632.56\n",
      "Epoch : 871 Training_loss : 1426887.5 Test_loss : 1034585.44\n",
      "Epoch : 872 Training_loss : 1426886.4 Test_loss : 1034539.1\n",
      "Epoch : 873 Training_loss : 1426885.5 Test_loss : 1034493.75\n",
      "Epoch : 874 Training_loss : 1426884.4 Test_loss : 1034449.0\n",
      "Epoch : 875 Training_loss : 1426883.2 Test_loss : 1034405.44\n",
      "Epoch : 876 Training_loss : 1426882.1 Test_loss : 1034362.5\n",
      "Epoch : 877 Training_loss : 1426881.2 Test_loss : 1034321.3\n",
      "Epoch : 878 Training_loss : 1426880.0 Test_loss : 1034280.94\n",
      "Epoch : 879 Training_loss : 1426879.4 Test_loss : 1034240.56\n",
      "Epoch : 880 Training_loss : 1426878.4 Test_loss : 1034201.1\n",
      "Epoch : 881 Training_loss : 1426877.5 Test_loss : 1034162.25\n",
      "Epoch : 882 Training_loss : 1426876.8 Test_loss : 1034124.5\n",
      "Epoch : 883 Training_loss : 1426876.1 Test_loss : 1034087.44\n",
      "Epoch : 884 Training_loss : 1426875.5 Test_loss : 1034051.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 885 Training_loss : 1426874.9 Test_loss : 1034016.06\n",
      "Epoch : 886 Training_loss : 1426873.8 Test_loss : 1033981.6\n",
      "Epoch : 887 Training_loss : 1426873.2 Test_loss : 1033947.8\n",
      "Epoch : 888 Training_loss : 1426872.8 Test_loss : 1033915.06\n",
      "Epoch : 889 Training_loss : 1426872.4 Test_loss : 1033882.25\n",
      "Epoch : 890 Training_loss : 1426871.6 Test_loss : 1033850.25\n",
      "Epoch : 891 Training_loss : 1426871.0 Test_loss : 1033819.1\n",
      "Epoch : 892 Training_loss : 1426870.5 Test_loss : 1033788.9\n",
      "Epoch : 893 Training_loss : 1426870.1 Test_loss : 1033758.56\n",
      "Epoch : 894 Training_loss : 1426869.5 Test_loss : 1033729.1\n",
      "Epoch : 895 Training_loss : 1426869.0 Test_loss : 1033700.4\n",
      "Epoch : 896 Training_loss : 1426868.8 Test_loss : 1033672.6\n",
      "Epoch : 897 Training_loss : 1426868.1 Test_loss : 1033644.7\n",
      "Epoch : 898 Training_loss : 1426867.9 Test_loss : 1033617.9\n",
      "Epoch : 899 Training_loss : 1426867.5 Test_loss : 1033591.6\n",
      "Epoch : 900 Training_loss : 1426867.2 Test_loss : 1033565.5\n",
      "Epoch : 901 Training_loss : 1426866.8 Test_loss : 1033540.2\n",
      "Epoch : 902 Training_loss : 1426866.4 Test_loss : 1033515.7\n",
      "Epoch : 903 Training_loss : 1426865.9 Test_loss : 1033491.2\n",
      "Epoch : 904 Training_loss : 1426865.8 Test_loss : 1033467.5\n",
      "Epoch : 905 Training_loss : 1426865.6 Test_loss : 1033443.8\n",
      "Epoch : 906 Training_loss : 1426865.1 Test_loss : 1033420.94\n",
      "Epoch : 907 Training_loss : 1426864.6 Test_loss : 1033398.2\n",
      "Epoch : 908 Training_loss : 1426864.4 Test_loss : 1033376.06\n",
      "Epoch : 909 Training_loss : 1426864.1 Test_loss : 1033355.0\n",
      "Epoch : 910 Training_loss : 1426864.0 Test_loss : 1033333.75\n",
      "Epoch : 911 Training_loss : 1426863.6 Test_loss : 1033313.44\n",
      "Epoch : 912 Training_loss : 1426863.5 Test_loss : 1033293.0\n",
      "Epoch : 913 Training_loss : 1426863.4 Test_loss : 1033273.5\n",
      "Epoch : 914 Training_loss : 1426863.2 Test_loss : 1033253.94\n",
      "Epoch : 915 Training_loss : 1426862.9 Test_loss : 1033235.2\n",
      "Epoch : 916 Training_loss : 1426862.9 Test_loss : 1033216.5\n",
      "Epoch : 917 Training_loss : 1426862.4 Test_loss : 1033197.9\n",
      "Epoch : 918 Training_loss : 1426862.4 Test_loss : 1033179.8\n",
      "Epoch : 919 Training_loss : 1426862.1 Test_loss : 1033161.94\n",
      "Epoch : 920 Training_loss : 1426862.1 Test_loss : 1033144.9\n",
      "Epoch : 921 Training_loss : 1426861.8 Test_loss : 1033127.75\n",
      "Epoch : 922 Training_loss : 1426861.8 Test_loss : 1033111.5\n",
      "Epoch : 923 Training_loss : 1426861.5 Test_loss : 1033095.3\n",
      "Epoch : 924 Training_loss : 1426861.2 Test_loss : 1033079.06\n",
      "Epoch : 925 Training_loss : 1426861.1 Test_loss : 1033063.6\n",
      "Epoch : 926 Training_loss : 1426861.0 Test_loss : 1033048.2\n",
      "Epoch : 927 Training_loss : 1426860.8 Test_loss : 1033033.56\n",
      "Epoch : 928 Training_loss : 1426860.8 Test_loss : 1033018.9\n",
      "Epoch : 929 Training_loss : 1426860.6 Test_loss : 1033004.4\n",
      "Epoch : 930 Training_loss : 1426860.5 Test_loss : 1032990.44\n",
      "Epoch : 931 Training_loss : 1426860.4 Test_loss : 1032976.8\n",
      "Epoch : 932 Training_loss : 1426860.4 Test_loss : 1032962.9\n",
      "Epoch : 933 Training_loss : 1426860.1 Test_loss : 1032949.94\n",
      "Epoch : 934 Training_loss : 1426860.0 Test_loss : 1032936.94\n",
      "Epoch : 935 Training_loss : 1426860.1 Test_loss : 1032923.94\n",
      "Epoch : 936 Training_loss : 1426859.9 Test_loss : 1032911.75\n",
      "Epoch : 937 Training_loss : 1426859.9 Test_loss : 1032899.7\n",
      "Epoch : 938 Training_loss : 1426859.9 Test_loss : 1032887.44\n",
      "Epoch : 939 Training_loss : 1426859.6 Test_loss : 1032876.06\n",
      "Epoch : 940 Training_loss : 1426859.5 Test_loss : 1032864.8\n",
      "Epoch : 941 Training_loss : 1426859.5 Test_loss : 1032853.56\n",
      "Epoch : 942 Training_loss : 1426859.4 Test_loss : 1032842.1\n",
      "Epoch : 943 Training_loss : 1426859.5 Test_loss : 1032831.5\n",
      "Epoch : 944 Training_loss : 1426859.5 Test_loss : 1032821.0\n",
      "Epoch : 945 Training_loss : 1426859.4 Test_loss : 1032810.44\n",
      "Epoch : 946 Training_loss : 1426859.2 Test_loss : 1032799.94\n",
      "Epoch : 947 Training_loss : 1426859.2 Test_loss : 1032790.25\n",
      "Epoch : 948 Training_loss : 1426859.0 Test_loss : 1032780.56\n",
      "Epoch : 949 Training_loss : 1426859.0 Test_loss : 1032770.75\n",
      "Epoch : 950 Training_loss : 1426859.2 Test_loss : 1032761.1\n",
      "Epoch : 951 Training_loss : 1426858.9 Test_loss : 1032752.2\n",
      "Epoch : 952 Training_loss : 1426858.8 Test_loss : 1032743.2\n",
      "Epoch : 953 Training_loss : 1426858.9 Test_loss : 1032734.44\n",
      "Epoch : 954 Training_loss : 1426858.9 Test_loss : 1032725.5\n",
      "Epoch : 955 Training_loss : 1426858.9 Test_loss : 1032717.44\n",
      "Epoch : 956 Training_loss : 1426858.9 Test_loss : 1032709.25\n",
      "Epoch : 957 Training_loss : 1426858.9 Test_loss : 1032701.2\n",
      "Epoch : 958 Training_loss : 1426858.5 Test_loss : 1032693.0\n",
      "Epoch : 959 Training_loss : 1426858.9 Test_loss : 1032685.0\n",
      "Epoch : 960 Training_loss : 1426858.5 Test_loss : 1032677.75\n",
      "Epoch : 961 Training_loss : 1426858.5 Test_loss : 1032670.5\n",
      "Epoch : 962 Training_loss : 1426858.5 Test_loss : 1032663.1\n",
      "Epoch : 963 Training_loss : 1426858.5 Test_loss : 1032655.94\n",
      "Epoch : 964 Training_loss : 1426858.5 Test_loss : 1032648.7\n",
      "Epoch : 965 Training_loss : 1426858.4 Test_loss : 1032641.25\n",
      "Epoch : 966 Training_loss : 1426858.5 Test_loss : 1032634.8\n",
      "Epoch : 967 Training_loss : 1426858.4 Test_loss : 1032628.4\n",
      "Epoch : 968 Training_loss : 1426858.4 Test_loss : 1032622.06\n",
      "Epoch : 969 Training_loss : 1426858.4 Test_loss : 1032615.5\n",
      "Epoch : 970 Training_loss : 1426858.2 Test_loss : 1032609.0\n",
      "Epoch : 971 Training_loss : 1426858.4 Test_loss : 1032602.5\n",
      "Epoch : 972 Training_loss : 1426858.2 Test_loss : 1032596.9\n",
      "Epoch : 973 Training_loss : 1426858.2 Test_loss : 1032591.3\n",
      "Epoch : 974 Training_loss : 1426858.2 Test_loss : 1032585.5\n",
      "Epoch : 975 Training_loss : 1426858.2 Test_loss : 1032579.8\n",
      "Epoch : 976 Training_loss : 1426858.2 Test_loss : 1032574.25\n",
      "Epoch : 977 Training_loss : 1426858.1 Test_loss : 1032568.6\n",
      "Epoch : 978 Training_loss : 1426858.2 Test_loss : 1032563.0\n",
      "Epoch : 979 Training_loss : 1426858.1 Test_loss : 1032558.1\n",
      "Epoch : 980 Training_loss : 1426858.2 Test_loss : 1032553.3\n",
      "Epoch : 981 Training_loss : 1426858.1 Test_loss : 1032548.4\n",
      "Epoch : 982 Training_loss : 1426858.2 Test_loss : 1032543.6\n",
      "Epoch : 983 Training_loss : 1426858.1 Test_loss : 1032538.8\n",
      "Epoch : 984 Training_loss : 1426858.1 Test_loss : 1032533.9\n",
      "Epoch : 985 Training_loss : 1426857.9 Test_loss : 1032529.0\n",
      "Epoch : 986 Training_loss : 1426858.1 Test_loss : 1032524.25\n",
      "Epoch : 987 Training_loss : 1426857.9 Test_loss : 1032520.06\n",
      "Epoch : 988 Training_loss : 1426858.1 Test_loss : 1032516.2\n",
      "Epoch : 989 Training_loss : 1426857.9 Test_loss : 1032512.06\n",
      "Epoch : 990 Training_loss : 1426857.9 Test_loss : 1032508.0\n",
      "Epoch : 991 Training_loss : 1426858.2 Test_loss : 1032504.0\n",
      "Epoch : 992 Training_loss : 1426858.1 Test_loss : 1032500.0\n",
      "Epoch : 993 Training_loss : 1426858.1 Test_loss : 1032496.0\n",
      "Epoch : 994 Training_loss : 1426858.2 Test_loss : 1032491.94\n",
      "Epoch : 995 Training_loss : 1426858.2 Test_loss : 1032487.94\n",
      "Epoch : 996 Training_loss : 1426858.1 Test_loss : 1032483.94\n",
      "Epoch : 997 Training_loss : 1426857.9 Test_loss : 1032480.56\n",
      "Epoch : 998 Training_loss : 1426858.1 Test_loss : 1032477.3\n",
      "Epoch : 999 Training_loss : 1426857.9 Test_loss : 1032474.2\n",
      "Epoch : 1000 Training_loss : 1426857.9 Test_loss : 1032471.06\n",
      "Epoch : 1001 Training_loss : 1426857.9 Test_loss : 1032467.7\n",
      "Epoch : 1002 Training_loss : 1426857.9 Test_loss : 1032464.4\n",
      "Epoch : 1003 Training_loss : 1426857.9 Test_loss : 1032461.3\n",
      "Epoch : 1004 Training_loss : 1426857.9 Test_loss : 1032458.06\n",
      "Epoch : 1005 Training_loss : 1426858.1 Test_loss : 1032454.75\n",
      "Epoch : 1006 Training_loss : 1426858.1 Test_loss : 1032451.6\n",
      "Epoch : 1007 Training_loss : 1426857.9 Test_loss : 1032448.3\n",
      "Epoch : 1008 Training_loss : 1426858.1 Test_loss : 1032445.2\n",
      "Epoch : 1009 Training_loss : 1426857.9 Test_loss : 1032442.75\n",
      "Epoch : 1010 Training_loss : 1426857.9 Test_loss : 1032440.3\n",
      "Epoch : 1011 Training_loss : 1426857.9 Test_loss : 1032437.9\n",
      "Epoch : 1012 Training_loss : 1426857.9 Test_loss : 1032435.44\n",
      "Epoch : 1013 Training_loss : 1426857.9 Test_loss : 1032433.0\n",
      "Epoch : 1014 Training_loss : 1426857.9 Test_loss : 1032430.56\n",
      "Epoch : 1015 Training_loss : 1426857.9 Test_loss : 1032428.06\n",
      "Epoch : 1016 Training_loss : 1426857.9 Test_loss : 1032425.75\n",
      "Epoch : 1017 Training_loss : 1426857.9 Test_loss : 1032423.4\n",
      "Epoch : 1018 Training_loss : 1426857.9 Test_loss : 1032420.94\n",
      "Epoch : 1019 Training_loss : 1426858.1 Test_loss : 1032418.5\n",
      "Epoch : 1020 Training_loss : 1426857.9 Test_loss : 1032416.06\n",
      "Epoch : 1021 Training_loss : 1426857.9 Test_loss : 1032413.6\n",
      "Epoch : 1022 Training_loss : 1426857.9 Test_loss : 1032411.1\n",
      "Epoch : 1023 Training_loss : 1426857.9 Test_loss : 1032408.8\n",
      "Epoch : 1024 Training_loss : 1426857.9 Test_loss : 1032406.25\n",
      "Epoch : 1025 Training_loss : 1426857.9 Test_loss : 1032404.0\n",
      "Epoch : 1026 Training_loss : 1426857.9 Test_loss : 1032402.25\n",
      "Epoch : 1027 Training_loss : 1426857.9 Test_loss : 1032400.8\n",
      "Epoch : 1028 Training_loss : 1426857.8 Test_loss : 1032399.1\n",
      "Epoch : 1029 Training_loss : 1426858.1 Test_loss : 1032397.6\n",
      "Epoch : 1030 Training_loss : 1426857.9 Test_loss : 1032395.94\n",
      "Epoch : 1031 Training_loss : 1426857.9 Test_loss : 1032394.25\n",
      "Epoch : 1032 Training_loss : 1426857.8 Test_loss : 1032392.8\n",
      "Epoch : 1033 Training_loss : 1426857.9 Test_loss : 1032391.1\n",
      "Epoch : 1034 Training_loss : 1426857.9 Test_loss : 1032389.44\n",
      "Epoch : 1035 Training_loss : 1426857.9 Test_loss : 1032387.94\n",
      "Epoch : 1036 Training_loss : 1426857.8 Test_loss : 1032386.4\n",
      "Epoch : 1037 Training_loss : 1426857.8 Test_loss : 1032384.6\n",
      "Epoch : 1038 Training_loss : 1426857.8 Test_loss : 1032383.06\n",
      "Epoch : 1039 Training_loss : 1426857.9 Test_loss : 1032381.44\n",
      "Epoch : 1040 Training_loss : 1426857.9 Test_loss : 1032379.94\n",
      "Epoch : 1041 Training_loss : 1426857.8 Test_loss : 1032378.2\n",
      "Epoch : 1042 Training_loss : 1426857.9 Test_loss : 1032376.56\n",
      "Epoch : 1043 Training_loss : 1426857.9 Test_loss : 1032375.0\n",
      "Epoch : 1044 Training_loss : 1426857.9 Test_loss : 1032373.44\n",
      "Epoch : 1045 Training_loss : 1426857.8 Test_loss : 1032371.7\n",
      "Epoch : 1046 Training_loss : 1426857.9 Test_loss : 1032370.2\n",
      "Epoch : 1047 Training_loss : 1426857.8 Test_loss : 1032368.56\n",
      "Epoch : 1048 Training_loss : 1426857.8 Test_loss : 1032367.0\n",
      "Epoch : 1049 Training_loss : 1426857.8 Test_loss : 1032365.3\n",
      "Epoch : 1050 Training_loss : 1426857.8 Test_loss : 1032364.56\n",
      "Epoch : 1051 Training_loss : 1426857.8 Test_loss : 1032363.7\n",
      "Epoch : 1052 Training_loss : 1426857.6 Test_loss : 1032362.8\n",
      "Epoch : 1053 Training_loss : 1426858.1 Test_loss : 1032362.06\n",
      "Epoch : 1054 Training_loss : 1426857.9 Test_loss : 1032361.25\n",
      "Epoch : 1055 Training_loss : 1426857.9 Test_loss : 1032360.5\n",
      "Epoch : 1056 Training_loss : 1426857.8 Test_loss : 1032359.6\n",
      "Epoch : 1057 Training_loss : 1426857.9 Test_loss : 1032358.8\n",
      "Epoch : 1058 Training_loss : 1426857.8 Test_loss : 1032358.1\n",
      "Epoch : 1059 Training_loss : 1426857.9 Test_loss : 1032357.25\n",
      "Epoch : 1060 Training_loss : 1426857.9 Test_loss : 1032356.4\n",
      "Epoch : 1061 Training_loss : 1426857.9 Test_loss : 1032355.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1062 Training_loss : 1426857.9 Test_loss : 1032354.9\n",
      "Epoch : 1063 Training_loss : 1426857.9 Test_loss : 1032354.06\n",
      "Epoch : 1064 Training_loss : 1426857.8 Test_loss : 1032353.25\n",
      "Epoch : 1065 Training_loss : 1426857.8 Test_loss : 1032352.3\n",
      "Epoch : 1066 Training_loss : 1426857.9 Test_loss : 1032351.7\n",
      "Epoch : 1067 Training_loss : 1426857.9 Test_loss : 1032350.8\n",
      "Epoch : 1068 Training_loss : 1426857.8 Test_loss : 1032349.94\n",
      "Epoch : 1069 Training_loss : 1426857.9 Test_loss : 1032349.25\n",
      "Epoch : 1070 Training_loss : 1426857.9 Test_loss : 1032348.4\n",
      "Epoch : 1071 Training_loss : 1426857.9 Test_loss : 1032347.7\n",
      "Epoch : 1072 Training_loss : 1426857.9 Test_loss : 1032346.75\n",
      "Epoch : 1073 Training_loss : 1426857.9 Test_loss : 1032345.94\n",
      "Epoch : 1074 Training_loss : 1426857.9 Test_loss : 1032345.2\n",
      "Epoch : 1075 Training_loss : 1426857.9 Test_loss : 1032344.4\n",
      "Epoch : 1076 Training_loss : 1426857.8 Test_loss : 1032343.5\n",
      "Epoch : 1077 Training_loss : 1426857.8 Test_loss : 1032342.75\n",
      "Epoch : 1078 Training_loss : 1426857.9 Test_loss : 1032341.9\n",
      "Epoch : 1079 Training_loss : 1426857.9 Test_loss : 1032341.1\n",
      "Epoch : 1080 Training_loss : 1426857.9 Test_loss : 1032340.4\n",
      "Epoch : 1081 Training_loss : 1426857.9 Test_loss : 1032339.5\n",
      "Epoch : 1082 Training_loss : 1426857.9 Test_loss : 1032338.75\n",
      "Epoch : 1083 Training_loss : 1426857.9 Test_loss : 1032337.9\n",
      "Epoch : 1084 Training_loss : 1426857.8 Test_loss : 1032337.0\n",
      "Epoch : 1085 Training_loss : 1426857.9 Test_loss : 1032336.3\n",
      "Epoch : 1086 Training_loss : 1426857.8 Test_loss : 1032335.5\n",
      "Epoch : 1087 Training_loss : 1426857.9 Test_loss : 1032334.56\n",
      "Epoch : 1088 Training_loss : 1426857.6 Test_loss : 1032333.9\n",
      "Epoch : 1089 Training_loss : 1426857.8 Test_loss : 1032333.0\n",
      "Epoch : 1090 Training_loss : 1426857.6 Test_loss : 1032332.25\n",
      "Epoch : 1091 Training_loss : 1426857.8 Test_loss : 1032331.5\n",
      "Epoch : 1092 Training_loss : 1426857.6 Test_loss : 1032330.7\n",
      "Epoch : 1093 Training_loss : 1426857.9 Test_loss : 1032329.8\n",
      "Epoch : 1094 Training_loss : 1426857.9 Test_loss : 1032329.0\n",
      "Epoch : 1095 Training_loss : 1426857.9 Test_loss : 1032328.25\n",
      "Epoch : 1096 Training_loss : 1426857.8 Test_loss : 1032327.5\n",
      "Epoch : 1097 Training_loss : 1426857.9 Test_loss : 1032326.75\n",
      "Epoch : 1098 Training_loss : 1426857.9 Test_loss : 1032325.8\n",
      "Epoch : 1099 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1100 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1101 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1102 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1103 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1104 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1105 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1106 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1107 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1108 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1109 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1110 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1111 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1112 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1113 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1114 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1115 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1116 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1117 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1118 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1119 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1120 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1121 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1122 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1123 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1124 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1125 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1126 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1127 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1128 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1129 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1130 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1131 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1132 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1133 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1134 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1135 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1136 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1137 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1138 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1139 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1140 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1141 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1142 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1143 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1144 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1145 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1146 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1147 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1148 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1149 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1150 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1151 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1152 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1153 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1154 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1155 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1156 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1157 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1158 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1159 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1160 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1161 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1162 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1163 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1164 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1165 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1166 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1167 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1168 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1169 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1170 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1171 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1172 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1173 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1174 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1175 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1176 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1177 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1178 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1179 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1180 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1181 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1182 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1183 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1184 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1185 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1186 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1187 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1188 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1189 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1190 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1191 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1192 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1193 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1194 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1195 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1196 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1197 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1198 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1199 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1200 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1201 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1202 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1203 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1204 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1205 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1206 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1207 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1208 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1209 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1210 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1211 Training_loss : 1426857.9 Test_loss : 1032325.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1212 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1213 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1214 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1215 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1216 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1217 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1218 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1219 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1220 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1221 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1222 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1223 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1224 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1225 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1226 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1227 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1228 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1229 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1230 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1231 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1232 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1233 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1234 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1235 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1236 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1237 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1238 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1239 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1240 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1241 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1242 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1243 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1244 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1245 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1246 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1247 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1248 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1249 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1250 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1251 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1252 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1253 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1254 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1255 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1256 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1257 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1258 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1259 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1260 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1261 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1262 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1263 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1264 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1265 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1266 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1267 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1268 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1269 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1270 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1271 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1272 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1273 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1274 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1275 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1276 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1277 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1278 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1279 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1280 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1281 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1282 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1283 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1284 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1285 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1286 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1287 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1288 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1289 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1290 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1291 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1292 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1293 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1294 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1295 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1296 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1297 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1298 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1299 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1300 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1301 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1302 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1303 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1304 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1305 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1306 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1307 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1308 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1309 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1310 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1311 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1312 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1313 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1314 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1315 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1316 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1317 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1318 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1319 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1320 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1321 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1322 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1323 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1324 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1325 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1326 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1327 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1328 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1329 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1330 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1331 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1332 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1333 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1334 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1335 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1336 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1337 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1338 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1339 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1340 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1341 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1342 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1343 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1344 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1345 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1346 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1347 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1348 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1349 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1350 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1351 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1352 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1353 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1354 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1355 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1356 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1357 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1358 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1359 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1360 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1361 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1362 Training_loss : 1426857.9 Test_loss : 1032325.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1363 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1364 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1365 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1366 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1367 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1368 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1369 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1370 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1371 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1372 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1373 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1374 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1375 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1376 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1377 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1378 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1379 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1380 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1381 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1382 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1383 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1384 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1385 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1386 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1387 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1388 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1389 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1390 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1391 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1392 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1393 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1394 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1395 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1396 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1397 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1398 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1399 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1400 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1401 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1402 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1403 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1404 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1405 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1406 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1407 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1408 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1409 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1410 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1411 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1412 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1413 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1414 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1415 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1416 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1417 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1418 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1419 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1420 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1421 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1422 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1423 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1424 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1425 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1426 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1427 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1428 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1429 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1430 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1431 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1432 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1433 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1434 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1435 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1436 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1437 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1438 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1439 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1440 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1441 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1442 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1443 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1444 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1445 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1446 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1447 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1448 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1449 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1450 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1451 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1452 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1453 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1454 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1455 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1456 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1457 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1458 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1459 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1460 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1461 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1462 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1463 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1464 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1465 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1466 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1467 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1468 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1469 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1470 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1471 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1472 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1473 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1474 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1475 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1476 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1477 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1478 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1479 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1480 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1481 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1482 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1483 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1484 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1485 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1486 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1487 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1488 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1489 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1490 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1491 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1492 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1493 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1494 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1495 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1496 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1497 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1498 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1499 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1500 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1501 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1502 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1503 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1504 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1505 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1506 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1507 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1508 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1509 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1510 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1511 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1512 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1513 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1514 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1515 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1516 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1517 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1518 Training_loss : 1426857.9 Test_loss : 1032325.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1519 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1520 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1521 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1522 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1523 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1524 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1525 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1526 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1527 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1528 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1529 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1530 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1531 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1532 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1533 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1534 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1535 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1536 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1537 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1538 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1539 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1540 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1541 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1542 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1543 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1544 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1545 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1546 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1547 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1548 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1549 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1550 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1551 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1552 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1553 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1554 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1555 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1556 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1557 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1558 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1559 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1560 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1561 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1562 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1563 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1564 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1565 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1566 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1567 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1568 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1569 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1570 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1571 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1572 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1573 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1574 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1575 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1576 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1577 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1578 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1579 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1580 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1581 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1582 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1583 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1584 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1585 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1586 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1587 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1588 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1589 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1590 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1591 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1592 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1593 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1594 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1595 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1596 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1597 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1598 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1599 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1600 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1601 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1602 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1603 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1604 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1605 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1606 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1607 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1608 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1609 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1610 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1611 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1612 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1613 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1614 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1615 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1616 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1617 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1618 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1619 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1620 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1621 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1622 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1623 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1624 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1625 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1626 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1627 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1628 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1629 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1630 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1631 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1632 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1633 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1634 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1635 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1636 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1637 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1638 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1639 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1640 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1641 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1642 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1643 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1644 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1645 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1646 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1647 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1648 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1649 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1650 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1651 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1652 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1653 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1654 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1655 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1656 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1657 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1658 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1659 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1660 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1661 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1662 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1663 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1664 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1665 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1666 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1667 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1668 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1669 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1670 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1671 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1672 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1673 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1674 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1675 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1676 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1677 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1678 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1679 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1680 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1681 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1682 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1683 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1684 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1685 Training_loss : 1426857.9 Test_loss : 1032325.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1686 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1687 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1688 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1689 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1690 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1691 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1692 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1693 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1694 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1695 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1696 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1697 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1698 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1699 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1700 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1701 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1702 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1703 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1704 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1705 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1706 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1707 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1708 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1709 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1710 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1711 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1712 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1713 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1714 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1715 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1716 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1717 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1718 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1719 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1720 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1721 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1722 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1723 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1724 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1725 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1726 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1727 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1728 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1729 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1730 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1731 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1732 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1733 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1734 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1735 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1736 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1737 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1738 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1739 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1740 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1741 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1742 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1743 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1744 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1745 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1746 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1747 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1748 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1749 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1750 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1751 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1752 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1753 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1754 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1755 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1756 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1757 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1758 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1759 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1760 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1761 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1762 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1763 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1764 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1765 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1766 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1767 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1768 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1769 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1770 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1771 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1772 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1773 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1774 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1775 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1776 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1777 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1778 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1779 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1780 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1781 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1782 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1783 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1784 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1785 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1786 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1787 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1788 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1789 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1790 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1791 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1792 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1793 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1794 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1795 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1796 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1797 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1798 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1799 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1800 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1801 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1802 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1803 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1804 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1805 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1806 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1807 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1808 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1809 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1810 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1811 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1812 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1813 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1814 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1815 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1816 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1817 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1818 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1819 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1820 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1821 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1822 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1823 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1824 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1825 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1826 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1827 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1828 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1829 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1830 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1831 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1832 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1833 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1834 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1835 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1836 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1837 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1838 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1839 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1840 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1841 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1842 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1843 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1844 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1845 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1846 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1847 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1848 Training_loss : 1426857.9 Test_loss : 1032325.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1849 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1850 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1851 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1852 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1853 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1854 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1855 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1856 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1857 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1858 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1859 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1860 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1861 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1862 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1863 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1864 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1865 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1866 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1867 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1868 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1869 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1870 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1871 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1872 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1873 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1874 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1875 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1876 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1877 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1878 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1879 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1880 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1881 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1882 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1883 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1884 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1885 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1886 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1887 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1888 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1889 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1890 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1891 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1892 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1893 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1894 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1895 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1896 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1897 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1898 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1899 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1900 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1901 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1902 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1903 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1904 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1905 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1906 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1907 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1908 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1909 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1910 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1911 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1912 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1913 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1914 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1915 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1916 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1917 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1918 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1919 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1920 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1921 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1922 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1923 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1924 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1925 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1926 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1927 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1928 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1929 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1930 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1931 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1932 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1933 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1934 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1935 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1936 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1937 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1938 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1939 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1940 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1941 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1942 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1943 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1944 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1945 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1946 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1947 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1948 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1949 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1950 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1951 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1952 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1953 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1954 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1955 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1956 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1957 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1958 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1959 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1960 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1961 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1962 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1963 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1964 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1965 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1966 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1967 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1968 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1969 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1970 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1971 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1972 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1973 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1974 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1975 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1976 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1977 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1978 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1979 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1980 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1981 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1982 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1983 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1984 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1985 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1986 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1987 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1988 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1989 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1990 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1991 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1992 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1993 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1994 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1995 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1996 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1997 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1998 Training_loss : 1426857.9 Test_loss : 1032325.1\n",
      "Epoch : 1999 Training_loss : 1426857.9 Test_loss : 1032325.1\n"
     ]
    }
   ],
   "source": [
    "Training_loss = [ ]\n",
    "Test_loss = [ ]\n",
    "epoch = 2000\n",
    "for i in range(epoch):\n",
    "    sess.run(train, feed_dict = {x:x_train, y:y_train})\n",
    "    \n",
    "    Training_loss.append(sess.run(cost, feed_dict = {x: x_train, y: y_train}))\n",
    "    Test_loss.append(sess.run(cost, feed_dict = {x: x_test, y: y_test}))\n",
    "    \n",
    "\n",
    "    print('Epoch :',i,'Training_loss :',Training_loss[i], 'Test_loss :', Test_loss[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sess.run(out, feed_dict = {x:x_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25989.865],\n",
       "       [25989.865],\n",
       "       [25989.865],\n",
       "       [25989.865],\n",
       "       [25989.865]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27359.160156],\n",
       "       [25169.880859],\n",
       "       [26452.660156],\n",
       "       [27171.900391],\n",
       "       [26783.490234]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Test_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
